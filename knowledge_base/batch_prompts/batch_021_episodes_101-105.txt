Process episodes 101 through 105 and add them to the knowledge base:

Episode 101: “Dumbest idea I’ve heard” to $100M ARR: Inside the rise of Gamma | Grant Lee (co-founder)
Guest: Grant Lee

```json
{
  "id": "grant-lee",
  "guest": "Grant Lee",
  "title": "“Dumbest idea I’ve heard” to $100M ARR: Inside the rise of Gamma | Grant Lee (co-founder)",
  "transcript": "# “Dumbest idea I’ve heard” to $100M ARR: Inside the rise of Gamma | Grant Lee (co-founder)\n\n## Transcript\n\nGrant Lee (00:00:00):\nI'm in my third pitch in, I get to the very end of the pitch, feeling pretty good about myself. The investor pauses a little bit, and then just says, \"That has to be the worst pitch, worst idea I have ever heard. Not only are you trying to go against incumbents, you're going against incumbents that have massive distribution. You are never going to succeed.\"\n\nLenny Rachitsky (00:00:18):\nYou guys are at over 100 million ARR now, worth over $2 billion. One of the most interesting ways you guys grew early on was influencer marketing.\n\nGrant Lee (00:00:25):\nAll the initial influencers, I onboarded manually myself. I would jump on a call with each one of them so that they understood what Gamma represented, how to use the product. You want to be able to have them tell you story but in their voice. I think a lot of people think influencer marketing and they'll think these big trendy creators, people that have a million followers. This is the wrong approach. You basically give them a script to read, immediately feels like an ad. That product is not connected really to them in any way. You're much better doing the hard thing, which is hard to scale, finding the thousands of micro influencers that have an audience where your product maybe is actually useful. People really trust what they say. That ends up becoming this wildfire that can spread really, really fast.\n\nLenny Rachitsky (00:01:04):\nSomething you talk about it, there is actually a lot of ways to think experimentally, even in the early stages.\n\nGrant Lee (00:01:08):\nWe would have an idea in the morning, come up with some sort of functional prototype, recruit a bunch of people that are legitimately good prospective users, but have zero skin in the game, ship fast so people can start playing with it. In the afternoon, we're already running pretty full scale experiment. You start actually hearing other people describe their usage of the product. We can also watch them struggle. By the evening or by the next day. We can actually go through all of it together and say, okay, we're going back and we have to fix this. This is not usable and we've done that for everything.\n\nLenny Rachitsky (00:01:36):\nToday my guest is Grant Lee, CEO and co-founder of Gamma. This is a really unique and inspiring, and very tactically useful conversation because Grant is building something that is essentially the dream for most founders. A massive AI startup that's profitable, and has been for a long time, that didn't raise a lot of money for a long time. And as a small team, it's just around 30 people, all who can fit in a small restaurant serving over 50 million users globally.\n\n(00:02:02):\nIf you're not familiar with Gamma, they're an AI powered presentation and website design tool. They just hit 100 million ARR in just over two years. They're valued at over $2 billion. And unlike a lot of the fast growing AI startups that you hear about, they're growing profitably and sustainably, and in a category that most people did not believe had a huge business opportunity. As you'll hear in the conversation, one investor told Grant, this is the dumbest idea that he has ever heard.\n\n(00:02:29):\nIn this conversation, Grant shares the very counter-intuitive lessons that he's learned, finding product market fit, how he knew they had product market fit, the specific tactics that helped them grow, including a deep dive into influencer marketing, which blew my mind. Also how they figured out their price, his thoughts on building a GPT wrapper company that is durable, a ton of hiring advice, and so much more. This could honestly have been another two hours of conversation. I suspect we'll do another follow-up conversation next year.\n\n(00:02:56):\nIf you love this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It helps tremendously. And if you become an annual subscriber of my newsletter, you get a year free of 16 incredible products, including Devin, Lovable, Replit, Bolt, n8n, Linear, Superhuman, Descript, Wispr Flow, Gamma, Perplexity, Warp, Granola, Magic Patterns, Raycast, ChatPRD, and Mobbin. Head on over to lennysnewsletter.com and click product pass. With that, I bring you Grant Lee after a short word from our sponsors.\n\n(00:03:29):\nMy podcasts guest and I love talking about craft and taste and agency and product market fit. You know what we don't love talking? About SOC 2. That's where Vanta comes in. Vanta helps companies of all sizes get complying fast and stay that way with industry-leading AI automation and continuous monitoring. Whether you're a startup tackling your first SOC 2 or ISO 27001, or an enterprise managing vendor risk, Vanta's trust management platform makes it quicker, easier, and more scalable. Vanta also helps you complete security questionnaires up to five times faster so that you can win bigger deals sooner. The result, according to a recent IDC study, Vanta customers slashed over $500,000 a year and are three times more productive. Establishing trust isn't optional. Vanta makes it automatic. Get $1,000 off at vanta.com/lenny.\n\n(00:04:24):\nDid you know that I have a whole team that helps me with my podcast and with my newsletter. I want everyone on that team to be super happy and thrive in the roles. Justworks knows that your employees are more than just your employees. They're your people. My team is spread out across Colorado, Australia, Nepal, West Africa, and San Francisco. My life would be so incredibly complicated to hire people internationally, to pay people on time, and in their local currencies, and to answer their HR questions 24/7. But with Justworks, it's super easy. Whether you're setting up your own automated payroll, offering premium benefits, or hiring internationally. Justworks offer simple software and 24/7 human support from small business experts for you and your people. They do your human resources right so that you can do right by your people. Justworks, for your people. Grant, thank you so much for being here and welcome to the podcast.\n\nGrant Lee (00:05:19):\nLenny, it's so great to be here. Thank you for having me.\n\nLenny Rachitsky (00:05:21):\nI see your face all the time in my LinkedIn feed. I don't know if you know this is a thing. On these JPMorgan Chase ads. I'm so curious if other people see this or if it's just me. Did you know this was a thing?\n\nGrant Lee (00:05:31):\nI think it's maybe once a day now I get a text message and just no message. It's just a screenshot or an image of me doing something in San Francisco on one of these ads that we're seeing. And so yeah, kind of embarrassing, but also we're happy customers of JPMorgan Chase, so trying to represent.\n\nLenny Rachitsky (00:05:49):\nOh my God, I hope you love them. Because it's always you. There's no one else. It's like Grant.\n\nGrant Lee (00:05:54):\nI know. Can I talk to you [inaudible 00:05:56] swap somebody out. I mean that'd be great. I'm totally fine with that.\n\nLenny Rachitsky (00:05:59):\nOkay, so to get serious, the reason I'm really excited to have you here is, unlike a lot of super fast growing AI startups, you are both growing like crazy, you are growing very profitably. We're going to talk about this. You did not raise a ton of money when you started. You waited a long time to raise a bunch of money. You also built a business in a category that I think most people never imagined there was this big of an opportunity. And you're basically, you've achieved the dream of a lot of founders these days, especially people building AI startups.\n\n(00:06:31):\nSo my goal with this conversation is essentially do an anthropological study of a really successful AI startup. Talk about how you found product market fit, how you grew, all the lessons you've learned along the journey. And I'm going to break this conversation up kind of along the different milestones of the journey. Before we get into the first piece, is there anything that you think is important for people to hear broadly about the story of Gamma?\n\nGrant Lee (00:06:57):\nYeah. Maybe I'll just start with a quick story if that's okay. And it's really just the founding story. So we started the company back in 2020. This is peak pandemic. And even fundraising was just so different. So all of the fundraising was done over Zoom. You were kind of sitting in these Zoom meetings trying to pitch. Many investors you never met in person. So just a different era. And so for us, we're first time founders. I was actually living in London at the time, and so different time zone. I had to do all of my pitches at night. And I have two little kids, so wait for them to go to bed. 8:00 PM. We had a pretty modest flat, so nothing big. I would basically find this little corner between the kitchenette and the laundry room to kind of set up shot, far enough from the kids so they wouldn't be woken up.\n\n(00:07:45):\nIn between 8:00 PM and like 2:00 AM, I'm just pitching. Trying my best. I had the fake Zoom background so people didn't know where I was, and just pitching. And so really the first day, I'm in my third pitch in, trying to tell the story of Gamma, obviously just starting to get the hang of the pitch. And I get to the very end of the pitch feeling pretty good about myself. And the investor pauses a little bit, and then just says, \"That has to be the worst pitch, worst idea I have ever heard. Not only are you trying to go against incumbents, you're going against incumbents that have massive distribution. You are never going to succeed.\"\n\n(00:08:23):\nAnd so in my head, I'm already kind of shell shocked and thinking what's my rebuttal? And before I could even respond, he hangs up. And so I'm there sitting there thinking about it. And before I could really get down on myself because I had to prepare for the next pitch, I just internalize this feeling that maybe he's right. Maybe something about what he's saying is actually correct. And so for me, I started thinking about, if we're going to succeed in this category, we're going to really have to think about growth from the very beginning. This category is going to be really, really hard to break into.\n\n(00:08:58):\nAnd so we really kind of made this sort of promise to ourselves, that as we continue to build, growth was going to be critically important. And so my thing to your audience is that I don't come from a growth background. So if I can learn growth, anybody can learn growth. And I think especially in this sort of market, hyper competitive, oftentimes very crowded, it's going to be essential.\n\nLenny Rachitsky (00:09:19):\nThat is such a fun story. Oh my god. How bad must this investor feel at this point. We won't name names. Just to share some stats, I know this is going to be by the time this launches, this will be out, but you guys are at over 100 million ARR now, worth over $2 billion. A business that, again, most people did not think was going to work in this category.\n\nGrant Lee (00:09:41):\nYeah, thank you. Yeah, we feel super proud to have accomplished that. And again, yeah, I'm excited to share some of the growth tactics and things that worked for us because I think hopefully it'll help others kind of on their journey as well.\n\nLenny Rachitsky (00:09:53):\nOkay. So let's dive into it. Let's talk about product market fit. Tell us the story of just how you found product market fit, and how you knew you found product market fit.\n\nGrant Lee (00:10:02):\nYeah. I'll start by telling kind of the moment where we thought we maybe had product market fit. And I think a lot of founders ask themselves, do we have it or are we not? And I think there's often a sort of temptation to kind of almost fool yourself into thinking you have it. And so we sort of did our first public beta launch, this is back in August of 2022. We launched on Product Hunt, and felt really good. We had what we felt like was a great launch, ended up winning product of the day, product of the week, product of the month. And it was like, wow, I think we have something here.\n\n(00:10:32):\nAnd then we'd look at signups, and you'd get that initial spike in signups, and then they sort of flatten out. We were still getting new users every day, but it was clear we didn't have strong word of mouth. There wasn't strong organic virality. And so if we just kind of played things out, we knew that the product wasn't going to grow on its own. Something was missing there. We didn't have that strong word of mouth so that the product could just continue growing.\n\n(00:10:55):\nAnd so we really asked ourselves, okay, what do we need to change? And the answer is we need to fundamentally change everything. It for us almost became this sort of bet the company sort of moment. Because at that point we were running low on runway. We knew we needed to make progress and we didn't really know what could be done. And so we got everyone together. At this point, the team was just over 12 people. And we said, okay, it's going to be all hands on deck.\n\n(00:11:22):\nWe are going to do everything we possibly can to make the first 30 seconds of the product feel magical. The moment you land into the product, it has to be great, and it has to be so great that someone that goes through that onboarding is going to tell all their friends. And if we can get that right, then maybe we have a chance at actually doing something in this space. And so we spent three, four months actually after the product launch, we felt great, but we knew we had to go back to the drawing board.\n\n(00:11:48):\nWe spent the next three, four months actually revamping the entire onboarding experience. And of course, this is also where AI for us kind of played a big role. We actually rebuilt it so that AI was part of the actual onboarding. So every single new user would experience this sort of magic in the first 30 seconds. And so we relaunched, this is end of March 2023. And all of a sudden, we'd go from a few hundred signups a day to now first day it was like a couple thousand, and then the next day would be like 5,000 signups, and then 10,000 signups a day, and then 20,000 signups a day.\n\n(00:12:21):\nAnd then it just kept going up. And we weren't doing any sort of marketing, no advertising. It was all sort of organic word of mouth virality of the product, people using the product and sharing it with others, where we for the first time really felt this pull. We didn't have to do anything. Product was just growing. And it was just such a distinct difference between that feeling and coming out of the Product Hunt launch where we could have fooled ourselves into thinking we have product market fit. I think the temptation would've been, hey, let's just spend more on ads or spend more on marketing. Because we'll just fuel the top of the funnel and everything else will work itself out.\n\n(00:12:53):\nI think that would've been a trap. I think that would've led us down to this path of trying to brute force our way into product market fit. And it would just always be sort of a fleeting sort of destination. We would never actually arrive. And so I think we made the tough call, the right call. It was a sort of bet the company moment, and I think on the other side it just felt so different.\n\nLenny Rachitsky (00:13:11):\nGrant, this is exactly what I wanted this conversation to be. I'm so excited. I have so many questions I have to follow up on the stuff you shared before we even get to the rest of the journey. So one is essentially what you're describing is product market fit to you was when organic growth started to really take off, and it was just growing through word of mouth. You weren't doing much because it was so awesome, people were telling their friends about it. Is there anything more there that might be helpful for people to share just to hear about just like, okay, here's what it actually looks like?\n\nGrant Lee (00:13:38):\nYeah, I mean my one piece of advice is when you're early on, your mindset should almost be like you're trying to create a word of mouth machine. If you can get that part right, everything else becomes significantly easier. And if you have any, and I think this applies to both prosumer, B2C, as well as even B2B products, if you have a B2B product, even if you're not telling all of your friends, you should be telling colleagues where that product is relevant. You should probably be telling former coworkers where, hey, you've discovered something like, oh, I wish we had this in our prior lives, and that should even be magical.\n\n(00:14:14):\nAnd then you should see that in all the leads that are coming through or people coming through through your prospects and your existing customers. If you're not seeing a healthy chunk of those leads come through that way, I would go back. I'm like, why? Why is that not happening? Because again, that's the massive tailwind you need where every single thing you do on top of that, all the marketing, all the sales, all the advertising, you're just going to have it becomes way, way easier.\n\nLenny Rachitsky (00:14:36):\nHow much of this was, you described it as a word of mouth machine, how much of this was word of mouth loops and virality features versus just the product itself? One was awesome and two is kind of innately shareable because it's presentations people share with each other.\n\nGrant Lee (00:14:50):\nYeah, totally. I think for us, we do benefit from being in a category where, by nature of it, if you like Gamma, you're sharing it, presenting it to others. So I think for us it's a combination of both. And ideally, you have other ways where word of mouth or organic virality can happen in your product. So by nature of usage, like it's being shared.\n\n(00:15:09):\nWe basically had an internal mantra that, we go back to the first 30 seconds, we want it to be dead simple for someone to create content, we want to be dead simple for them to share it. And everything we did for that first 30 seconds, or call it the first few minutes, is remove friction so that they can do both of those things. Create and share. And I think other people, when you look at your own product, you think about, okay, what is it about my product and how it gets used? Can you remove friction such that it can actually spread, and even if it's locally within an organization or within a workspace, just be able to enable that as much as you possibly can.\n\nLenny Rachitsky (00:15:43):\nThe other really profound point you're making here is the story of you won product of the day on Product Hunt, which alone is so hard. So many people try to win and don't. Most people don't. I've tried to help companies win, and it's a really hard thing to achieve. And then you won product of the week and product of the month, and still you're like, no, this isn't working. Most people that achieve that are like, no, we got this, and they would not have to bet the company. There wouldn't be a feeling that we have to rethink everything. What is it there that you're just like, no, this isn't going to work, as much as exciting as this is, this isn't it?\n\nGrant Lee (00:16:20):\nYeah, I mean part of being a founder is being as self-aware as you can and be your own worst critic. And so oftentimes you want to have these vanity metrics that feel good to celebrate, and you should celebrate. But you should know when it's a vanity metric versus is this core to our growth engine? If this number goes up, does it mean the product is working? And I think that's where we looked at, okay, it felt good to win those things. We kind of put ourselves at least on the map. But it wasn't good enough to actually have this sort of feeling that we had a core growth engine we could just invest in and get better and better. That wasn't there yet.\n\nLenny Rachitsky (00:16:53):\nSo essentially it kind of started to just plateau and slow. It wasn't like this rocket ship that took off from that point.\n\nGrant Lee (00:17:00):\nYeah, it was still like we were still getting signups. They were coming through. But you could just tell there wasn't this building momentum. And I think that's where it's always hard to tell. You have to, me and my co-founders, we sat down, we're trying to be honest with ourselves. Okay, is this going to be enough? And it just really felt like it wasn't going to be good.\n\nLenny Rachitsky (00:17:17):\nThe other point here is the power of onboarding, which comes up a bunch on this podcast when you talk about driving retention. So you launched Product Hunt, did great and then started kind of petering out. How much did the product change after things started to work versus onboarding? Just like how important was onboarding? And then just tell us why the first 30 seconds, where'd you come up with that number?\n\nGrant Lee (00:17:41):\nYeah. So for us, the onboarding and the product experience, for us that's intertwined. The analogy I always think about is if you go into a restaurant and maybe the food is good, but when you really think about the user experience, it's like the moment you walk into the door, you get seated, the waitress, waiter comes by, greets you, you can order. And of course the food has to taste good. And then you finally get the bill and you leave. Is that entire experience something that feels delightful? Is it good enough for you to tell your friends about? If someone just came by and dropped the food on your plate on the table, and just left and never came with a bill, I'm like, okay, maybe I'm not going to recommend this to somebody else.\n\n(00:18:18):\nAnd so for us, we thought about, okay, the first moment someone walks through our door, dropping into the product, what is something we can give them? Can we shorten that time to value as much as possible? A lot of this is inspired by Scott Belsky, he talks about that first mile, the first 15 minutes. And I think that's totally right. And I think one approach is you think about new users as you almost have a cynical view of them. You have to think about them being selfish, vain, and lazy, right? They're coming in, they have no desire to learn a new tool.\n\n(00:18:48):\nAnd so what can you give them in that first 30 seconds that earns you the next 30 seconds and then the next 30 seconds? And so for us, we knew that if we can't, people's attention span is even shorter to today than maybe 10 years ago, and so what is it in that first 30 seconds, can we actually show you something and earn the right to keep building that relationship with you? We really thought a lot about that, and certainly that's all we could really afford at the time. We only had 12 people building. It's like we couldn't make an entirely revamp the entire product. We knew that we had to least put all of our energy into one spot, and so we made that coming into the door, come through the door, make that moment feel magical so that we can do a little bit more over time.\n\nLenny Rachitsky (00:19:27):\nI love your point about how you could think of it as like, okay, it's onboarding versus the product. The lens of how do we make this incredibly valuable and aha-ish for the first 30 seconds almost informs what the product should be.\n\nGrant Lee (00:19:39):\nYeah. It really helps you pull forward what is the most magical thing about your product. Sometimes founders will think about the five, 10 features. Well, maybe there's only one thing that kind of differentiates you. I try to learn a lot from, we'll get into some of the marketing pieces of this, but even just having this sort of founder led marketing lens of what can I do to help a new user just understand. There's this thing from consumer advertising, which is you throw a consumer one egg, they can probably catch it. You throw them four or five eggs, they're probably going to drop all of them.\n\n(00:20:12):\nAnd oftentimes founders want to talk about the four or five features they have, maybe 10 features. And then the consumer is totally confused, like why do I need this thing? We try to just give them that one egg, that one first experience. We're like, okay, create a slide in seconds. That's the egg. I'm going to throw you this egg. Is that compelling to you? Some people are still going to opt out, but for the people that catch that, you're solving a real problem for them, and then you can continue building on that over time. You've given them enough so that they'll sit around and keep playing with your product.\n\nLenny Rachitsky (00:20:41):\nThat is a hilarious metaphor I've never heard for onboarding time to value, just focus on one egg at a time. Just going even further back, what was the original insight that you had that led to Gamma and what Gamma is today?\n\nGrant Lee (00:20:55):\nAfter the last startup I was at was acquired, I went back into kind of my roots, which is consulting. I was advising early stage startups. And the sort of medium I was using was Google Slides. So I just remember this late night trying to prepare for next day's meeting, trying to format and figure out the right layout, and spending hours just trying to get the sort of look and feel right, rather than the content itself. And for me, that just felt completely backwards. I should be spending 90% of the time on the content, 10% maybe on the design and formatting.\n\n(00:21:25):\nAnd so the question just was what if there's a better way? What if we could reimagine this format from the ground up? Slides have been around for almost 40 years as the default medium of choice for a lot of this. And so we thought about, okay, if we had different building blocks, different primitives, so you're not locked into the fixed 16 by nine slide, what could we offer to users? And so that was really the starting point of all this.\n\nLenny Rachitsky (00:21:48):\nHearing this, I could see why investors would be like, I guess so. But slides has been around, PowerPoint has been around 40 years. I get it. I get why people would be... And specifically AI, was that a part of the vision initially, or did AI start to come up, and then wow, great timing,\n\nGrant Lee (00:22:04):\nGreat timing. It wasn't part of the original vision, although the spirit was there, which is we wanted to make it incredibly fast and effortless for people to create content. So it just so happened that AI was a magical gift that allowed us to do all those things along the same sort of ambition or vision that we had. And so we integrated it core to all the building blocks we were already building well before AI was part of the picture.\n\nLenny Rachitsky (00:22:27):\nIt's such a cool other example. There's just so many examples of ideas that were not possible before are now very possible with AI. And it's a great opportunity for people to come after as these places, categories, people think is an impossible place to build a big business. AI now allows it. Awesome.\n\n(00:22:43):\nSpeaking of that, let's talk about the growth journey, and how you actually grew from nothing to 100 million ARR in just over two years. I'm thinking we break it up. I know these milestones aren't that clear, but kind of like zero to 100 million ARR, one to 10, 10 to 100, something like that. And let's just see how it goes. How did you get your first set of users? How'd you get your, say, first 100 users? How'd you get to 100 million ARR from zero?\n\nGrant Lee (00:23:10):\nOur first 100 looks very different, I'd say. So this was even pre this sort of AI launch we had. The first 100 users for a product like ours, you're trying to convince all your friends to use the product. Anybody that's ever made a slide deck, you're trying to talk to. And I think early on, your friends want to do you a favor, so they're going to try the product. They're also going to lie to you. They're going to tell you how great it is. And then you look at the usage and nobody's coming back.\n\n(00:23:33):\nAnd so I think our first 100 was sort of gradually hard-earned post the product launch, people learning like, okay, this is kind of becoming a little bit more useful. Usage was still pretty episodic, so they weren't coming back every week. And then I do think the moment post the AI launch is where all of a sudden we saw that sort of organic growth happening, people coming back to the product regularly. And so that's where, it wasn't even the first hundred, it was like probably the first 10,000 users all came within a pretty short time period after that initial launch.\n\nLenny Rachitsky (00:24:02):\nAwesome. We're going to talk about monetization pricing later, which is obviously an important part of actual getting to million ARR and 10 million AAR. So what I'm hearing essentially is the Product Hunt launch was a big part of just the first 10,000-ish users. I know there was also a tweet when you relaunched that helped in a big way. Talk about that.\n\nGrant Lee (00:24:24):\nYeah. So when we did our AI launch, we didn't do our AI launch on Product Hunt. We basically said, hey, let's just put it out on Twitter, see if we can get some virality. And honestly, we kind came up with kind of a clickbaity sort of tweet. It was like the most valuable skill in business is about to become obsolete. And so it was intentional in that we wanted to create a little bit of engagement. We knew that having sort of a more provocative in a tweet would allow people to engage with it.\n\n(00:24:54):\nAnd so after a couple of days, all of a sudden it started getting a little bit more viral and a lot more engagement. And we looked and it was basically because Paul Graham had commented and saying something like, \"Surely, the thing that the slide deck is describing is more valuable than the slide itself.\" And obviously it was fun just to see that comment. I think once that comment came through, even more engagement on the post. And then that was really the whole intent of that post was just to be able to have that level of engagement so that people, it would have some level of reach.\n\n(00:25:24):\nAnd so for me, it was almost like my first learning moment, going back to what does founder led marketing even mean? It means how do you actually break through the noise? How do you get a chance to have people even engage with a post like that? Part of that is copywriting, part of that is storytelling. Part of that is just having even the right visuals to share. And so it was for me kind of a moment just understanding, hey, to kind of do this right, you kind of have to do things that maybe you're not super comfortable with, but it makes a difference.\n\nLenny Rachitsky (00:25:52):\nSuch a fun story. So you intentionally set that announcement up to be controversial is what I'm hearing?\n\nGrant Lee (00:25:58):\nTotally. Yeah. I'd say provocative. A little spicy.\n\nLenny Rachitsky (00:26:01):\nThat is so cool. So essentially you got to 10,000 users through Product Hunt, and then essentially one controversial tweet that ended up baiting Paul Graham to comment.\n\nGrant Lee (00:26:11):\nTotally.\n\nLenny Rachitsky (00:26:12):\nAmazing. And it was just a comment. It wasn't even him retweeting it.\n\nGrant Lee (00:26:16):\nNo, just a comment. And then others would pile on.\n\nLenny Rachitsky (00:26:19):\nYeah. It's interesting how much a comment can increase the distribution of a tweet versus them retweeting it or quote tweeting it.\n\nGrant Lee (00:26:26):\nTotally. And of course the algorithm change all the time. So part of it was just luck based on when it happened, how it happened, who posted.\n\nLenny Rachitsky (00:26:32):\nAnd you use this term founder led marketing, which I love, and I'm already seeing it in action here. This is you thinking about, it's not delegating to someone in marketing, it's not hiring an agency, it's like how do I tell a story that I think will break through the noise based on you building this company, having the insight to build this product. And I guess is there anything more there you think is important for people to hear about the importance of the founder thinking through this stuff?\n\nGrant Lee (00:26:56):\nYeah. I mean, I think most people today are probably familiar with founder-led sales, which is still very, very important. I think before you hire your first salesperson or AE, it's great for the founder to understand what it takes. And they're going to craft the right narrative, the right story. At my previous role, I was the COO at a startup where I was doing a lot of, I wasn't founder, but I was early. And so I was helping the founders go through this, and really helping go into meetings with a client or a prospect and saying, \"Hey, this is why our product is interesting.\"\n\n(00:27:26):\nAnd I think today there's so many AI startups that are much more either B2C or prosumer. And so you're not necessarily talking to individual prospects, but the idea that you can be really in control of the narrative on the marketing side is really, really important. And I think I'll describe a few things where, over time, I think that skill set just really, really helps you.\n\n(00:27:48):\nOne is you have a chance to be a creator yourself these days. I think a lot of founders are trying to be more active on social media. And I think if you can kind of overcome the initial cringe factor of seeing yourself and postings like, oh, this doesn't feel authentic, if you can overcome that initial feeling, you start investing into like, okay, how do I become a better copywriter? How do I articulate something that is clear, not just clever? I think there's that saying where obviously if you can have that clarity, that's super important. And most people will try to get super creative with their copywriting, but that's not usually the right way to break through and communicate something. So how do you improve your own copywriting?\n\n(00:28:27):\nAnd then that allows you to actually have a higher bar when you start working with other marketers, or in this case for us, working with influencers. If you're working with influencers and creators, and you can totally empathize with how they approach that work, and you know what a good hook looks like or you know how to structure a good post, you can only do that if you've gone through it a little bit yourself and you know how hard it is. And I think too many founders will then just say, they'll write something that just feels so much like an ad, and then they'll give it to a creator to help amplify. And then that just never works. And so I do think part of founder-led marketing is going through this yourself.\n\nGrant Lee (00:29:00):\nAnd so I do think part of founder-led marketing is going through this yourself, using your own platform. In the beginning it's probably going to be super small. But as you get bigger, you have a platform to ... You have a voice and people listen. And you're going to get better and better at your own storytelling. I think these are all skills you should invest in as early as possible, because you know you're going to have to get better and better. It's like practice, you got to practice over and over.\n\nLenny Rachitsky (00:29:21):\nI definitely want to pull on this thread more because you tweeting the lessons you learned building Gamma is what led to this conversation. I was reading, I'm like, \" Okay, he's sharing a bunch of stuff, but there's so much more I want to hear.\" And we're going to talk through this and go in a lot more depth than what you've shared on Twitter. But I love that that's example of that working, having this conversation.\n\n(00:29:41):\nSo let me ask a couple of questions here. One is just how do you find time as a founder or CEO of a very fast growing crazy startup? We have so much to do. How do you just allocate the time to do this? And then any just key lessons you've learned about doing this well, beyond what you've already shared for people that want to try to start sharing things on LinkedIn and Twitter?\n\nGrant Lee (00:30:00):\nMy advice is definitely just to try to start small. Don't let it become so intimidating that you just don't get started. For me, it was like just having a notepad or a Google Doc around in the beginning where I would just constantly jot down, okay, this is something I learned or something I observed or something that worked well, something that was unintuitive but worked, and just start creating a log of that. And then once I had enough of those, and I'd spend basically every week, I'd block off a few hours to go a little bit deeper. I'd take a lot of those bullet points and try to say, \"Is there enough here to turn this into maybe a post or something that can be shared broadly?\" And in the beginning I didn't have enough. It was all sort of scattered thoughts. But over time you start accumulating some interesting themes. And then I would start stress testing some of that.\n\n(00:30:45):\nSo I would tell my teammates like, \"Hey, this is something interesting. Do you find this interesting?\" And if there were enough like, \"Oh yeah, I would not have expected that\", or, \"That's not something I've ever heard before,\" then I'd actually start crafting the initial post. And then you actually just put it out there. I think what I've learned is, even for LinkedIn versus Twitter, the audiences want different things. And so you almost have to then have different tones of voices or even nuggets or sharing. For me, I invested much more in LinkedIn early on just because it felt a little bit more natural for me. And then over time I said, \"Okay, well I'm going to start packaging certain content for Twitter that's actually different than what I would post on LinkedIn.\" Sometimes on Twitter you get even more tactical or even more into the weeds. And so I found that to be helpful.\n\n(00:31:27):\nBut honestly, I'm still learning. And so every time you post, you go back, after a couple of weeks you go and say, \"Okay, what things are actually being engaged with? Are things actually creating ...\" Ideally you're creating enough value where people are either bookmarking it, sharing it, retweeting it, these things that are signals for there's something valuable there. And then you just go back and you start collecting your own sort of, these are my all-star posts, these are the ones that I've actually broken through. And then you go back and try to understand, okay, what about that post do I think was actually useful? Was it the actual content? Was it the structure of the content? Was it some sort of contrarian advice? And you start thematically bunching that together such that as you're brainstorming every week, you just have a good sort of body of work to work off.\n\nLenny Rachitsky (00:32:10):\nThis is so interesting and valuable. So let me mirror back a few of the lessons that I heard here that I think is easy for people to miss. So one is just what to share. What I heard here, and I completely agree with this, and this is what I try to do, is pay attention to things you've learned, things that you find interesting, things that are unintuitive to you. Just have a doc and just put these there. And every time you learn something, find something interesting, just add it to the doc. Or yeah, I haven't heard before is a good one too.\n\n(00:32:39):\nSo it's essentially just like if you find it interesting, people on social media will also find it interesting. And one approach is just share it as it's happening, which is what I try to do. Just like, \"Oh hey, just learned this thing with with clock code. Check it out.\" Or save it up for a big long post. The other interesting, I've never heard this before, of post different things to LinkedIn and Twitter. I just copy and paste the same thing. I love that you do something different for the two platforms.\n\nGrant Lee (00:33:05):\nI think we all kind of have intuition that there's just different audiences, right? And so if you know that kind of fundamentally, then the question is how do you package up the story the right way so that the audience is ready to receive it? And I think this can differ by the type of creator or the founder, whoever's posting it, and of course the actual content itself. And so for me, I'm still tweaking, but I do find that just copy and pasting from one to the other doesn't usually work. You almost need to be in the right mindset of, okay, what do I think will be more engaging on Twitter? And then what do I think will be more engaging on LinkedIn? And then test a bunch, see what actually works. Go back and iterate a little bit.\n\nLenny Rachitsky (00:33:49):\nSo if you had one bullet point tip for what works on Twitter versus LinkedIn, you shared maybe more tactical on Twitter, is there anything more there you can share?\n\nGrant Lee (00:33:56):\nYeah, that's what I've found is tactical, oftentimes more contrarian on Twitter. And also I would say technical too. People really like to know, again, going back to getting into the weeds, is this something I feel like I could replicate? And I'm not going to give you ... There's no credibility if you just give a blanket statement or something that feels generic. I really need to know, if you could show me the metrics, even better. I feel like that ...\n\n(00:34:22):\nVersus LinkedIn, it's oftentimes more even just either more aspirational or aspirational or a topic or a theme that just feels relevant at that point in time. And you can just kind of make more of a broader statement. It doesn't need to be as tactical. It's more like inspirational, is like, \"Oh, okay, now I need to go and learn a little bit more about pricing and packaging,\" for instance. And that could be the sort of spark that somebody needs. And you don't need to spell it completely out. Part of it's also that on LinkedIn you can't really do threads. And so doing a super long form post isn't as practical. Maybe that changes in the future, where maybe the tactical pieces, that element might actually change.\n\nLenny Rachitsky (00:35:01):\nAnd last piece is you said you just block off time. Is there a specific time of the week you do this? How do you actually ... Because everyone's like, \"Oh sure, I'll block off time. And then, I don't know, okay, but I actually got to do all this other stuff so I'm not going to use it this time or maybe next week.\"\n\nGrant Lee (00:35:12):\nFor me, it's usually two times of the day, very first thing in the morning and last thing at night. And partly it's because of kids. It's almost like I need time where there's just zero distraction and there's no noise in the house, and so I can actually think. And then I think in the mornings it's about where are you finding inspiration, what are topics you're energized by? And then I think at night it's about reflection. What are the things you actually went through that day? You can almost pull up your calendar and be like, \"Okay, I talked to X, Y and Z people. And was there anything from those conversations that might be relevant?\" That's where I write some of those things. It's more of a recap of actually what happened.\n\nLenny Rachitsky (00:35:50):\nAnd what helps me to not feel like this is some cringey self-promo egotistical stuff is just it's useful stuff that I've learned that ends up being helpful to people. And people in the comments are always just like, \"Oh, that is really cool and useful. Thank you.\" It's not like self-promotion-\n\nGrant Lee (00:35:51):\nTotally.\n\nLenny Rachitsky (00:36:05):\n... it's not just like, \"Look how amazing I am. Check out my amazing products.\" Like, \"Here's a thing I learned. You might find it useful.\"\n\nGrant Lee (00:36:11):\nThat's exactly right. I think one way of thinking about it, with founder-led sales, it's always about exchange of value, right? You want to be able to give the customer this feeling that they're getting an amazing product. In exchange they're going to pay you money for it. I think with founder-led marketing, it's almost this mindset of you want to give people a ton of content, maybe it's a value in the content, so you're sharing something, maybe some secret tactic or you're giving them something where inherently there's value in it, and in exchange you sort of get goodwill back. You're not necessarily getting money back, you get goodwill. They're going to follow you, they're going to engage with your post, they're going to tell others about it. And then over time you can exchange maybe some of that goodwill for actually talking about my product and announcing it and they're going to help amplify the news. And I think that's magic, where you kind of bank the goodwill for a long period of time by providing just a ton of value with no expectation of anything immediately in return.\n\nLenny Rachitsky (00:37:06):\nThe book I always point people to when they're struggling with this sort of thing and like, \"Okay, I did this and no one cared, didn't do any good,\" is there's a book by Scott Pressfield, I think is his name, called Nobody Wants to Read Your Shit, which is exactly what is right. Nobody wants to read it. The bar for people to care is very high. There's so much stuff to read and process. And so this book gives you a really good lens of just like, okay, the bar is very high and nobody wants to read your shit, so you have to try really hard to make it really good.\n\nGrant Lee (00:37:38):\nGreat reminder.\n\nLenny Rachitsky (00:37:39):\nWe'll link to that in the show notes. Okay, let's come back to the growth of Gamma. So we've talked about how you got your first tens of thousands of users, essentially product hunts, rethinking, onboarding, making it really magical, and then this very controversial tweet that Paul Graham commented, created some buzz. Let's talk about the next phase and maybe, I don't know, tell us kind of the ARR at that point through 100 million. Just broadly, what should we know?\n\nGrant Lee (00:38:07):\nSo when we got to about 10 million in ARR, I think there was this feeling for me, which was we knew we needed ways to help just continue to amplify and spread the word about Gamma. I think it was already working in terms of the organic virality was there, and so we did feel like it was time to start amplifying some of this. And I think the main blocker of my mind that I started feeling was that our initial brand was holding us back. And I think a lot of people will discount whether or not a rebrand is valuable. And I think sometimes it is, sometimes it isn't.\n\n(00:38:39):\nFor us, there's a few different things we looked at. So one, our initial brand was almost more of a placeholder brand because we created it the moment we incorporated the company, which was again late 2020, beginning of 2021, where we needed something so that as we built, we could at least share it with people. We could put up a landing page and just feel like, okay, there's something here. But we didn't invest a whole lot into it. And so it was pretty limited in sort of what I call the DNA of the brand. There wasn't that many ... The art direction was very limited in scope. There wasn't much when it came to voice and tone.\n\n(00:39:12):\nAnd so it was something that we knew was good enough to start, but it wasn't scalable. And when I think about something that could be scalable, it's almost like you can take the ingredients of a brand and replicate it a ton, this DNA is something where you can imagine creating tons of content around and all feeling pretty cohesive. And I think that needs to be done by design. You're really being thoughtful about every single element. Like what is the art direction you want to go with? What is the voice and tone? Such that as you're creating thousands of pieces of copy, it all feels pretty cohesive.\n\n(00:39:45):\nAnd so we went back to the drawing board and we spent many months rethinking what would be the brand, what is this vision that we have longer term? Our creative director internally partner with Smith & Diction, an amazing agency that has helped folks like Perplexity also do their rebrand or their initial brand. And we [inaudible 00:40:05] many months just really trying to craft what we think is the core DNA of the brand, and doing so in a way that we could replicate it as much as possible.\n\n(00:40:15):\nReplication piece of it comes into play, because as you start scaling you're going to have to create a ton of content, your own content on social media, ads for performance marketing, assets for influencers to be able to use and showcase in their content. And so you're going from tiny pieces of content to all of a sudden every week we're testing thousands of pieces of creative. And you cannot do that if you don't feel confident that as you're replicating, you have that sort of cohesive feel. So for us that we realized it was going to be necessary and it's why we invested so much. It ended up being way more expensive, way more time consuming than I would've imagined. But I think coming on the other side of it being the right investment, feeling that that was the right time to do it.\n\nLenny Rachitsky (00:40:55):\nI love how many things you did that feel like this will not work out. Building a startup within the presentation space, doing a whole rebrand in the middle of scaling, also just reworking the entire product after you launched and just rethinking the whole thing. All these things and everyone's always like, \"No, this is not how we win.\" And interestingly, worked out for you guys.\n\n(00:41:19):\nI want to come back to the brand stuff, but one of the most interesting ways you guys grew early on was influencer marketing, which a lot of people hear about and talk about. I haven't heard much of how to actually do this and what actually works. Talk about that as a broad growth lever for you guys. And then I want to get into just what tools did you use, who actually was really helpful there, thing like that. So yeah, just give us the big picture.\n\nGrant Lee (00:41:44):\nYeah. I think a lot of founders assume that with influencer marketing it's almost like turnkey. You set aside a budget, you find some creators, you figure out the right campaign or the right moment of time to do it and it's all done, you're ready to go. And I think the reality is, going back to this founder-led marketing mindset is like, well, you're going to set yourself up for success if you actually are super involved in that entire process. So for us, what this meant was all the initial influencers I onboarded manually myself. I would jump on a call with each one of them so that they understood what Gamma represented, how to use the product. You want to be able to have them tell your story but in their voice, right? And they can't do that if you're not willing to put in that investment.\n\n(00:42:32):\nAnd so we would spend a lot of time going through ... It wasn't my job to tell them how to pitch Gamma, but it was my job to make sure that they understood what Gamma was as a product. And so we'd spend a lot of time, like me just walking through the product, them asking questions, us just kind of brainstorming what could the hooks be, and me just giving them some initial feedback and saying, \"Oh yeah, this one, I love that. I figure it's going to work great for your audience.\" But not trying to be super prescriptive. And working with a ton of micro influencers, people that don't have massive followings but are committed to ... Going back to giving value to your audience. They're committed to giving value to their audiences. They want to be able to showcase tools that actually they would use or they are using. And how do you do that in an authentic way? You can't really fake that. You really need to spend the time doing that.\n\n(00:43:16):\nAnd just like you would onboard a customer, you onboard an influencer the same way. You want them to be an extension of your team. And I think they can feel whether or not you're willing to put in the work. And if you're not, then they're just going to treat it like any other project, ship it and be done with it. If you invest in that relationship, guess what? They'll be back to actually post about you again. And you're all of a sudden having this sort of this relationship that actually you can build over time. I think that's really where the magic is. Too many people discount that initial piece.\n\nLenny Rachitsky (00:43:47):\nThis is awesome. To be clear, influencer marketing, essentially a person with a following on say TikTok, Instagram, Twitter, LinkedIn, whatever, gets paid in some way to promote your product. That's the simple way to understand influencer marketing, yeah?\n\nGrant Lee (00:44:00):\nYes, that's definitely the simple way. And I'd say there's definitely different levels. I think a lot of people think influencer marketing and they'll think these big trendy creators, people that have a million followers for instance. And the idea is that, okay, we're going to carve out a really big budget, we're going to choose five or six that we feel like are really the tastemakers in the space and put all of our money into just having them talk about our product. And I think usually this is kind of the wrong approach. Because many of them, they do have massive audiences. And for you, you basically give them a script to read and it immediately feels like an ad. That product is not connected really to them in any way. It's just something that they're ... For this week they happen to be working with you and then they move on with their life. And it never feels organic or authentic and you wasted a ton of money doing so.\n\n(00:44:55):\nI think you're much better doing the hard thing, which is hard to scale, but it's finding the thousands of micro influencers that have an audience where your product maybe is actually useful. And for instance, for us early on it'd be educators, people that for them, part of their job is creating slides every day because they need to engage their students. And so for them, having a tool that actually saved them a ton of time was something they love talking about. And if you can find some of these pockets, we call them echo chambers, where if you find a pocket like educators, teachers love telling other teachers about products they love using. During summer break, they all come together and talk about, \"Okay, what are the things that are going to actually improve my job next school season?\" And obviously during this AI wave, a lot of those have been, \"Okay, what are the AI tools that just save me a ton of time?\"\n\n(00:45:43):\nAnd so if you can start actually tapping into these pockets of echo chambers, that's even better. It doesn't have to be this flashy, well-known influencer. It's actually just this person that has an audience where people really trust what they say. And that's amazing. That ends up becoming this sort of wildfire that can spread really, really fast.\n\nLenny Rachitsky (00:46:03):\nAnd what's the dollar amount these folks get? It's like a few hundred bucks, a few thousand bucks, something like that?\n\nGrant Lee (00:46:07):\nYeah, few hundred to low thousands, low single digit thousands.\n\nLenny Rachitsky (00:46:11):\nHow do you find these folks? Is there tools that you use? Is it just like a bunch of manual searching and looking?\n\nGrant Lee (00:46:17):\nYeah, in the very beginning it was all manual, a lot of cold outreach. And then we ended up finding a couple of different things. One is a platform, a YC company called 1stCollab. That has been amazing. They basically do all of the automated outbound for you. Plus you can help them actually create profiles or personas of different creators, so for instance educators being one, and then they'll go out and actually, based on that profile, find all the right creators for you. So they've been amazing, really great to work with.\n\n(00:46:46):\nAnd then we've also found small agencies to also help kind of augment that. I look for agencies that are super young and hungry. These are people that they're native to social media and so they really understand it. And they can really be able to bring in creators that are great to work with. And I think part of it is if you find creators that are great to work with, everything else becomes easy. So we've had a few, one is AKG Media, actually out in the UK, and they've been fantastic to work with as well. So you kind of find a few different things, either agencies or platforms that can help you actually scale this thing up.\n\nLenny Rachitsky (00:47:22):\nAnd when they post, they're generally transparent about this is a paid promotion, right?\n\nGrant Lee (00:47:23):\nYes.\n\nLenny Rachitsky (00:47:27):\nThey're not just pretending, \"I found this tool and I love it\"? Cool.\n\nGrant Lee (00:47:31):\nYeah, exactly right.\n\nLenny Rachitsky (00:47:32):\nOkay. And so how much impact did this lever of influencer marketing have on your growth, say from 10 to 100? Is this the biggest lever of growth other than just word-of-mouth, people continue to share it?\n\nGrant Lee (00:47:44):\nYeah, so word-of-mouth has definitely been the biggest. So we look at all new subscriber growth, over 50% of this is word-of-mouth. It's either people searching direct, coming direct, entering Gamma.op, or going through search and typing in Gamma, like a branded keyword search, where they're looking for Gamma, they've heard about Gamma. But I think for us, social media and influencers specifically has always been an amplifier. So every time we invest in influencer marketing, we actually see word-of-mouth increase even more. And it's always like you can just see it. Basically anytime you spend a little bit of money, you start seeing people come through influencer, the word-of-mouth factor actually will get another 1.5 additional users on top of that, which has been really interesting to see.\n\n(00:48:29):\nAnd I think part of this is just recognizing that ... And I think we kind of understand it, but with influencer marketing, why it's so effective, we all know Dunbar's number, which is have this number of 150 people that you call kind of your network. And your network you trust more than the average stranger down the street. If they tell you something, they recommend something, there is a sort of halo effect, you learn to trust them. A lot of these influencers, the reason why they share so much about their lives is because they want to be in your network. They want you to feel super close. And once you feel super close, you trust them to actually share things that are going to be useful.\n\n(00:49:05):\nAnd so when they recommend a tool, there is a sort of halo effect, where it doesn't feel like it's coming from a stranger, it feels like it's coming from a friend. And that's where every time we've spent money there you actually see this amplifications, like, \"Okay, that's kind of interesting.\" You wouldn't necessarily expect that. But for us it's been this sort of amplifier from the very beginning.\n\nLenny Rachitsky (00:49:24):\nThis is so fun to hear about. I've not heard this level of detail on how influencer marketing works and how to make it work. A few more questions here. So you said there's maybe a few thousand people you ended up working with, roughly, influencers?\n\nGrant Lee (00:49:35):\nOver the course of a year. It wasn't all in the same time. Basically in the beginning you do ... In the very, very beginning we had a small budget. It was like 20 creators a month. And then you start increasing that to like 50, then 100. We're definitely not fully scaled at this point, but I could easily see a point where you're working with many, many creators every single month. And that gives you a chance to actually test a variety of, again, content hooks, ways to talk about the product, value props.\n\nLenny Rachitsky (00:50:03):\nAmazing. And you said the key here is you spend time with every one of these creators, influencers early on to help make sure they understand what you're doing and get excited about it. It isn't just like a thing you outsource.\n\nGrant Lee (00:50:16):\nI think there's a lot of value there. It's again hard to quantify. And most founders probably feel like they're too busy to allocate that time. But I think it was a good investment. Because going back to you want them to feel like an extension of your team. They're not going to feel like an extension of your team if, one, they've never met you, and two, you've never even told them really how the product works. They're forced to go to your website to figure it all out. Those are going to be not a whole lot of love that they're feeling from the outset.\n\nLenny Rachitsky (00:50:47):\nSo what I'm hearing is quality over quantity, especially when you're getting started. And then there's this other piece of niche which I think is very counterintuitive. Instead of going to large influencers with a huge audience, it's good with folks that are small. What's like a audience size roughly that you think is ideal for this, or what niche just means?\n\nGrant Lee (00:51:05):\nHonestly, I don't think there's a minimum, because even with platforms like TikTok, they oftentimes are giving you credit for a brand new account. They want to help amplify that new account, because obviously if they're thinking from a creator perspective, if that new creator feels like, \"Oh, coming to TikTok is a massive win for me,\" they're going to be more invested in it. And so there really isn't a minimum. A lot of these platforms are trying to shift to kind of the same thing, where they really reward new creators on the platform. And so you could have a small audience, it doesn't really matter. You could have 10,000 followers, that's also good. I think as long as the content feels, again, engaging, authentic to the people you're talking to I think it has a really good chance of actually taking off.\n\nLenny Rachitsky (00:51:46):\nThat's such a good point with TikTok, where it's not follower related, if it's useful and people find it clickable or whatever, likable, viewable, the algorithm will spread it to a lot of people. Such a good point.\n\nGrant Lee (00:51:59):\nTotally.\n\nLenny Rachitsky (00:52:00):\nOkay, there's a couple more points you made in the tweet that I want to make sure we highlight. So one is you made this point that 90% of your reach in influencer marketing comes from less than 10% of people. Is there anything there that you think is important for people to hear?\n\nGrant Lee (00:52:11):\nYeah, I mean this just goes back to it's hard to know where that 10% is going to come from. So you kind of just have to cast a super wide net. You can sort of, I think try to, again, trick yourself into thinking you're great at picking creators or you're great at telling them how to post about your stuff. But the reality is, even for me, I could never guess. I kind of had some idea, but I just had to make sure that I was meeting enough creators broadly such that when you meet enough, they're all posting, there's some pocket that end up just taking off. And I was not a good predictor of that. I was not smart enough to actually know which ones would take off. I just knew that we had to play the sort of numbers game to make it work.\n\nLenny Rachitsky (00:52:49):\nSo one of your tips here is people fail often in this because they start too small or their budget's a little too small. You recommend 10,000 to 20,000 a month over six months and just trying this. It sounds like you're doing like 20 to 30 creators a month. Is that the right framework for how to just start this thing and explore?\n\nGrant Lee (00:53:07):\nYeah, totally. And I'd say it's not just that the budget's too small, it would be that they put all their eggs in one basket. So you can also easily blow that 20 K on just one bigger influencer and then be like, \"Oh, that didn't work, so I'm going to try it again. That didn't work. Okay, I give up.\" And I think rather than doing that, you should be like, \"Okay, that 20 K, I could actually probably work with 40 different influencers and see what actually works.\" And across the 40, I'm going to try to find a variety of personas, a variety of use cases, spend a lot of time with them, and then actually see what's working, what's not.\n\n(00:53:39):\nAnd then next month take those learnings and double down. If educators are working, go with educators. If consultants are working, go with consultants, find more consultants, there's going to be more there. I think that's where the putting all your eggs in one basket is just probably the surest way to fail, because you're going to miss a ton and it's going to take you way too long to learn and you're going to come to the conclusion that it's a waste of time.\n\nLenny Rachitsky (00:54:00):\nI feel like this whole podcast conversation could be just about this one lever. So much I want to keep talking about, but there's more questions here because this is so useful and interesting. You had this line in your tweet about how reality is not an accident and that this approach is how you figure out what actually works. And then once you do that, then you start leading into that messaging. Talk about that.\n\nGrant Lee (00:54:20):\nThis goes back to obviously if you're testing a bunch, you'll finally find that sort of post or set of posts that actually go viral. Going back to just the fundamentals, like make it easy for your influencers to be able to tell your story in their voice. One thing we did, we open sourced basically our entire brand. We have Brand.gamma.app, which is everything about our brand, our voice and tone, our art direction, what we use in mid-journey to create the sort of art direction that we have so that a creator can do the same, right? And they can actually just copy all of that so that they don't have to reinvent the wheel every time they're trying to post about Gamma. They have all of that.\n\n(00:54:56):\nAnd I think going back to this notion of just make it dead simple for them, remove friction. They already have enough on their plate to have to figure out. Don't make it any harder than it is. And if you remove friction, then it's like, oh, you get into this rhythm of adding creators is easy, having them post is easy, reviewing what's working, what's not is easy. And if you're able to do that relentlessly over many, many months, then all of a sudden hitting the sort of viral post is easy because you're going to have enough bats there where some are actually going to pop off. But you only can get there after you've done all the hard work before that to remove friction from the process. And feeling like it's almost like a well-oiled machine at that point.\n\nLenny Rachitsky (00:55:38):\nIs there a platform you find most helpful for the stuff you guys are doing? Is it like TikTok, Instagram, LinkedIn, something else?\n\nGrant Lee (00:55:45):\nYeah, I mean this is one where for us we cast a pretty wide net too. But it's very clear, LinkedIn, the conversion rates are just substantially higher. They're 4X, maybe 5X higher than other platforms. And I think a lot of people are probably still sleeping on LinkedIn, frankly. And so it's one where some of the influencers there or creators there can be a little bit more costly. But if you can eventually be more targeted knowing that, hey, this type of creator is pretty impactful for our product, then working with them, it's just like, \"Oh, that's great.\" The conversion rates are just so strong and it really feels like we're just getting started there.\n\n(00:56:22):\nSo if in the beginning you're not sure, it's always helpful to cast a pretty wide net. And then similar to just the influencer strategy, like test and iterate, you'll figure out ... Many of these things will follow a power law. So it's like one or two channels are going to be the most important for you. For instance, Twitter for us hasn't been that impactful. And I think for tools like Notion, they've been really, really impactful. You're not going to really know, and so just test and then double down on the ones that really move the needle for your product.\n\nLenny Rachitsky (00:56:48):\nI think that many people listening now are like, \"Wait, LinkedIn posts are sponsored sometimes? I didn't know that.\" How do you know if it's a sponsored post? Is it like a #sponsored or something like that? How do they communicate this?\n\nGrant Lee (00:56:57):\nUsually they'll say they're a partner or yeah, basically it is sponsored in some form, or they'll #ad or something along those lines. So that's probably the way you'll see it the most.\n\nLenny Rachitsky (00:57:12):\nCool. By the way, I don't do this sort of stuff. Have you ever seen me on LinkedIn? I'm not doing any paid stuff. It's just so people know. And I don't plan to do that.\n\n(00:57:19):\nThis episode is brought to you by Miro. Every day, new headlines are scaring us about all the ways that AI is coming for our jobs, creating a lot of anxiety and fear. But a recent survey for Miro tells a different story. 76% of people believe that AI can benefit their role, but over 50% of people struggle to know when to use it. Enter Miro's Innovation Workspace, an intelligent platform that brings people and AI together in a shared space to get great work done. Miro has been empowering teams to transform bold ideas into the next big thing for over a decade. Today, they're at the forefront of bringing products to market even faster by unleashing the combined power of AI and human potential. Guests of this podcast often share Miro templates. I use it all the time to brainstorm ideas with my team-\n\nLenny Rachitsky (00:58:00):\n... Often share Miro templates. I use it all the time to brainstorm ideas with my team. Teams especially can work with Miro AI to turn unstructured data, like sticky notes or screenshots, into usable diagrams, product briefs, data tables, and prototypes in minutes. You don't have to be an AI master or to toggle yet another tool. The work you're already doing in Miro's Canvas is the prompt. Help your teams get great work done with Miro. Check it out at miro.com/Lenny. That's M-I-R-O .com/Lenny. Let's come back to this brand point. So, one of your big lessons is invest in brand before you go heavy into hit paid ads and performance marketing. I imagine you do some ads at this point on Facebook and Google and things like that.\n\nGrant Lee (00:58:42):\nYeah, we run ads performance marketing. I think there's this stigma that brand marketing and performance marketing are sort of at odds with each other. I very much follow the sort of thought that brand marketing is performance marketing. Everything is some form of performance marketing. It just might not be as attributable, so the ability to actually map back to every single dollar spent is a little bit harder, but it doesn't mean that it's not impactful.\n\n(00:59:12):\nAnd I think as a company scales, you have to invest in both, and ideally they work really, really well together. The more you invest in brand marketing, it strengthens your performance marketing. This goes back to having enough creative to even test. If you're too limited in scope and you don't have a brand, you feel like you can actually amplify your handicapping, your ability to actually have a good performance marketing program.\n\nLenny Rachitsky (00:59:32):\nI love that heuristic of how do you know if you are under invested in brand is if you're limited in the number of ideas you can try in performance marketing.\n\nGrant Lee (00:59:32):\nTotally.\n\nLenny Rachitsky (00:59:43):\nIs your design system just... Is everyone having to redesign things from scratch and come up with all these frameworks every time they run an ad?\n\nGrant Lee (00:59:52):\nExactly. Yeah, yeah. Basically you kind of have a feeling for, \"If I were to scale this up to a thousand pieces of creative, would it still feel cohesive or is it kind of all over the place?\" And if it feels like it's all over the place, then you kind of have to go back to the drawing board.\n\nLenny Rachitsky (01:00:05):\nYou said when you talked about the rebrand, that it took a lot longer than you expected, that it was more expensive than you expected. That's the fear, I think, everyone has when they hear this like, \"Oh, I don't have time for a rebrand.\" I also imagine, because your product is so visual, that it makes more sense to invest there and to spend the time and money.\n\n(01:00:23):\nFor the typical founder, do you have any just, I don't know, thoughts of just like, \"Here's when it makes sense. Here's a sign you really need to invest here heavily.\" Versus, \"It'll probably be all right.\"\n\nGrant Lee (01:00:32):\nYeah. I mean, I do think it's probably more geared toward anything that's a little bit more prosumer or consumer because so much of your product, you're trying to create this feeling for a user, what are they experiencing. And the experience happens way before they even drop into your product. It might be they see an ad or they see a billboard or they see something. It's like, \"Okay, that piqued my interest a little bit.\"\n\n(01:00:56):\nAnd then you need some sort of symmetric messaging in that they see there's some symmetry in that, they see the ad, then they drop into the product or they land on your website, it feels cohesive and it feels like, \"Okay, this is interesting. I'm going to go all the way through to sign up and then maybe actually start using the product.\"\n\n(01:01:12):\nThat's a little bit different when it's a B2B product or where there isn't as much reliance on that initial moment. They might just hear about it through a colleague and then sign up for it and then go through a huge procurement process and then it's like, \"Okay, maybe it matters, but probably not so much as for a product where the brand can have so many different touch points.\"\n\nLenny Rachitsky (01:01:33):\nI want to talk about some broader things that have worked to help you grow, but before we do that, I just want to visualize the pie chart of how Gamma grows. Say, post-10 million ARR. If I have it correctly in my head, it's over 50% just word of mouth, organic, people are sharing it, doing presentations for each other, \"Oh, it's Gamma. Just go check it out, sign up.\" Then it feels like the second-biggest bucket is influencer marketing, social stuff. And then is the third performance/paid marketing?\n\nGrant Lee (01:02:03):\nYep, that's right. Yeah.\n\nLenny Rachitsky (01:02:04):\nCool. On that last piece, is there anything else there for people that are starting to explore performance marketing, essentially Facebook Ads, Google Ads, all these other platforms, is there anything else that you think might be helpful for people to hear or learn just to get started down this road?\n\nGrant Lee (01:02:19):\nI would just have two recommendations. One, going back to my initial piece of advice, which is don't invest until you have word of mouth. Don't fool yourself into thinking that you'll solve other problems by just starting to ramp up a performance marketing program. Just get the word of mouthpiece first, so that you're coming into this program with some tailwinds and then start ramping it up.\n\n(01:02:40):\nThe second piece is: set some constraints. You don't want your product to be at a point where more than 50% of your acquisitions are coming through paid acquisition. I think if that is happening, your core growth engine is broken. And it feeds right back to point number one, which is if your core growth engine is broken, you just have this leaky bucket. You're trying to spend so much money building top of the funnel, people are not making it all the way through. Something else should be fixed before you really try to dial it up. And it doesn't mean you don't spend a little bit of money, but just don't dial it up until you feel like your core growth engine is actually working.\n\nLenny Rachitsky (01:03:17):\nWhen you said the first point about wait until you have word of mouth before investing in performance marketing, is that essentially a large chunk of your growth should be coming from word of mouth, direct, organic?\n\nGrant Lee (01:03:29):\nYeah, yeah. And for us, even at scale, again going back to more than 50% of new signups still come through word of mouth. That, for us, is a sign like, \"Okay, if something is still working, people are using the product, telling other people, then you want that feeling before you really start dialing anything else up.\"\n\nLenny Rachitsky (01:03:45):\nIs there a percentage that you think is helpful for people to think about just... Is it 25% or more something like that or just word of mouth for you to feel like, \"Okay, we actually have organic growth as a major growth engine\"?\n\nGrant Lee (01:03:55):\nYeah, I think this comes back to maybe just how maybe aggressive you want to be. I think just rough heuristic is the more, the better. If it's over 50%, I think that's great. If it's approaching that, good. And just going back to, don't fool yourself into thinking just ads is going to be the way you grow. Because you can do that, but everything else becomes harder and harder. \n\n(01:04:16):\nIf you rely on paid acquisition to be the main growth engine, you should be prepared for things like CAC, like customer acquisition costs, to keep going up. The more you're trying to reach a new audience, it gets more and more expensive. So, don't assume it's going to be flat, and then all of a sudden you're running on this treadmill that's actually running faster and faster. And so that's where it's easy to get hooked on that early on when you're just investing a small amount of money, and then it's almost impossible to get off that treadmill when you're too far into it. So, anticipate that and give yourself a better chance at actually being able to sustain that growth long-term.\n\nLenny Rachitsky (01:04:50):\nImportant advice. Okay. There's a couple more elements you've shared that were key to Gamma's growth. One is sharing prototypes with users before you ship. What does that look like? What does that mean? Why is that so powerful?\n\nGrant Lee (01:05:05):\nYeah. I mean, this for us was a huge unlock. Going back to early days, when you're just trying to get your product into anybody's hands, you're getting to your friends trying to use it, and again, they're going to lie to you, tell you how great it is, and then never come back to using the product. \n\n(01:05:19):\nI think what you want to be able to do, the ideal situation is, you recruit a bunch of people that are legitimately good prospective users or customers of your product, but have zero skin in the game. They do not care at all whether or not your product succeeds. They're just in it to test it. And for us, it was people that have made slide decks or make slide decks regularly, let's drop them into Gamma, give them very little context. We just tell you, \"Hey, this is an alternative to PowerPoint. Go ahead and try it.\"\n\n(01:05:48):\nAnd then as you're going through the onboarding flow and testing the product, just tell us everything that's going on in your head, describe what you're seeing, tell us what you're trying to do. We'll give you maybe a few different tasks like, \"Oh, create a piece of content.\" And when you watch them do that and also hear what they're saying, you just immediately feel the pain. \n\n(01:06:10):\nAll the places they're struggling and all the places are so confused by what they're seeing and you sort of then can internalize that pain and say, \"Okay, we're going back and we have to fix this. This is not usable.\" We, oftentimes, are dog-fooding everything. And so you can get to the point where you're so familiar with your own product, everything feels kind of easy and you know where things are, but when you start actually hearing other people describe their usage of the product, that's a gift.\n\n(01:06:36):\nYou're all of a sudden you're like, \"Okay, now I know where to actually spend the time.\" People aren't even getting to the third screen. They're stuck on the first screen because they can't even find the button that we think is so dead obvious, and so let's go back and actually re-engineer, re-architect that piece of it.\n\n(01:06:50):\nAnd we've done that for everything: landing pages, onboarding experience, new feature launches, export, sharing, every single piece of that such that we can actually see where things are working or not. And then every time, we'll learn something that's kind of painful, but obvious that we need to fix and we go back and fix it.\n\nLenny Rachitsky (01:07:08):\nHow do you scale this sort of thing? How do you run every new big idea, new feature change by people? Do you have a closed beta group, a decent platform? How do you actually do this?\n\nGrant Lee (01:07:19):\nThere's a couple of great platforms. Voicepanel, which is also YC company, full disclosure, angel investor, and then there's also platforms like UserTesting, that really help you source and find these people that fit again, that persona or profile you're looking for. So, in our case, people that are in a specific job function or create decks regularly, and so you can use those platforms to really scale up these programs pretty fast.\n\n(01:07:45):\nAnd then once your team knows how to use those platforms... We would have an idea in the morning and in the afternoon, we're already running a pretty full scale experiment or a research study, and by the evening or by the next day, we can actually go through all of it together. So, it's pretty fast once you have it set up. It's more about how do you get the program onboarded the right way. \n\n(01:08:06):\nI think the other sort of mechanism we did early on was once we had some repeat users, we created sort of a program for our power users. We called it the Gammaster Program, which is we put them into a separate Slack workspace, and that was a place for us to share very, very early prototypes like wireframes, sometimes they were functional prototypes, and get them to get some initial feedback as well. This definitely helps with more later stage or features or things that aren't going to be necessarily important as part of onboarding, but once you understand Gamma, like, \"Oh, how do you share it?\" \n\n(01:08:41):\nFor instance, this was a great way to start testing some of that because they already understand Gamma, but now you're adding that new functionality. And then we can also watch them struggle and hear how they're struggling with the product. And so that has been a great way just to have a separate Slack workspace. Anytime we're thinking about something, they're the first to hear about it. Give us some early indication if we're on the right track or not.\n\nLenny Rachitsky (01:09:00):\nI love this workflow that you shared of you have an idea in the morning, you built a prototype. Do you built it with AI prototyping tools like Cursor, Lovable, something like that?\n\nGrant Lee (01:09:11):\nYep. That or it could be... Yeah, I mean we're lucky a lot of our designers also know how to code, so even before the recent tools, come up with some sort of functional prototype and then be able to ship that so people can start playing with it.\n\nLenny Rachitsky (01:09:24):\nYeah. So, you have an idea in the morning, you build a prototype using various tools. By the end of the day, you are getting feedback from real people using one of these platforms, Voicepanel or UserTesting, and just that loop saves you, I imagine, potentially months of just building the thing nobody wants and shipping it, launching it, and then just failing. \n\nGrant Lee (01:09:45):\nTotally. Yeah. I think this is even more probably helpful for certainly a lot of folks that are starting to do much with vibe-coded apps. Yeah, that's great. You've lowered the amount of time to get something out there. Now, prove that it's useful to some set of users, and this should be again, every day, every week, you should be able to go through a ton of these, and then build on the things that seem to be working. \n\n(01:10:08):\nI feel like that's almost like a way to speed run a lot of that early... you're in the idea maze, you think you have something that could kind of work. How do you actually break through so that people are actually finding value in what you're building?\n\nLenny Rachitsky (01:10:21):\nYeah, I love it because so many people hear this idea of just run your stuff by users before you do the user research. It sounds so hard and heavy lift-y, and the way you're describing it is very automated, very quick to do. You don't have to go think about finding random people in a coffee shop. It's just like these platforms exist where you could go plug in your thing, get feedback by the end of the day.\n\nGrant Lee (01:10:44):\nTotally. Yeah, and that helps you just move way faster.\n\nLenny Rachitsky (01:10:48):\nIs there a feature that you were super excited about that you built and ran through this and just like, \"Okay, that was a huge failure\"?\n\nGrant Lee (01:10:54):\nI don't think any of the ones where we... We always try to chunk it down. So, none of the things we're testing earlier on are these massive features. They're always an inception or a starting point of, \"This could be something interesting.\"\n\n(01:11:07):\nAnd then we take that initial learning and actually then build the product around it. It's never like, \"Oh, we spent four months working on this thing. Let's see if anybody actually wants it.\" It's almost like we always start super early. And then a bunch of ideas die right away, but they're still pretty small ideas. \n\n(01:11:20):\nAnd then the ones that kind of pass the initial test, you start building towards something that could be, hopefully, more game-changing or much more value-add. And by the time you're actually shipping, you've gone through 10 different layers of actually testing and iteration before it actually sees the light of day.\n\nLenny Rachitsky (01:11:34):\nWhat's really interesting about you and the way your company operates, you guys are ex-Optimizely people, so you're very versed in experimentation. A lot of people talk about A/B testing experimentation as something that doesn't make sense for a startup because the scale is so low. What I'm hearing here, and I know this is something you talk about, is just there is actually a lot of ways to think experimentally, even in the early stages. Is there something more there you think is important for people to hear?\n\nGrant Lee (01:12:01):\nYeah. I really think it's more of... The mindset you go into to almost any problem or opportunity is the saying of, \"Strong opinions weakly held.\" I think it's totally fine to have some of these assumptions or hypotheses going into a lot of these things, but you should always know that there's a way to start trying to validate some of this.\n\n(01:12:20):\nAs a founder, you're always trying to build conviction, and so you build conviction by not having it all live in your head, but getting it out there and start testing this with users, prospect customers, and starting to see what are the things that actually feel right. And sometimes you have enough data to be statistically significant. Sometimes it's more of a, \"Hey, we at least were able to gut check this a little bit and get some qualitative feedback.\" I think that's still valuable. It shouldn't be this sort of all or nothing like, \"It has to be static for it to be useful.\" I don't think that's true. \n\n(01:12:49):\nIn fact, I'll just share the very early story of when we first started Gamma, we knew we wanted to help change the way people communicate, and we actually had two different ideas we were parallel pathing kind of at the same time. Of course we had presentations, reimagining how people were creating presentations, and we also actually had this separate idea, which we called The Lobby, which is a virtual office. This is a place where this is, again, peak pandemic, much more hybrid work, much more virtual work. And so this was a place where everybody on the team, whether you're in the same office or not, could gather, collaborate to feel pretty magical. \n\n(01:13:26):\nAnd we worked on both for six months. We worked on both the virtual office and the presentation product for six months. We would dog-food both. So, oftentimes we'd be in the virtual office, presenting the presentation tool and kind of use both. And after six months, we came to this conclusion that we wanted to go all in on the presentation tool. And the reason was, when we looked at the virtual office tool, we were sort of always competing against what we thought was something we'd never be able to surpass, which is in real life experience, actually being able to work shoulder to shoulder with your colleagues and having this environment where you really feel like you're building together. \n\n(01:14:05):\nVirtual office could get pretty good, but we would never beat that. And so we were almost limited in our own imagination about how good could this product be versus the presentation product. We ended up with a million more ideas we thought we could actually introduce that could be better than how you work in PowerPoint today. We were just so energized by it. \n\n(01:14:25):\nAnd so for us, that was this sort of A/B test of testing these two things that we invested equal amount of time into, and coming out at the other end realizing that, one, was going to be the product we were pour all of our blood, sweat and tears into making it great because we saw the potential in it.\n\nLenny Rachitsky (01:14:43):\nI didn't know that about you guys, that you explored that other idea. There's so many startups that did that during COVID times, and like, \"Okay, this is the future. We're all going to be remote, and let's work remotely in virtual offices.\" There's a startup and I'm a tiny investor in, Lindy, that is now a big AI agent company, and they did the same thing. I imagine that was their whole first concept. It was just these little avatars. It was like a little game where you walk around, go to little virtual meeting rooms. \n\nGrant Lee (01:15:07):\nYeah, it was a fun product to work on, but yeah.\n\nLenny Rachitsky (01:15:12):\nYeah, it's interesting how we just reverted back to the mean of just like, \"Yeah, people are in offices again. That was a fun experience.\" Although things have changed. Just to highlight this point you made that I think is really powerful, I haven't heard it described this way before, just the power of testing prototypes very, very, very early, using these platforms that make it super easy.\n\n(01:15:34):\nWe always hear, \"Okay, you have a mock, you have a prototype. Yeah, testing with users always feels like this heavy thing. You got to have a user research team, go do interviews, do one-on-ones.\" What you're describing is something... I don't know why everyone's not doing this. With AI tools, it's so easy now. Have an idea, build the prototype, test it with, I don't know, is it like dozens of people? How many people actually run through a prototype on average?\n\nGrant Lee (01:15:58):\n20.\n\nLenny Rachitsky (01:15:59):\n20 people. So, 20 people look at this thing, give you a bunch of feedback, you realize this was very dumb, or, \"Here's the nugget that we want to lean into.\" And instead of building a thing, instead of doing all these user research sessions, things like that. Super cool. \n\n(01:16:12):\nOkay. Is there any other big lesson or lever that has helped you grow to today's 100 million ARR, and $2 billion company? We talked about influencer marketing, we talked about testing prototypes, investing in brand and a little bit of paid growth. Anything else?\n\nGrant Lee (01:16:33):\nCertainly for us, the ability to adapt and move fast in this environment. I attribute a lot of what we've been able to accomplish to a few things. One is we do have a small team and a lean team, one that's able to move really fast. I think that means, by default, we have to look for a lot of levers where a small team can do a lot of different things. \n\n(01:16:54):\nSo, we can talk, one, obviously about how do you construct a team like ours and what I think has worked. And then two, going back to experimentation being this sort of thing where for us it's been a huge unlock because it allows you to not only test things and iterate a ton, it allows you to build much more efficiently. And so we've been in a startup where we've had great unit economics from the very beginning. We run profitably, we have really strong margins. \n\n(01:17:22):\nI don't think that would be possible if experimentation wasn't core infrastructure to us because the temptation would be you just throw the most expensive model at every job and assume that's going to work. And the reality is that never is the case. And so for us to be able to actually test across 20 different models in production today, always trying to align the kind of value we're delivering to our customers with our ability to actually scale this operation, that's been in the background. \n\n(01:17:48):\nAnd not always things that are easily quantifiable or things that you're sharing broadly, but it is core to our DNA. And again, going back to a team that came from Optimizely, probably not surprising, but I do think that's been part of our ability to actually execute at this level.\n\nLenny Rachitsky (01:18:03):\nI love that the product is so beautiful and such a good experience. It's such a good example of experimentation and being really obsessed with running experiments, A/B tests. Data can create really beautiful products and experiences that aren't just feeling like some kind of micro-optimized flow.\n\nGrant Lee (01:18:19):\nTotally. Yeah. And just going back to one part of the team, part of that plays a huge role, which is you want to build a product where people talk about taste and brand and all these things, what they emote. I'm not going to throw in whether or not, yes or no, but I do think it makes a difference. And for us, at some point, more than a quarter of a team or about a quarter of our team was product designers. I think that's an unintuitive level of investment, or at least that's not common. You don't see many startups at our stage or early stage where a quarter of the team's product designers. \n\n(01:18:55):\nAnd I think that was an important investment for us because when you think about with AI companies, so many companies are trying to invent new surface areas or new user experiences, and that's not possible if you're not really getting the foundation right, really thinking deeply about user problems and how can you solve them in a novel way. And so for us, we made that investment in the very beginning, even if it was counter to what other startups would do, because we actually felt like it was the right thing.\n\nLenny Rachitsky (01:19:21):\nOkay. I definitely want to spend more time on hiring. Before we get to that, you've touched on this kind of concept that you guys are, what many described, a GPT wrapper company. You essentially sit on top of other models, do some cool stuff, charge for that. There's historically, and there's been a big shift here, historically, there's this sense that, \"Okay, you're just this thin layer on top of the model. How is there any sort of motor leverage or long-term business model here when it's just the model that is doing the thing and charging you guys to do all this AI work?\"\n\n(01:19:57):\nIt feels like people have started to realize maybe that is the only place money will be made because the models are just so hard to compete with, and that's not a place you can build a business anymore. Talk about just what you've learned about this concept of being a GPT wrapper, and what people may not be understanding about the business opportunity there.\n\nGrant Lee (01:20:15):\nWhen you think about maybe just literally only being a wrapper on one model or one provider, yeah, maybe there's only limited amount of utility or value add. But then when you start thinking about, \"Okay, I'm going to go really deep into this one workflow, and it's not just one model.\" It's maybe 20 plus models powering all different parts of the product, and then you're thinking about the orchestration that's required and you're thinking about, obviously if you're experimenting constantly being able to test across the newest models versus models that have been around that are cheaper, you're doing a lot to really... Your job is to, again, align value, maximize the value you're delivering to the end user in a way that's sustainable for you as a business. And so there's a lot more to that. \n\n(01:21:00):\nAnd so for us, we've always been passionate about being very close to our customers, our users. Who, for them, their job to be done is visual communication, visual storytelling. And the default tool today is going into a PowerPoint or Google Slides where the amount of effort to create something... We've all been there late at night trying to format a deck, trying to find the right layout, all this manual and tedious work. \n\n(01:21:24):\nWell, what if you could abstract all that away and give them something that feels a little bit more delightful, a little bit easier, much more effortless? What would that earn you in terms of a customer that wants to come back to your product? And so that's everything that we focus on is you need to go deep into the workflow, be empathetic to the user and their job that you're trying to solve, and of course, apply the best technology possible so that you're delivering on that promise of a product experience that's way better than the status quo. \n\n(01:21:52):\nAnd I think if you can be laser-focused on, it doesn't matter if you're a wrapper or not, what is the technology you're doing and applying, that makes a real difference? So, that's the ultimate goal.\n\nLenny Rachitsky (01:22:01):\nThis is a really cool framework for how to think about what it takes to build a successful wrapper company that is... I don't know, I'll keep using that term. I don't know if it's [inaudible 01:22:09], but just some ideas don't work. These model companies are launching their own products here and there.\n\n(01:22:16):\nSo, what I love about what you're describing here is almost like a heuristic that'll tell you if there's an opportunity/how to be successful as a GPT \"wrapper company.\" I'm putting in air quotes. It sounded like maybe there's three here. But talk about just if you think about the bullet points of what you need to do to build a successful business on top of existing models.\n\nGrant Lee (01:22:37):\nI mean, I think the most important thing is before we even talk about the technology, so we're skipping to the part where you're trying to apply the most interesting models or technology to solve some problem. Start with solving the problem you actually care about. \n\n(01:22:53):\nI think it's very tempting right now to chase shiny objects, like a founder might be able to gain some initial traction by literally being that GPT wrapper and solving any sort of problem. And you start to see some traction and then you just go with that. But before you do that, you should think about, \"Is this a problem I can invest five to 10 years into actually solving? Do I care deeply enough about it?\"\n\n(01:23:13):\nBecause if you can't, you're never going to go... actually think about overcoming the variety of different hurdles, whether it's other startups or other incumbent tools that are trying to start doing the same thing. \n\n(01:23:24):\nAnd so for us, we go back to... We've started this company because we really care about helping change the way people communicate their ideas. We really feel like this idea of visual communication, visual storytelling matters. It helps elevate everybody and it gives people much more opportunity to do the things that in the past if you weren't great at making slides, your ideas might've died, and someone else that was able to actually articulate their ideas better ended up winning the deal or winning the favor of their manager or their boss. \n\n(01:23:53):\nAnd so we felt like that was the wrong, that didn't feel right, and so could we help democratize visual storytelling, visual communication? That was sort of our north star. And then you go back into thinking, \"Okay, every step of the way, what are the tools and technology I can apply to help solve that and actually move the average person closer to that long-term vision of ours?\"\n\n(01:24:14):\nAnd of course, AI has been, again, this gift where yes, you can apply that to solving this job. You can also apply that AI to many different jobs. Figure out what is the problem you care deeply enough to go deep, because going back to this sort of idea, you need to own the sort of end-to-end workflow. A customer needs to have... You want your product to live in their brain somewhere where when they think about, \"Hey, I have to create a presentation.\" They come to you as the default tool.\n\n(01:24:40):\nAnd the moment they start creating it to the moment they ship it and send it to their boss, that end-to-end experience needs to feel magical, needs to feel great. And I don't think you can really do that unless it's actually a workflow you either know deeply yourself or you care deeply to actually help solve for somebody else.\n\nLenny Rachitsky (01:24:57):\nSo, some of the elements I'm hearing here is, one, is just like actually really care about solving this problem, not come from, \"Oh wow, this cool thing happened and it worked. Wow, maybe I could sell this thing to people.\" Because you may end up having to work on this thing for 10, 20 years.\n\nGrant Lee (01:25:11):\nTotally.\n\nLenny Rachitsky (01:25:12):\nTwo is: understand the problem really, really deeply and have real empathy for the people trying to solve this problem. You have this problem creating presentations at your previous job, so you understood it, and I imagine you understand even more deeply now that you worked on this. Essentially, care really deeply, go really deep on the problem, have real empathy for the people facing this problem, and then there's this actually be able to solve this problem using the technology out there.\n\n(01:25:39):\nAnd you made this point about how you guys use 20 different models to do what you do. Talk a little bit more about that just because people may think, \"Oh, okay, I could just go to ChatGPT or Claude and it'll create an awesome presentation for me.\" Why does it take 20 different models? Why is that such a big part of the success here?\n\nGrant Lee (01:25:55):\nYeah, so going back to the workflow, you're trying to go and break down every step of the creation process for a user. So, the moment of the initial idea to maybe creating an outline of what you want to present or articulate to creating the first draft. What do we show you there to editing the content? \n\n(01:26:14):\nLike, \"Okay, I have a first draft, but it's only 60% of the way there. How do I get to 100% of the way there?\" Those are all things that might require different models. The initial outline might be better served by something that's purposed-built for actually creating the best outline, the best narrative, the best story arc, versus one where you're actually going back and saying... Gamma, our agent can actually go and review your entire deck and say, \"Actually, if I were to add suggestions, I would say, 'You may want to change the visual layout on these set of cards, and you may want to actually change the imagery or the visuals for these cards to match everything else.'\"\n\n(01:26:50):\nThese are things that, again, every model might be better served for different things. And so knowing how that actually breaks down into the end user, sitting down, working on the product, working on their presentation, how do you help them? I think you can only do that by understanding-\n\nGrant Lee (01:27:00):\nTheir presentation, how do you help them? I think you can only do that by understanding that workflow and then breaking that down into finding the right tool for the right job.\n\nLenny Rachitsky (01:27:08):\nThat is super interesting. Essentially there's like, here's the things that AI has to do for us. I don't know, imagining some kind of a storyboard and then it's figure out the model and level of model and prompt for that model to achieve each of these steps as best as possible.\n\nGrant Lee (01:27:23):\nTotally. Yeah, and it applies to even on the visual side, finding the right image. I think certain models are great at photorealistic output versus others want more of the artistic vibe or something that feels more abstract. And so again, you can choose the right model for the right job. It doesn't mean that you'll never use other models, it's just like as you're going through that workflow, the content might dictate what's the best use case. So in that particular use case and so you're always trying to map to that, what is the end user trying to accomplish in that certain moment?\n\nLenny Rachitsky (01:27:55):\nWhich models are you finding most useful in the work? I don't know, is there anything surprising about, \"Oh wow, this model's actually really good at this and this is really bad at that.\"\n\nGrant Lee (01:28:04):\nYeah, I think surprising or not surprising, the leaderboard is constantly changing and so this is where you almost, you have to have the mindset that you can be adaptable. We're just getting to the point where there's going to be more personalization too, is going back to a consultant's going to have different needs than an educator. Educator's trying to engage their students. They may want to have language or visuals are more animated and that makes sense. Consultant can't get away with that. They may need something that is much more structured or information dense. And so again, how do we actually be the same tool that can serve both of those needs? I think that's where it becomes interesting and it's almost like there isn't necessarily even a \"best model\". It's like what's the right model for that particular user? That's actually a much harder problem to solve and we're just starting to try to embrace some of that now.\n\nLenny Rachitsky (01:28:50):\nIs there one that's just like, \"Oh, that's actually really cool.\" For something that we didn't expect?\n\nGrant Lee (01:28:54):\nEarly on, things like creating the outline, Perplexity was actually great. Not one that people talk about as often, but creating the outline and doing web search and actually integrating some of those elements together for us was pretty powerful.\n\nLenny Rachitsky (01:29:07):\nOkay, there's two more things I want to talk about. One is pricing. How you guys figured out your approach to pricing. You guys started monetizing really early and that feels like such an important piece to AI startups because you're spending real money on an inference and other things. And then I want to talk about hiring. So in terms of pricing, when did you decide it was time to really start charging and then how did you pick your approach to pricing your actual price?\n\nGrant Lee (01:29:32):\nSo we kind of stumbled into having to do pricing and packaging. I mentioned our big AI launch in March 2023. This was pre-revenue, so we wanted just to get, we focused all of our effort on the first 30 seconds, literally all hands on deck, just trying to get that right. And we spent zero time on actually monetization. So we had a credit system. All new users get 400 credits. Once they burn through those credits, there was actually nothing more you could do with AI. It kind of just got cut off. And so Intercom for us was just blowing up with people saying, \"How do I purchase more credits? I want to keep using this thing.\" In a very fair question. So basically all of April we ended up having to do a quick sort of pricing and packaging exercise. We did a couple of different things.\n\n(01:30:18):\nWe did run a form of Van Westendorp, which is just understanding-\n\nLenny Rachitsky (01:30:22):\nClassic.\n\nGrant Lee (01:30:23):\n...what is the overall willingness to pay. And so we did use that. We did kind of integrate some forms of conjoint analysis, which is just trying to understand what are the features or things that people actually value in your product. And so we'd survey a lot of our early users and ended up coming to a price point that was, in the beginning we only had one plan type. It was roughly like 20 bucks a month. Part of this was also just realizing we almost didn't need to overthink it too much because this is when the initial wave of AI companies and startups were coming out and products are coming out and you almost end up becoming anchored by, what does ChatGPT charge? Because everyone becomes familiar with that price point. And so we ask ourselves how complicated do we want to make it?\n\n(01:31:05):\nWe always default to remove friction, make it as easy as possible for a user to understand. And so we end up coming up with a similar price point and ship that as the V1, basically end of May 2023. And for us, we wanted to see a couple of things like, okay, we have the initial price point. Is it economical for us at that price point? Are we actually making money? And we'd monitor there for that for many, many months realizing that yes, actually at that price point we could still generate pretty good margins and that would allow us to reinvest that profit back into growing the business.\n\n(01:31:37):\nContinue to add obviously head count, but then also investing even more into inference costs, whatever we wanted to do to experiment broadly with AI. And so we've always had that pricing and packaging. Does it not only need to be easy to understand for a user, obviously you have to have strong conversion rates, but it should be something where you feel like you could build an enduring durable business off of. And if it's not right, then you can always go back and try to tweak things, but it's something you should be monitoring as early as possible.\n\nLenny Rachitsky (01:32:06):\nWhat's interesting, we have the head of ChatGPT on the podcast and he shared the way they picked ChatGPT's prices is the Van Westendorp survey that they ran in Google Forms.\n\nGrant Lee (01:32:14):\nTotally. Yeah, I listened to that one. Yeah, it was a great one.\n\nLenny Rachitsky (01:32:17):\nOh, what a, that survey man, what a, and not only just that survey being the slick, I'm imagining that XKCD comic with the open source little, I don't know, block holding up the entire world, like this one little piece of code. That's like the survey, just behind everything. But it's interesting how that one decision just created this ripple effect on all AI startups, 20 bucks a month. Of course that's what everyone's doing.\n\nGrant Lee (01:32:43):\nTotally. Who knows what would happen if they chose a different number, but here we are.\n\nLenny Rachitsky (01:32:46):\nEveryone would be so much more rich if it's just like 25 bucks. Someone. Imagine the GDP of growth-\n\nGrant Lee (01:32:52):\nYeah.\n\nLenny Rachitsky (01:32:54):\n...from that. Oh man. Okay. And then the way, so was Van Westendorp helped you pick a price point and then you said this conjoint analysis, we actually have a guest post that I think describes how to do this well and if not we'll find, appoint people to how to actually approach this. You started charging, so was post product [inaudible 01:33:11] launch, you launch with no paid features. People were just like, \"I want to pay, I want to keep using this.\" You're like, \"Okay, I guess we've got to start charging.\"\n\nGrant Lee (01:33:18):\nThat's right. Yep.\n\nLenny Rachitsky (01:33:20):\nIs there anything, looking back, I guess just you wish you'd known when you were approaching pricing for folks that are doing this now like, \"Oh, we should have done this differently or should have thought about this.\"\n\nGrant Lee (01:33:28):\nYeah, I mean that one's hard. I think you never want to obviously just throw something together without giving it much thought, but for us with limited resources, we focused on the only thing we thought mattered, which was getting some initial users and having that organic growth. And I think there's maybe two checkpoints of thinking about whether or not you feel like you have product market fit. I think one checkpoint is organic growth. The second is are people willing to pay for the product? And I think if you pass both those, you feel like you've, at least within some pocket of the market you have some PMF. So I think those are both important questions to ask depending on your resources. Ideally you can do kind of both and experiment a little bit along at the same time. And then by the time you actually have paying users, you sort of check some of those boxes for yourself.\n\nLenny Rachitsky (01:34:15):\nBy the way, I love that your launch pricing. You're like, \"Okay, let's figure out if we're actually making money or not.\" It's not obvious with AI companies. [inaudible 01:34:24]\n\nGrant Lee (01:34:24):\nWell, we also didn't have a choice because at that point runway was also low. So if we weren't making money, well we would actually be in a tough spot.\n\nLenny Rachitsky (01:34:31):\nThat's such a good point that you did not have a huge amount of money sitting around to spend on the way a lot of companies in AI do, just, \"We'll deal with it and we'll figure out how to make money later.\"\n\nGrant Lee (01:34:40):\nTotally. Within a couple of months we had hit a million in ARR and became profitable. And so those were both two exciting milestones for a company that months prior were heads down figuring out how we even survive.\n\nLenny Rachitsky (01:34:53):\nWhat a story. I'm so excited to be telling this story. Okay, final topic I want to talk about is hiring. You have some really hot takes on hiring, clearly it's worked out for you guys. What are some things you've learned about hiring well, what is your approach to hiring?\n\nGrant Lee (01:35:08):\nSo we had this mantra internally, I mean even before the sort of AI launch for us, which was \"Hire painfully slowly.\" And I think the temptation is once things start working, you just start, [inaudible 01:35:23] scaling this thing and start adding more and more headcount. And for us that always, I don't know, that didn't feel right. We wanted to build the team very thoughtfully, be lean, but also be a team that every individual feels like they have high amount of impact that they have on a daily basis. And so for us that was from sort of the very beginning. And so even as we've scaled up, I think we've been super efficient by nature of just being a very lean team.\n\nLenny Rachitsky (01:35:47):\nSo I think a lot of people hear this advice of \"Stay lean, be efficient.\" You have some even more concrete pieces of advice here of just what that looks like. You have a huge, I don't know, philosophy of just huge leverage per person. You focus a lot on revenue per employee. Talk about that.\n\nGrant Lee (01:36:03):\nSo obviously we look at things like revenue per employee, but we never let that be the sort of North Star. It's not something that dictates our strategy per se. Same thing with profitability. I think by being efficient, that ends up becoming a side effect of yeah, we are profitable and growing. But I think for us more is like we care a ton about adding the right team members. So it's easy. You can almost sort of shoot yourself in the foot as a founder by just setting the wrong goals. If your goal is to double in headcount and add a hundred people to the company, then that becomes the goal. The goal is no longer to add the best people. It's no longer to maintain a high quality bar. The goal is to hit a hundred people and that's everything everyone's focused on. So then guess what?\n\n(01:36:44):\nIf that's the goal, then the thing that ends up dropping is, okay, maybe we will settle for employees that are good enough and we're going to make sure we get to that hundred because a hundred is going to help us get to the next phase of growth. And then the next phase of growth. And I think we've tried to resist that temptation, which is we want everybody that comes through the door and joins our team to really be the type of person that represents Gamma. Our first 10 to 12 employees, we spent so much time really getting the DNA right. I think Brian Chesky talks about this. Your goal is you get that first 10 and you want to be able to then replicate the next 10 and then replicate the next 10, but that 10 needs to be all super cohesive. You need to have the same shared values, same principles, same ambition, same vision, and if you don't, then what are you even replicating?\n\n(01:37:32):\nRight? At that point you're just adding headcount and you can easily just be adding headcount because we're chasing the next shiny startup to join. And so for us, we really focused on that first 10. That allows us to really have this sort of community of teammates that basically want to stick around. A lot of founders think about not wanting to have a leaky bucket on the product side, but the same thing applies on the team side. If you have a leaky bucket, people are just constantly leaving, revolving door. That's a huge amount of cost. The continuity cost is massive and it's really hard to quantify.\n\n(01:38:08):\nAnd I mentioned our first 10 employees, all 10 of them are still here today, five years later. And so that sort of continuity, it means that you have this tribal knowledge that sticks with you. It means that people can continue building and having that sort of cohesive feeling. So I think that's one piece that's just like, it's hard to quantify, but I do think, new startups I would advise just to really be thoughtful about that, get that to a point where you really feel confident that as you're adding the next 10 and next hundred, you're actually replicating the same DNA and not actually just adding a bunch of random people to the company.\n\nLenny Rachitsky (01:38:39):\nOne of your very specific piece of advice is hire generalists versus someone that's just really deep in one thing, why is that so important? What does that look like?\n\nGrant Lee (01:38:47):\nThis very much feels like the age of the generalist or the rise of the generalist right now. When we think about of a team that can be really flat, you still want people that can be very spiky in certain areas. So for instance, obviously a product designer that knows how to code, that's great. It allows you to actually span across many different domains. And again, an organization that's super flat, when you're wearing a lot of different hats, that means every individual, if you're a generalist, you can wear by default a lot of different hats. And that's helped us just maintain this idea that the work, the opportunity in front of us can easily expand. Each person plays a huge role. They don't need to wait and ask for permission. They can go after and pick up a piece of work because it's there and they can at least get it started even if they're not the one that always sees it through.\n\n(01:39:34):\nAnd so for us, I do think this rise of the generalist is going to be important. You can always augment that by working with contractors or agencies that are hyper-specialized in certain areas. And this is where, for instance, going back to influencer marketing, you might work with an agency rather than kind of scale up your own influencer marketing team. You work with a hyper-specialized team there. But my marketing team is all generalists. Our head of marketing more recently launched this cool drone show in San Francisco. We have 4,000 people in San Francisco. The mayor came by, she created that entire project and managed that entire project end to end herself while also managing the entire marketing team. And so it's like this idea that a generalist is able to play all these different roles, can still be super high impact. For us, it also goes hand in hand with, I mentioned sort of this other role, which is the player coach. Traditional management layer is like a manager manages a ton of people and that's kind of their core focus.\n\n(01:40:33):\nAll of our people leaders are player coaches in that they still do the end work themselves and they can mentor and coach those around them. This analogy came from, or this mindset came from, it's something I borrowed from just the sports world in general because there's a lot of sports that just move incredibly fast. Like football for instance, it moves really, really fast. And so you might have a coach that's calling into play, but the quarterback can actually make a last-minute adjustment based on what he's seeing the defense do. And so you want a player that is on the field that can adapt as needed and that way the coach doesn't have to call every single thing in. He's just kind of giving you the general intent of what you want to run as the play and then the quarterback can still make the adjustments.\n\n(01:41:13):\nAnd so all of our people leaders, our management layer is all folks that actually can make those adjustments. If they're seeing something that doesn't feel right on a daily or weekly basis, they're adjusting priorities for that team and they're still doing the work themselves too. They're so close to it that they understand what is the relative prioritization at all times. So both of those I think have been, when I think about founders, they oftentimes think about innovation in the sense, we're going to innovate on products or innovate on technology. I think every founder today has a chance to innovate on org design, and that starts by just thinking about what is the type of company we want to build today? What does it mean? And when it comes to the management later, what type of sort of leaders do we hire? What does it mean when it comes to hiring specific roles and specific functions? All of that is a chance for you to be very thoughtful and build a company that you're excited to be at for hopefully many, many years to come.\n\nLenny Rachitsky (01:42:07):\nSo what I'm hearing here is manager, there's no pure managers at Gamma and your plan is to not have people that are just managing the ideas. Everyone, even a manager is doing their own IC work.\n\nGrant Lee (01:42:18):\nYes.\n\nLenny Rachitsky (01:42:19):\nAnd then the other piece of advice here is just generalists. People that can do a lot of stuff, and I hear this a lot on this podcast just like everyone is, there's no more just like you're a designer, that's all you're going to do. Designers need to build stuff, market stuff, write some PRDs probably.\n\nGrant Lee (01:42:34):\nJust one thing to add is that's where we are at today. I think being adaptable means that certain things may evolve and change and how the player coach model evolves and maybe in certain functions or as the team scales, that's not going to be practical everywhere. I think the reality is you just have to know and be willing to adopt different frameworks over time and being honest with yourself with what's working, what's not. I think we're constantly trying to learn and evolve ourselves. I think we're doing it in a way that we don't believe it really has been done before, and so we have to kind of pave our own path over time.\n\nLenny Rachitsky (01:43:10):\nI love that. Giving yourself an out when the time comes when you need to just hire managers.\n\nGrant Lee (01:43:14):\nA year later when you invite me back, I'll say, \"Yeah, this is what we learned.\"\n\nLenny Rachitsky (01:43:16):\nYeah, that makes sense. That's probably going to happen. It's like people are like, \"We don't need product managers.\" [inaudible 01:43:22] \"I see. Maybe we do.\" Yeah, that makes sense. One last piece I want to talk about is you have this really cool quote that you shared on Twitter. \" When you find someone exceptional, bet big on them.\" This is a big part of your philosophy is just bet big on the people that are doing super well. Just talk about what that looks like and why that's so important.\n\nGrant Lee (01:43:40):\nYeah, I mean this starts top of the funnel, which is you meet candidates, you decide who you actually hire. And so if you don't start with a high bar there, going back to what is the goal? The goal is to hit a hiring target versus maintaining a super high quality hiring bar. Those are different goals and I don't think we've ever kind of dropped our bar. And so then you bring somebody in that you think is exceptional, that brings something unique to the table, that can be a good teammate. When they're thriving, you just give them more and more resources. I think what you realize, another sports analogy is when you're on an A team for instance, or a team that you feel like is exceptional, A players want actually more playing time. You never see a star player that says \"I actually want less playing time.\"\n\n(01:44:26):\nThey want more time on the field, they want to actually go after the hardest problems. They want to be able to feel like they're making a huge impact. And so if you have fewer exceptional teammates where you can just throw almost anything at them and they'll figure it out, that feels for you as a team, just feels great because then they get what they find rewarding, which is the ability to go after hard hairy problems and to be able to come out through the other end feeling like they've accomplished something. And I think that for us has always been something that goes beyond just almost everything else. We give people a chance to really thrive in this environment and we try to nurture that as much as humanly possible every step of the way.\n\nLenny Rachitsky (01:45:04):\nIs there anything else around hiring that you think is really important or a big lesson you've learned or lesson you share that we haven't talked about yet?\n\nGrant Lee (01:45:12):\nI mean, there's some of these intangibles, which is for the founding team, does this feel like this could be your life's work? When you're pitching a potential candidate, does this feel like something where you're actually committed? The unfortunate side of a lot of what's happening in the AI world broadly is I think you're coming to learn which founders are sort of missionaries versus mercenaries. And many that were just chasing maybe just a big outcome, or something shiny, or something to feel good about themselves, they'll go off and then the company, I guess doesn't live on or maybe has to find a different way, a different path.\n\n(01:45:47):\nAnd so something I admire is you look at some of the founders, obviously before this sort of AI era, folks like Dylan or Melanie, Figma, Canva, Ivan at Notion, they've been doing this for over a decade now. They had many chances to just leave and sunset off into doing whatever they wanted to do, but they care so much about the mission, they've stuck it through. And I think when you're talking to candidates, candidates can kind of tell, is this something the founders even care about? And I think you're going to have a better chance of attracting true missionaries, people that want to build with you for the long haul, if it's authentic and it's something you actually care about.\n\nLenny Rachitsky (01:46:24):\nOh man, I keep saying this. I feel like we could have at least five more hours of stuff to talk about. That'll be good content for when you come back and you're making a billion dollars a year. Before we get to our very exciting lightning round, Grant, is there anything else that you think is important for people to hear? Any last, I don't know, lesson you want to double down on? Anything you want to leave listeners with?\n\nGrant Lee (01:46:48):\nYeah, I mean, just going back to the original story of that was probably the ultimate low point. Having an investor that spent 20 minutes listening to my pitch, telling me that it was the worst thing, worst idea in the world. I think throughout the journey there's going to be those low moments. And when I think about being a founder and working a startup, you're honestly just trying to increase your luck surface area as much as possible. And for me, luck surface area has two dimensions. The first dimension is people. Who are you surrounding yourself with that share your same ambition, share the same values, same principles, and find those people and then give yourself enough time to prove that you guys can accomplish great things. I'm lucky to have two amazing co-founders. We've been working on this thing for five years. There's 0% chance we'd be where we are if I didn't have them. And we've had to overcome a lot. But I guess for us it's like creating that own luck over a long time horizon has been the only way that it's been possible.\n\nLenny Rachitsky (01:47:48):\nWell, it's clearly showing in the success you guys are having. Grant, with that, we've reached our very exciting lightning round. Are you ready?\n\nGrant Lee (01:47:56):\nYep. Yes.\n\nLenny Rachitsky (01:47:57):\nAll right. I've got five questions for you. First question, what are two or three books that you find yourself recommending most to other people?\n\nGrant Lee (01:48:07):\nYeah, so I'll give one that's for pre-product market fit folks and the one post-product market fits.\n\nLenny Rachitsky (01:48:07):\nPerfect.\n\nGrant Lee (01:48:12):\nPre-product market fit I would say is Shoe Dog, which is written by Phil Knight, founder of Nike. And he talks about two things. One, you should chase your sort of crazy ideas, but these should be ideas you're passionate about. He was an athlete, and so not surprisingly, he focused a lot about creating tools or shoes in this case for other athletes and was passionate about that. And the second thing he taught me was going back to the team thing, he surrounded himself with other people that were super passionate about athletics and shoes as well. And so the folks that he had as his initial startup crew were all that.\n\n(01:48:49):\nAnd I think that gives you a chance of overcoming a ton, where you're focused on problems you actually care about solving, and you're dealing with a team that shares the same ambition as you. Post-product market fit, there's a book called 7 Powers by Hamilton Helmer, where I think when you think about how to build a durable business, there's so much in there. I think there's a lot that you can kind of read and reread as you kind of evolve and hit new milestones yourself. And so much of that can be both tactical but also zooming out and thinking about what are the big picture strategic questions you as a founding team you need to be thinking about. So both are great.\n\nLenny Rachitsky (01:49:21):\nHamilton was on the podcast, we'll link to that episode. I also love that book. Many people mention it. Next question, do you have a favorite recent movie or TV show you've really enjoyed?\n\nGrant Lee (01:49:31):\nYeah, The Lazarus Project is one, I'm a huge sucker for time travel and sci-fi, so this is one I just started watching. And for me, it has all the right ingredients for a fun show.\n\nLenny Rachitsky (01:49:44):\nIs there a product you recently discovered that you really love?\n\nGrant Lee (01:49:47):\nYeah, I mentioned this already, Voicepanel. So again, a full disclosure angel investor, but we've been using it and going back to kind of being a cheat code for folks that are starting to experiment a ton with different ideas, vibe coding, maybe some of these might get this into the hands of users, hear what they think about it. I think, and honestly can help speed up a lot of things and save you time from wasting time on the wrong ideas or ideas where there's no market.\n\nLenny Rachitsky (01:50:13):\nIs there a life motto that you often find yourself coming back to in work or in life?\n\nGrant Lee (01:50:18):\nYeah, so there's this Chinese idiom that my mom used to say, it's a [foreign language 01:50:24] which the translation is \"A frog at the bottom of a well\". And the story is like there's a frog at the bottom of the well. He looks up every night and he sees the world and he imagines he knows everything about the world. And then one day a bird comes along and describes all the things that he sees, the ocean, the mountains, and the frog realizes that what he sees is just such a limited part of the world and the bird asks, \"Do you want to come join me?\" And kind of see the rest of it. And so frog goes along.\n\n(01:50:54):\nAnd so for me, I came from a pretty modest childhood. My parents, we didn't have a whole lot and I think it would've been very easy to kind of have a very narrow lens on the world and be like, \"Oh, this is what it is.\" But my mom never allowed me to do that. She always pulled me to dream much bigger to say, \"Hey, the world is vast. It's your opportunity to seize it. You have to go out there, don't have a narrow view of what's possible. Always dream bigger.\" And so for me, that's always carried through and every time I feel like I'm thinking too little or too small, I try to zoom out and remind myself that there's much more out there to go after.\n\nLenny Rachitsky (01:51:30):\nThat is so good and so important and so valuable in today's world where so much more is possible. Just like, most of what limits people it feels like now is just, I don't know what idea I have. I don't know what to do. Now you could get things done so much quicker and so much more is possible. So that's such valuable thinking. Okay, final question. You help people present better. Your tools basically help people become better presenters. What's one tip you've learned or one tip you teach people to become better presenters that might be helpful to listeners?\n\nGrant Lee (01:52:00):\nYeah, I mean, I'll go back to the consumer advertising concept, which is one idea at a time, this notion of you give them one egg, someone can catch it, give them too many eggs, they're going to drop it. So don't try to throw too many concepts all at once. Keep it simple. People will appreciate it. And so with every sort of presentation, break it down into the core concepts, try to make sure you're covering one at a time. And I think once you sort of see a through line there, then it becomes easy for it to package up into something that feels more cohesive.\n\nLenny Rachitsky (01:52:32):\nLess is more, as they say.\n\nGrant Lee (01:52:35):\nTotally.\n\nLenny Rachitsky (01:52:36):\nGrant, two final questions. Where can folks find you online? Where can they find Gamma? What should they know? Just plug anything you want and then how can listeners be useful to you?\n\nGrant Lee (01:52:44):\nYeah, so you can find me on both Twitter and LinkedIn, DM's open. I honestly hear just knowing how hard the journey is in general, whether just thinking about a startup idea or you're deep into startup land, I want to be hopefully helpful. I'm going to be hopefully your biggest cheerleader, so let me know how I can help. And then for us, we're always looking for feedback. So if you're trying Gamma, it's falling short of your expectations, let us know. We'd love to help in terms of just trying to make it better. And yeah, really appreciate all the feedback and support along the way.\n\nLenny Rachitsky (01:53:16):\nGrant, this was awesome. I really appreciate you making time. I know you're trying to build a crazy fast-growing startup with a lot going on, so I really appreciate you making time for this.\n\nGrant Lee (01:53:24):\nThanks, Lenny. It's been a blast.\n\nLenny Rachitsky (01:53:24):\nIt's been a blast for me too. Bye everyone.\n\n(01:53:29):\nThank you so much for listening. If you found this valuable, you can subscribe to the show on Apple Podcasts, Spotify, or your favorite podcast app. Also, please consider giving us a rating or leaving a review as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at lennyspodcast.com. See you in the next episode."
}
```

Episode 102: Lessons from working with 600+ YC startups | Gustaf Alströmer (Y Combinator, Airbnb)
Guest: Gustaf Alstromer

```json
{
  "id": "gustaf-alstromer",
  "guest": "Gustaf Alstromer",
  "title": "Lessons from working with 600+ YC startups | Gustaf Alströmer (Y Combinator, Airbnb)",
  "transcript": "# Lessons from working with 600+ YC startups | Gustaf Alströmer (Y Combinator, Airbnb)\n\n## Transcript\n\nGustaf Alströmer (00:00:00):\nIf I drill down what makes companies fail, it's quite simple. It's just like they don't talk to users, which means they don't find product market fit. And if they don't find product market fit, nothing else really matters. What mistakes do people make is like it is all about that. It's all about talking to customers and learning that you're building something that's actually useful. YC Slack headline is make things people want, and it's still true and it's always going to be true.\n\nLenny (00:00:30):\nWelcome to Lenny's Podcast where I interview world-class product leaders and growth experts to learn from their hard one experiences building and growing today's most successful products. Today my guest is Gustaf Alstromer. Gustaf is a group partner at Y Combinator where he's been for almost six years. Prior to that, Gustaf was at Airbnb for over four years where he started the original Airbnb growth team and where I was very lucky to get to work alongside him for a number of years. Gustaf is also at the heart of YC's increased focus on climate tech, and in my opinion is one of a handful of people who've had an incredible impact on the increasing amount of investment and people flowing into climate tech.\n\n(00:01:10):\nWe chat in depth about what's happening in climate tech, why things have shifted so much recently, what's new and exciting, and how to think about the space if you're hoping to make the jump. We also get deep into Gustaf's experience working with over 600 startups over his time at YC. We talk about what are the most common mistakes that early stage startups and founders make, what advice YC partners give founders most often, the most common attributes of successful founders, the importance of having a technical co-founder and why that's the case, so much more. I guarantee you will leave this episode smarter and more inspired and I can't wait for you to hear it. With that, I bring you Gustaf Alstromer after a short word from our wonderful sponsors.\n\n(00:01:53):\nThis episode is brought to you by Linear. Let's be honest, the issue tracker that you're using today isn't very helpful. Why is it that always seems to be working against you instead of working for you? What does it feel like such a chore to use? Well, Linear is different. It's incredibly fast, beautifully designed, and it comes with powerful workflows that streamline your entire product development process from issue tracking all the way to managing product roadmaps. Linear is designed for the way modern software teams work. What users love about Linear are the powerful keyboard shortcuts, efficient GitHub integrations, cycles that actually create progress, and built-in project updates that keep everyone in sync. In short, it just works. Linear is the default tool of choice among startups and it powers a wide range of large established companies such as Vercel, Retool and Cash App. See for yourself why product teams describe using Linear as magical visit linear.app/lenny to try Linear for free with your team and get 25% off when you upgrade. That's linear.app/lenny.\n\n(00:03:02):\nThis episode is brought to you by Eppo. Eppo is a next generation A/B testing platform built by Airbnb alums for modern growth teams. Companies like Netlify, Contentful, and Cameo rely on Eppo to power their experiments. Wherever you work, running experiments is increasingly essential, but there are no commercial tools that integrate with a modern grow team stack. This leads to wasted time building internal tools while trying to run your experiments through a clunky marketing tool. When I was at Airbnb, one of the things that I loved about our experimentation platform was being able to easily slice results by device, by country, and by user stage. Eppo does all that and more, delivering results quickly, avoiding annoying prolonged analytics cycles, and helping you easily get to the root cause of any issue you discover. Eppo lets you go beyond basic clickthrough metrics and instead you turn north star metrics like activation, retention, subscriptions, and payments. Eppo supports test on the front end, the back end, email marketing and even machine learning clients. Check out Eppo at getepo.com. Get, E-P-P-O.com, and 10X your experiment velocity.\n\n(00:04:15):\nGustaf, welcome to the podcast.\n\nGustaf Alströmer (00:04:17):\nThank you, Lenny. Honestly, it's so great to see you. I'm excited to be talking to you.\n\nLenny (00:04:22):\nI've been looking forward to this conversation for a while ever since we booked this. We worked together at Airbnb for many years. I was really lucky to get to work with you before you moved on to bigger and better things at YC. Speaking of Airbnb, you once tweeted about how special an experience that was for you, and I think even more interestingly, how many of the people that have left Airbnb can't find another place that's as special that just like that bar has been set too high. And so my first question is just like, what do you think it was that made Airbnb so special? Why was it such an important experience for you and other people? And even more importantly, just what is it that you take from that experience that you bring to startups that you work with now?\n\nGustaf Alströmer (00:05:02):\nYeah, it's funny. I think this year in a couple of months it'll be 10 years ago since you and me started Airbnb.\n\nLenny (00:05:07):\nWow.\n\nGustaf Alströmer (00:05:08):\nIt was 2012. The reason I treated that was I asked everyone that I'd met after Airbnb, because I had this experience of like, this was the highlight of my career up until then, at least being in the team like that and I asked everyone, \"Have you found anything better?\" Besides maybe one or two people, I haven't heard anybody say that they found something better. And they all missed that dearly. I thought a lot about why that was the case, but I would say Airbnb did not feel like a normal job. It felt like more like a group of friends trying to just do something together and we were friends. At least in the beginning it did not feel like this was a job. It was sort of like an ongoing project and an assembly of amazing people. I think in the end we managed to build two things, like a really successful company, thanks to Joe and Nate and Brian just for starting this. Without them there would be nothing to build.\n\n(00:06:04):\nI think people don't like to use the word family, but I feel like that way because when I go and meet with Airbnb alumnis from that team. We have a very special bond that reminds me of close social connections more than anything else. It does not remind me of coworkers. I asked myself why this was the case. The best answer I have is probably we brought in a special type of people. We had very diverse backgrounds. A lot of us was former founders. Not many of us were career people from the technology industry of the early days, not many of us.\n\n(00:06:37):\nI think that bar that we set, the first, I believe when I joined there was probably five or seven PMs and there was 30 engineers or something and you joined a little bit before me, those people set the bar or set the standard of what we were looking for afterwards. I think it took a long time to change that narrative. I mean, eventually you have to hire people that only had big corporation careers, but I don't remember we did that for a long time. When I actually read my goodbye note recently, my words there still means a lot to me and it means sort of like you're trying to reflect on exactly these things on like, building a great company that became successful and being part of this sort of family of really close friends.\n\nLenny (00:07:21):\nSo it sounds like if you had to boil down what Airbnb did, it sounds like hiring was the main piece that impacted the way it turned out just like the founders being very specific about the type of people they were hiring.\n\nGustaf Alströmer (00:07:32):\nAbsolutely.\n\nLenny (00:07:32):\nIs there anything that's like, I don't know, a take away there of just what you recommend to founders, like hiring. People know, be very careful who you hire. The first, I don't know, 10 people will impact the culture long term. But I don't know. Is there anything just abstracting away there of just what to look for in that first batch of hires?\n\nGustaf Alströmer (00:07:50):\nIt's a tricky one because I would like to say that all the things that we did were the cost of the outcome of this, but that's not really how the world works. Some of the things we did work, some of the things that we did did not work. It's hard for us to actually disentangle what those things are, but I think we can talk about the things that we did. First of all, we made sure we hired people that were really excited to be there, right? They wanted to build Airbnb and they were really excited to work on Airbnb. That was the most important thing. Of course people had other offers, but I think you can figure out from those offers are you excited to be here or not. So that was probably the first thing.\n\n(00:08:30):\nThe second thing I think is trying to understand the true motivations of the people that were there, like lower, \"Why are you here?\" We did something we call culture interviews that I think the founders have written about or there's probably content online about this. We did a lot of culture interviews early on to try to figure out we got the people that were there that mapped our core values and were really excited to work on Airbnb. I think finally, I don't know how this happened, we did pick people from diverse backgrounds. Most startups don't have most of the PMs being former founders, but I believe that was the case for the first 10 or 12 or 15 PMs at Airbnb. A lot of us were former founders. I think that that made a big difference for how you make decisions and how you get started on things.\n\n(00:09:21):\nI think I actually see this a lot in the Airbnb founders. They really care about the time at YC and they tried to recreate YC inside Airbnb a couple of times with demo day and with so new products completely isolated from the rest, starting with doing things that don't scale and talking to customers. So I think that that experience had made a big impact on them, but it's hard to say just these two things go and apply these things. It's actually kind of hard to say, \"Well that will work at it as Airbnb.\" I think that's a really tricky question to answer.\n\n(00:09:51):\nThe last thing I would say is Airbnb had an incredible business model, an incredible business from early on and it was hard to fail. What I mean by that is it's hard to fail with a company. You can fail with individual things inside the company, but a company was still going to succeed. I think we all felt that. A lot of companies don't have the ability to take risks like Airbnb did early on because they don't have something that's so obviously great.\n\nLenny (00:10:18):\nThat's a really interesting point on the last piece that you may have the most amazing culture and hire incredibly well, but if the company doesn't work out, it's not going to be looked back as like, \"Wow, that was an amazing experience, but we failed.\"\n\nGustaf Alströmer (00:10:18):\nYeah.\n\nLenny (00:10:30):\nYeah, that's interesting. So you mentioned YC and this kind of is a good segue to where I want to focus most of our time. You mentioned that you've worked with over 600 companies at this point, which is absurd. It feels like it's you get some statistical significance on takeaways at this point of what works and doesn't work. So I have just a bunch of questions about your experience working at YC and with all these companies. The first is I think about this quote that Elad Gill tweeted once and he wrote, I think, a post about it. Elad Gill is a legendary angel investor. He said that starting a company is an act of desperation. You're either desperate to change the trajectory of your career or you're desperate to make a bunch of money, you're desperate to achieve some kind of mission or build a specific product that you're just like, \"I need this to exist.\" I'm curious if you agree with that sentiment and then I have a follow-up question around that.\n\nGustaf Alströmer (00:11:20):\nYeah, actually I haven't heard it before, but you know me, I'm an optimistic person. I think it probably reflects on my view of this question, but I would say desperation sounds like a negative place that you're starting at. I actually think that most people to start companies start from a positive place. But the motivations, I agree with what he said, can be very diverse for successful founders, right?\n\n(00:11:41):\nSo we actually asked this in one of the early Group Office Hours sessions. We asked them, \"Why are you doing this?\" And we don't want to hear an answer like, \"I found this niche of the market.' That's not the why. The why is like, \"Why will you come in and work late after four years when you have no money left and everything's going to shit?\", right? Then the niche market is not the answer. There's something deeper than that. We've learned that it varies a lot for people's motivations to start companies. Some of them just want to solve some technical problem that they feel are passionate about solving. Some of them want to prove themselves in front of others or prove themselves towards themselves. Some have grand really important motivations to change the world and they will say things like, \"I want to give everyone water or I want to solve climate change or I want to democratize publishing.\" You can imagine any number of large ideas. Some people just want to start a big company and just want to be successful.\n\n(00:12:44):\nIt doesn't matter, in my experience, what your motivation is. I don't think either of these motivations. It sounds like some of them will be better than others. In my experience, it's not the case. The motivation will change over time to just running this thing, right? Like once something becomes big, it's hard to think every day about exactly why you got started because the motivation is just like it can be fun and work on boring things because it's fun to build something big. Everything doesn't have to be shiny and big and grandiose because there are many ideas that are \"boring,\" but just the idea of running the company becomes the motivation eventually.\n\nLenny (00:13:17):\nThat is really interesting that one of the main things you look for, it sounds like when you're interviewing, is how strong and durable is that drive to build this company. Is that what you're saying?\n\nGustaf Alströmer (00:13:26):\nIt's actually kind of hard for screen for motivation, I would say, in interviews because the purpose of this kind of Office Hour question is to highlight why someone is there and highlight the diversity of reasons people are there and sometimes even highlight from one founder to another co-founder why they are doing this. They might have never talked about this. Actually, surprisingly, often founders have not talked about why they're doing this. And just knowing why someone is here really helps with conflict resolution for example, really helps with understanding why someone is there on a certain day or something like that. So it's not something we screen for as much as I think we try to help founders discover this among themself and really know this about themself. I've just accepted that the motivations to start companies is widely diverse.\n\nLenny (00:14:13):\nDo you ever discourage founders from starting a company when you see that maybe it won't be a durable kind of lasting motivation or whatever other reasons just knowing how hard starting a company ends up being?\n\nGustaf Alströmer (00:14:23):\nIt's a good question. I would say sometimes if someone doesn't have a motivation or don't know why they're doing this, they're doing this because they read that it would be a natural career step. So a good reason to not start a company is if you think of starting a company as a career step. Well, it is not. Because if it's successful, it'll be your entire career. It'll be 10 years most likely. And if it's not successful, then it's not something that people generally aspire to start. So non-successful startups. So I think people will start companies because this is sort of something they want to put on their resume. They have not understood what startups are or why you should do them.\n\n(00:14:58):\nSometimes you can screen and kind of figure this out, but sometimes people don't even know why they're starting a company when they get started and it kind of gets figured out along the way. And that's okay. I don't want to discourage people who don't know exactly why they're starting this company just to start a company. They might figure out along the way and find the true motivation after doing this for a little bit or finding that it's really fun.\n\n(00:15:21):\nI think I discourage people to start startups if they have so many other things that are important than their life that are more important than the startup. So if there are financial constraints or family constraints or relationship constraints and they are going to trump this, then yeah, you should think a second time perhaps because startups are hard. They're much harder than a normal job. Equally hard if they're successful or failure, right? There's no middle way, we're like, \"Oh, my company is doing great so I can chill.\" That doesn't work that way either. So I don't really discourage or encourage people, I just want them to have all the information.\n\nLenny (00:16:03):\nYou mentioned YC Office Hours and I had a question around this. I'm working on this piece where I'm interviewing a bunch of B2B founders of companies that are doing super well and I asked them a few questions like, \"How'd you come up with the idea? How'd you find your first few customers?\" it's shocking how many of them bring up a conversation in YC Office Hours as the most pivotal point that set them on the trajectory that they're on now. I'm curious what happens in these Office Hours? What are the most common pieces of advice that you give or maybe most surprising pieces of advice that you give in these Office Hours so that people can get maybe a glimpse into these conversations you have?\n\nGustaf Alströmer (00:16:43):\nSo at YC we have two type of Office Hours, two of the common ones. We have regular Office Hours, which is usually a one-on-one or basically the founders talking to me, or me plus another partner. They happen every week or every other week throughout the entire program. And they happen years after the program on the regular basis. Then we have Group Office Hours, which is you and six or seven other startups talking to us. They have [inaudible 00:17:07] a little bit different purpose. So the goal of the regular Office Hour, I always ask the question, \"What's holding you back from moving faster? We don't want to hear updates, we don't want to hear strategy questions. We want to understand what's slowing you down or what's holding you back from moving even faster. You generally have a specific goal.\"\n\n(00:17:27):\nAnd I think that question of what's slowing you down or were holding you back crystallizes the priorities. There are only so many things you can do as a startup and there are only so many things that matters at that stage. And by asking that question, we can start digging into, \"Okay, what's the goal? What are the things that drives towards that goal? What are the things that's slowing you down towards that goal?\" Usually, the founders don't know what's slowing it down, so the conversation and us probing questions actually leads to us or them discovering what it is just slowing them down.\n\n(00:17:59):\nIn the Group Office Hour, it's a little bit different. The Group Office Hour holds a couple of different purposes. One of them is if I think back on Paul and Jessica's motivation to start YC, this is a surprise to them, but starting a company is incredibly lonely. You can't really lean on your employees and say, \"Hey, I'm feeling really shitty as a founder today. Everything is going to shit.\" Employees isn't going to take that well. So you lean on perhaps your investors, but they're not really available. But what you can lean on is other founders because they're all in the same situation. It's sort of like when you ask a founder the question, \"How are things going?\", it's so emotional for them to answer that question because it's never going well, right? It's never like, \"Oh, everything is going fantastic.\" They might say that, but everybody knows all founders, when they look each other's eyes, then no, that's not the answer.\n\n(00:18:48):\nSo founders have infinite number of problems that they're thinking about all the time, which is why they're allergic to the question, \"How are things going?\" But when YC started, we put all the founders in a group together in a room and they started learning that all founders, all companies are broken in some way, right? They're all having these massive problems and they're all feeling that anxiety when they hear the question, \"How are things going?\" And just hearing other founders explain their problems, perhaps solving their problems, is a really good way for yourself to both feel motivated to do it yourself and see how problems get solved when other companies are having similar problems. Nowadays because of the scale at YC, we group companies together that have the same problems or the same area that they're operating in.\n\n(00:19:30):\nThe second thing that Group Office Hours do well is accountability, right? We ask you, \"What were your goals? What were your goals for the last two weeks? Did you hit them?\" And then that gives founders accountability because founders are competitive. They don't want to look bad. They don't want to come back after two weeks and say \"Nothing worked,\" or at least like, \"We didn't learn anything.\" They want to learn something and make progress whether it's positive or negative.\n\n(00:19:54):\nGroup Office Hours to me is the most magical moment because it really creates this very intense three or four month period. Founders often come back to us afterwards and say, \"Hey, we want to do that again. We don't have this really intense, really productive period. We don't have a program exactly. We have other programs, but we don't have anything exactly that mimics that experience.\" But we do encourage founders to continue with Group Office Hours after YC. Many of them do and many of them, ad hoc, continue to meet for years in this group setting where they ask the same kind of questions to each other, to hold themselves accountable, to learn from each other, and to just have someone else to lean on. I think this was unknown and somehow the world didn't know before that starting company is super lonely and you have all these anxiety. By just talking to other people who have the same problems, it's just one of the best thing you can do.\n\nLenny (00:20:45):\nThere's so many things that come up when you talk about this. One is I had a startup at one point and we worked in the coworking space. We joined the coworking space because we're like, \"Oh, we'll meet other founders. It'll be social, we won't be alone.\" But it turns out everyone's just heads down, headphones on, \"I don't have time for anything. I just need to work.\" It's like a microcosm of that experience that even if you're surrounded by founders, no one has time to do anything. They're just working.\n\nGustaf Alströmer (00:21:09):\nYou got to schedule it and force it and put the laptops on the floor and the phones on the floor and you just sit there with pen and paper. That's how you have to do it. We tried to mimic that as much as we could over Zoom. But honestly the best experience of this was in person, in a ring, in mountain view with no computers and everyone just paying attention to everyone. That was the best experience. Yeah, that's what I remember is one of the most meaningful parts of YC. I didn't have it myself when I did YC, but now everyone has it.\n\nLenny (00:21:36):\nThe other thing that's made me think about is someone tweeted once, \"Don't ever ask a founder how they're doing or how it's going. It just creates all this anxiety because nothing's ever going great.\"\n\nGustaf Alströmer (00:21:44):\nDon't do it. Everybody looks at each other's eyes and they know that they're allergic to that question.\n\nLenny (00:21:50):\nThat's hilarious. So just to summarize the questions you said you asked, the one is in the individual Office Hours, is what's holding you back. And then in the group setting, what was the question again that you asked?\n\nGustaf Alströmer (00:22:01):\nWhat are your goals for next two weeks and what were your goals for the last two weeks and did you hit the goals? And if you didn't hit them, what came in the way of hitting the goals? It's very simple. That can uncover lots of problems that other founders are having exactly in the same way. Just by talking about the things that held you back or the things that allowed you to hit your goals, uncover something material for the other seven companies doing that in the ring.\n\nLenny (00:22:26):\nIf you kind of zoomed out a little bit and thought about the startups you've worked with, what would you say are the most common mistakes that early stage startups make broadly?\n\nGustaf Alströmer (00:22:36):\nThere's so many. I mean, this is how I initially learned about startups, by searching for that on Google and landing on Paul Graham's articles because he kept-\n\nLenny (00:22:45):\nWow.\n\nGustaf Alströmer (00:22:45):\nI think I've written many articles about this topic because it is so common. So this topic can go on forever. But if I take the most recent experience I've had in YC, I would say startups fail, one, because they don't talk to customers. And if you don't talk to customers or users, you don't actually know what's important. If you don't know what's important, it doesn't matter what you build, it doesn't matter kind of what ideas you have in your head if you don't actually know what it is that you need to build and you don't validate with customers. That's where a lot of the failure stems from. A lot of early YC for us or early part of the program is us pushing and probing founders to be like, \"Tell us about the conversations you've had with the customers. What did you learn? Can you show us the organizations?\" Or like all these questions, like, \"What are the software they're using? What are they paying for? What problems do they have? How are they describing the intensity of that problem?\" So that's what we spent a lot of time early at YC.\n\n(00:23:43):\nAfter that, I would say one of the commons mistakes in... I'm not talking about generally startups here, I'm talking inside YC. The second most common thing I see in YC is people are just afraid to talk to customers. They're just not trying hard enough to get in front of customers. I think this comes from, technical people tend to think that software is just sort of solution to everything. But really what you should need to do is to talk to someone over Zoom or over phone or in person, even better. People are just afraid of doing that and they're afraid of being rejected. These are common, people that want to build good products are just really afraid of people saying no.\n\n(00:24:25):\nThe problem is, which anyone who hasn't done sales before that joined YC, they realize this, is that if you take the average customer group in the world, 90% are not early adopters. It doesn't matter if you have something new and cool, they're just not interested. They are not incentivized to take risks in their job to try something new. They're just incentivized to not take risks and just continue what they're doing. Those 10 percents are the early adopters. They're The ones that you actually want to reach. But that means you have to reach 10 to find one. And then you convince that one person to get on the phone or a video call with you. And that takes work and it takes a lot of work. I think people don't really think of this. This is common knowledge, basic stuff for salespeople, but founders should never done sales before just get surprised by the percentages and the sort of what it means to do this.\n\n(00:25:12):\nIf I think of more generally outside of YC, so these are two kind of things I experience within YC, but I think generally outside of YC, I would say the two most common problems the same one is not talking to customers, the other one is not being technical and not knowing what it takes to build successful technology company. It means having technical founders and it means being able to build the first prototype. This is something we screen for when we interview people in YC and we aren't accepting a whole lot of team that don't know how to build or get their first prototype built themselves because we know it is a super common value pattern.\n\n(00:25:50):\nI can go on and on and on and on for this one, but honestly if I drill down what makes companies fail, it's quite simple. It's just like they don't talk to users, which means they don't find product market fit. And if they don't find product market fit, nothing else really matters. What mistakes do people make is it is all about that. It's all about talking to customers and learning that you're building something that's actually useful. YC Slack headline is make things people want, and it's still true and it's always going to be true.\n\nLenny (00:26:23):\nThis is really interesting and good advice. It's interesting that... Like, \"Talk to customers,\" people hear that all the time. They're like, \"Of course we're going to talk to customers. We're going to do that of course.\" And your experience is they know this but they just don't do it probably because they're afraid. Maybe also because they think they already know what they need to build and it's like, \"Yeah, we're good.\"\n\nGustaf Alströmer (00:26:41):\nAnd you have all these validators, right? So the people are validating that even if you don't talk to customers, why has he accepted you? This investor invested in you. This investor said you were great, like blah blah blah. All these different validations that you confuse with product market fit, right? We have to remind everyone on the first day in YC, \"None of you have product market fit because you probably don't,\" right? Almost nobody has. Because people confuse this external validation with the thing that matters the most, which is talking to customers and learning what matters. It's just like a thing that just keeps coming back. Some get really good at it. And that is the source of successful startups, is when you really get good at this.\n\nLenny (00:27:21):\nIt reminds me coming back to Airbnb, one of the most important moments in Airbnb history was Paul Graham telling the founders of Airbnb, \"Where are your customers?\" And they're like, \"Oh, they're in New York\" and he's like, \"Why are you talking to me and not in New York right now talking to them?\" And they talk about that all the time.\n\nGustaf Alströmer (00:27:35):\nYeah, it's absolutely true. I think he wrote the article Do Things That Don't Scale as a learning.... The learning there was the Airbnb founders doing the trips to New York and learning about how to build Airbnb, which is a very counterintuitive idea, which is when you have to spend the most amount of time with your customers. I think Airbnb is sort of one of the best stories inside YC of doing this well.\n\nLenny (00:27:57):\nThis also reminds me, I've been talking to a bunch of founders recently. I asked them, \"How many customers have you talked to help figure out this idea?\" So just the other day it was 150 financial CROs that they talked to before they actually started raising this round. Another company, actually two Airbnb guys that started, they actually ran ads I think on LinkedIn to find specific people to talk to and that specific role. And they talked to probably at least 100, maybe 200. So there's a strong correlation there.\n\nGustaf Alströmer (00:28:26):\nYeah, I think that's the volumes that people don't expect. They think they might have to talk to five, but I think you have to talk to 25 to 50. That means you have to reach out to a lot more to be able to get to people that are potentially early adopters. Those ones you talk to are also the ones that become your customers. So you are already doing most of the sales by just doing this work anyway.\n\nLenny (00:28:45):\nDo you have maybe one tactical tip you could share of just either getting over your fear of talking to customers or just holding yourself accountable to actually doing it?\n\nGustaf Alströmer (00:28:55):\nYeah, I tell this story, I actually told this story yesterday. So remember when you sign up first service that's a cool service and you hear about in TechCrunch or something like that and then you realize you already signed up a year ago, right? And then from the founder's perspective, you sign up to something you never used it. So the founders who build those services, their inclination to think is that \"Everybody hates me because they sign up and they never use me.\" They never use the service. The fear of that is that basically the fear of rejection. So if I put my thing out there and most people won't use it and they will tell all their friends how shitty this thing is, you should never even sign up for it, that's the fear people have, but the truth is that people sign up for it and be like, \"Oh, I'm busy. I got something else to do.\" And they actually don't remember or care.\n\n(00:29:43):\nSo whenever you sign for something that you signed up a year ago, think of yourself that that is the common customer experience, which is that you just sign up for a lot of stuff you don't even remember you. You never have this, \"I hate this thing' reaction. You always have this like, \"I'm indifferent to this thing. I don't actually care to even complete the sign up flow or try it out,\" right? I think that's the thing that people need to remember, is that the worst thing that can happen to startup is not that people hate what you're doing, it's that they're completely indifferent to what you're doing. It's not the worst thing, but the most common thing that happens is people just is indifferent. But it doesn't give you a second chance. Let's say, it always gives you a second chance and you really need to internalize that people have busy lives and if people don't actually use what you're building, that's fine. You can reach out to them a year from now or six months from now or two weeks from now and they probably will if you make some improvements.\n\n(00:30:37):\nI think people just have this fear that, \"If I get a lot of rejection, that means everything is bad.\" But the rejection should be put in context to the early adopter idea and that most people who don't care are not early adopters, who don't want to dig into new things. And that the more narrow of a solution you have to a specific problem, the fewer people actually want to dig in. But that's where you have to start because you cannot build the whole thing right up front and make everybody loves you. That doesn't really work that way.\n\n(00:31:07):\nEven Airbnb was like air mattresses or staying in someone's homes when they're home. That was not the complete solution of Airbnb. But there were early adopters who dug in and be like, \"Yep, I like that. I like those two things. I want to have people stay in my living room in my mattress.\" But that's not what Airbnb is about today, and a lot of those things were unknown at the time. But I think people are just afraid of rejection and you just need to overcome that fear and just learn that there's nothing that's really that bad that can happen when people don't use the service or sign up and don't care. It's not really that bad.\n\nLenny (00:31:42):\nIt reminds me of a quote that I always love for Mark Andreessen, that everyone's time is already allocated. They don't have space for your product right now. They already have plans for their day.\n\nGustaf Alströmer (00:31:42):\nTotally.\n\nLenny (00:31:51):\nAnd it takes a lot to convince someone to pay attention to anything. I guess that just comes back to why it's so important that your product is solving a real pain and not just a nice little toy that is better than what's out there, but not so much better that you're like, \"I need this right now.\" So maybe just along those lines, do you have any thoughts on just the importance of that pain and just how critical that is?\n\nGustaf Alströmer (00:32:15):\nI actually recorded... I'm happy of these videos, but I've recorded two videos on YouTube as part of YC Startup School last fall. You can go watch them on YouTube right now. One of them is how to talk to users and the other one is how you sell or how you use sales. The one about talking to users, I think there's a difference in asking someone, \"Do you have a problem with X, Y, Z? Is this podcasting setup working for you?\" And people say, \"Yeah, it's kind of working.\" But if you are a podcasting setup experts and you watch people use some other thing that's really shitty, they might also think that it's pretty good. But you have to watch them do it. And the best way for you to figure out what is the intensity of the problem is not to ask them but to watch them or to watch them solve the thing that they do.\n\n(00:33:06):\nYou know how a lot of non-technical people don't know how to automate things? So they will do the same thing in Excel like a million times by just tapping because that's the only thing that they know and they're not technical enough to write some kind of script to do it, and you just have to watch those people to just feel the pain. You can't actually ask them how difficult is it to do X, y, z because they won't even know that it's that difficult to them. So the best thing I've learned about how to discover the pain is to watch people, have them screen share, have them walk you through their daily workflow about the area where you're doing some discovery. That is the best thing.\n\n(00:33:43):\nI'll give you another example. So there's a bunch of waste companies that are doing EV charging for electric cars and they're like, \"What are the problems in EV charging?\" And I was like, \"Just rent an EV and go and charge at all the non Tesla chargers and see what they say, see what you experience.\" And the truth is that it's just like garbage. A lot of EV charting systems are just so shitty and the apps are terrible. You just have to just use them yourself to know how bad it is.\n\nLenny (00:34:13):\nIt's cool how often it just comes back to, \"Just go do the thing. Do things that don't scale.\" Classic YC advice. I want to come back to something you mentioned that I want to pull a thread on as the technical co-founder, being technical early on, just to cover that. So I know YC looks to... Having a technical founder is an important variable in you're deciding to accept a company. Say someone doesn't have a technical co-founder, do you have any advice for what they could do, like what often can work?\n\nGustaf Alströmer (00:34:41):\nYeah, I think the first thing is to understand the value of technical co-founder. So some people who are in this situation where they have an idea of something they want to build and they don't have anyone to help build them in building it, I had a friend Paul who gave this incredible quote, he said, \"I have an idea for a song, I just need a musician to help me make it,\" right? That's kind of similar to how it is with engineering. If you view output of engineering as like, \"I just have an idea for a song, I just need someone to actually make it for me\" and then you're not valuing software engineering or mechanical engineering or any engineering skill set deep enough, right?\n\n(00:35:17):\nThe truth is that the engineering part is the really hard part. The first thing I would say is you need to learn how to value the engineering piece. Let me give you an example of how you don't do that. You applied to YC and you have 90% for yourself and 10% for the engineer. You're basically saying that like, \"Oh, the engineering part of this company is only worth 1/10th of me. I'm the non-technical person.\" So that to me is a signal that you're not really valuing engineering.\n\n(00:35:44):\nOkay, so how do you go out about and find someone? Well, the truth is that there are a lot of technical co-founders. The technical people, they also want to find business co-founders. They don't want to do other part. They don't want to do sales and they actually don't really care that much about fundraising. They just want to solve the problem. And that's fine. We built something called co-founder matching where those funders can meet, but if you don't participate in that, you can just start by asking the best technical people that you know. \"Are you interested in starting a company with me?\" Same thing with rejection. Many of them will just say, \"No, I have a great job, I'm really happy.\" But some of them will have thought about starting a company for a while and was hoping that someone would come and ask them to do that. So you have to remove your fears and go and ask the best people.\n\n(00:36:29):\nThe reason you want to have a technical co-founder and not a hired engineer or not a hired contracting team is because so many of the decisions you're going to make are technical and so many of the iterations you're going to make relies on engineering. And if you don't understand that, you won't actually make the right decisions anyway. It's not like service. You have an idea for a product, you build a product and you're done. There's infinite number of iterations in that process.\n\n(00:36:54):\nAnd then finally, I would say a lot of people learn how to code themselves, right? So there are a lot of places online that you can learn the skillset that takes to build a prototype. You might not be the best engineers. So there are many successful startup founders who are not the best engineers because they stop coding when they hire three or four engineers, that's fine. But you need to be sufficiently good that you understand the value of engineering and that you understand that the best way to solve most of the problem is with software. There are a lot of founders who just, for whatever reason, study something else that doesn't have to be a conscious or very precise reason that you had when you were 18 or 19 and then you're 25, you're like, \"Oh, I wish I know how to code\" and then just learn to code and they learn how to code. It's not that more difficult than that.\n\nLenny (00:37:42):\nHave you ever seen a startup work out if they had a contracting firm, like engineering firm build a product? Does that ever work or were you just like, \"No, do not ever do this\"?\n\nGustaf Alströmer (00:37:53):\nBasically, I can't recall any specific ones where people have a contracting firm but I've recalled founders where let's say you had two non-technical founders, but they valued engineering and they had an ability to build a team of great people that were not co-founders and they gave them equity and they become successful. There are many examples of that I would say, but I don't remember any specific examples where you had a contracting team building the whole thing. I think the reason for that is it takes more than just sort of riding a spec to build a product. You can't actually spec yourself to a great product. You have to just be part of the iterations yourself. That's why I think someone being the engineer, having the idea of what iteration looks like and just doing it is how you do things.\n\n(00:38:38):\nI think that the cases where I've seen non-technical founders make this work is that they have really good engineering teams who feel like they're founding team. It might not be co-founders per YC-7 definition of having 10%, but they feel like they're the bonding team.\n\nLenny (00:38:52):\nThis reminds me of a story of just the recent podcast interview I did with the CPO of Calendly. She talked about how when Calendly started, they actually had a Ukrainian dev team built the first product. Not only did they help them build the first product, they actually ended up driving all the growth initially because they saw Calendly and started using it within their firm.\n\nGustaf Alströmer (00:39:14):\nWow.\n\nLenny (00:39:14):\nAnd then everyone that they knew started using it and spread within Ukraine and they actually continue to work with that firm. They're still the [inaudible 00:39:20] team for Calendly or some part of it.\n\nGustaf Alströmer (00:39:22):\nWow.\n\nLenny (00:39:24):\nYeah\n\nGustaf Alströmer (00:39:24):\nWow, that's cool.\n\nLenny (00:39:24):\nThere's a success story.\n\nGustaf Alströmer (00:39:26):\nI mean, I would say it's certainly the case that in some countries people have other jobs while they start the startups, so like the engineers. In Ukraine for example or in Eastern Europe, it's very common that if they start their own startup, they actually have a full-time job as a contractor while they're starting the startup because that's how you pay the bill because often you can't raise money. And that's fine too.\n\nLenny (00:39:46):\nAmazing. That's some hustle.\n\n(00:39:49):\nThis episode is brought to you by Pando, the always on employee performance platform. How much do you love the performance review process? Yeah, it's time-consuming, subjective, biased, and there's rarely any transparency. With the rapid shift to distributed work, it's a struggle to create the structure and transparency that you want to help your employees have the highest impact and growth in their careers. Pando is disrupting the old paradigm of performance management, including a continuous employee-centric approach so employees stay engaged, see their progression in real time, and know exactly when and how they can level up. With Pando, managers can leverage competency-based frameworks to effectively coach and develop their teams and align on consistent growth standards resulting in higher quality feedback and higher performing teams. Visit pando.com/lenny for more info and get a special discount when you sign up and reference this podcast. That's pando.com/lenny.\n\n(00:40:47):\nI want to zoom out a little bit and ask another big question and see what answer you have for this. If you just think about the most successful startups in YC or even just the companies you worked with, if you had to pick just one or two attributes of what's most common across successful companies, what would that be?\n\nGustaf Alströmer (00:41:04):\nI would say the most common reason that I've seen founders succeed or companies succeed, it comes down to the founders and characteristics of those individuals. The most important characteristics of those individuals are they're really determined to win and they don't give up when things are hard and they have an internal motivation that's just really infectious to people around them, which is how they end up building really good teams around them. People are actually going to want to go and work for them. I have numerous examples of people like this where the CEO or one of the founders are just really inspirational people.\n\n(00:41:42):\nThe second thing I would say is they are technical. So that's kind of like they're technical enough. If I would grade companies on a scale of technical to less technical, more technical founders are more likely to succeed, I would say. And then I would say they figure out how to talk to users and move fast early on so they don't wait for permission from their investors or from YC or from someone else to make progress. They're like, every day or every week there's continuous progress. They're not doing this for someone else, they're doing this for the customers. They're not doing this for the investors, that's for sure. The investors are sort of in the way more or less. And they're just naturally focusing on the customers.\n\n(00:42:29):\nFinally, what I would say is the skill that's really attributed to great founders is excellent communication skills. So the ability to communicate really complicated ideas clearly, to enjoy the communication part, right? Enjoying communication is often kind of correlated with enjoying doing fundraising, which is an important part of some companies' success. Not all of them, but for some of them. I would say communication or storytelling is part of the same arc, right? And those are part of the same thing that actually motivates people around you. If you can communicate why you're building, what you're building and why it's important to the world, tell a story about that, that can motivate people around you to just want to follow you. I think it's rare that I've seen founders succeed where the founder isn't in some way an inspirational person or someone that is a good communicator. Most of the time, you at least have respect or you somewhere know that they're going to succeed, right? And that is what inspires you to be around them or be on their team.\n\nLenny (00:43:33):\nThat is a really cool list. So just to summarize, one, they have the strong will to win, and with that they're inspirational. They kind of pull people along and get people really excited. Two is they're more likely to succeed if they're technical and can build a thing. Three, they figure out how to talk to customers, don't wait, just start doing it. And they're just obsessed with that versus what investors want them to do and they don't want to talk to the investors to make time for the customers. And then excellent communication skills, which comes back to the first. They're able to story tell and get people excited. Is that right?\n\nGustaf Alströmer (00:44:06):\nYeah, I would say those are the attributes of successful things. To be a super successful company, there's something else that have to happen. Those things are not things you can put on a list because they are the outliers, right? If you look at startups on a typical YC batch, there'd be a couple billion companies. Those are the outliers. They'll almost certainly have all the things that we've talked about. Many other companies in the batch will have that too, but then what makes someone a true outlier is something that is unknown. That's why so many investors said no to Airbnb when they were not trying to raise money because that was an outlier idea. It was an idea that was not logical and did not make sense to most people. Those kind of ideas, the ones that end up succeeding often don't make sense to people. There's some reason that no one has done this before because they're just not natural next step of the world.\n\nLenny (00:44:57):\nThat's a great segue to a question I've been meaning to ask, which is, how good are you at predicting in a batch which startups are going to be the monster hits? So maybe like you and then just generally YC, how good are you all at knowing what's going to work out, is going to be the next Airbnb or Dropbox?\n\nGustaf Alströmer (00:45:14):\nI think the truth is that we're not very good at knowing what's going to succeed. Certainly we cannot figure out who's going to be the really successful company in the batch. That's not possible. What we're good at is knowing what failure looks like. What we sometimes like to tell founders at the beginning of the batch is like, \"If you fail, please do it in some new exciting way. Not one that we've seen 100 times.\" Because we have seen people fail for a large number of reasons. The best way for us to sort of not predict, but the best way for us to make more companies succeed is to tell them how they might fail, right? Be very direct and honest with them and say, \"You doing these three things, these things are likely going to lead that you won't succeed.\" And if we do our job well, most people get that feedback and they're on the track for succeeding.\n\n(00:46:06):\nNow, which of those companies end up becoming the best? There are so many things that are uncorrelated to being the best, and it's the things that people don't like. I'm in a hot industry, I was writing up on TechCrunch like, this investor started talking to me. You'd be surprised how many of the things I just mentioned are uncorrelated to outlier success, right? That's why it's so hard to actually do this. I think people really want these questions to be answered. People really want to believe that you can pick really great companies at the seed stage, but everything I've learned from the plus 600 companies that I've worked with is that it's just not that easy and it's maybe not even possible and certainly not possible when you talk about finding the outlier companies. I don't think it's that easy. And if it was easy, then we would accept a lot fewer companies. We just accept those ones, but it's just not that easy.\n\nLenny (00:46:59):\nDo you have a sense of which ones are likely to work out better than others? Or is it just like, \"We have 150 really unclear, but one of these, hopefully.\"\n\nGustaf Alströmer (00:47:09):\nOne good indicator is if each new Office Hour there is really exciting new stuff, right? We're not talking about the same thing we talked about two weeks ago or four weeks ago. They've already done that stuff, right? Like, \"Oh, I was trying to sell to these three customers. Well, they already bought it. I'm not actually talking to seven others.\" And now we are talking about a different price and different product because they're like, they want more of what we're doing. If I'm experiencing that, and that's like a consistent trend, then when people draw this revenue graph of this 10% weekly growth rate kind of situation, those are the companies that we attribute that to. It's like if you're able to make that progress on that short amount of timescale, you are on track to do something well.\n\n(00:47:51):\nNow, a lot of other things have to go well for you to ultimately be able to succeed, but progress on this weekly or biweekly timescale is a really good indicator of someone who'll succeed. To me, much better indicator than I am in this market or I'm talking to this investor or something like that. But those are much worse indicators of someone succeeding than I'm making progress and it's pretty fast clip.\n\nLenny (00:48:17):\nInteresting. And so what I'm hearing is at the beginning of a batch, we're just in a bet on a bunch of companies that have a lot of potential founders, technical maybe, they have the strong will to win and all these things. Through the batch, you're looking at the companies that are exceeding your expectations week to week in terms of progress that they're making.\n\nGustaf Alströmer (00:48:34):\nI mean, sometimes it could be different reasons that people are not making progress. But if you are making continuous progress, and I think Paul and Jessica said this, this was true early days in YC, if you are hitting your goals and you're making progress continuously, if that continues, that's a really strong correlation to some success. But again, going back to the question, can we predict who's going to be the best ones? No. And that's why we really focus on trying to meet people not to fail. Especially good teams can't fail. If you have a really talented team who's really technical in how to build a product but they make some other basic mistakes, like not talking to customers or something like that or trying to build everything all at once, I feel like it's our responsibility to make sure they don't make the basics mistakes that we've seen many times. We need to help them at least make some spectacular mistake that we haven't seen before. Then that's a high potential team. If someone's on a good track for a decent idea but they're still early, that's a really good potential.\n\nLenny (00:49:30):\nI have kind of a fun question that I wanted to try, which is kind of connected to this idea around attributes of successful founders and companies. So I had this founder friend named Flow, and he was asking me recently, \"If you had to think about the most successful founders, which attributes do they have?\" And he kind of gave me this list and it's kind of like two ends of a spectrum. So I thought it'd be fun to just go through this list and see, in your experience, which end of the spectrum, if any, are associated and correlated with the most successful founders. Does that sound good?\n\nGustaf Alströmer (00:50:01):\nSure. Sure. Let's do it.\n\nLenny (00:50:03):\nOkay. So the first is speed versus quality. Is there end of the spectrum where you find that most successful founders are either speed-focused or quality-focused?\n\nGustaf Alströmer (00:50:10):\nSometimes founders ask us this question, \"What should I focus on? Growth or retention?\" And the answer is, they're asking us for permission to not do one or the other. The truth is, to succeed, you have to do both. I would argue that speed versus quality, there's different level of speed and different level of quality at different stage of the company. But the truth is that you always have to move fast and you have to understand what the meaning of quality is, right?\n\n(00:50:35):\nSo I think I actually don't see that as a spectrum, but I would say if you move fast with talking to customers, you'll build something that have potential having quality because you know a lot about the problem. I think when people think about quality, they often think about, \"What is my personal definition of quality?\" I have a bar of quality, but quality to me of a good product idea, a good startup idea, has more to do with the customer think is valuable. And if you move fast by talking to customers and having customer learnings and know the problems, then you will actually come up with something that's high quality. So they're not at a spectrum to me.\n\nLenny (00:51:11):\nAll right. Let's try another one. Confidence versus humility as a founder?\n\nGustaf Alströmer (00:51:16):\nI don't think that they're on a spectrum. I think that learning to predict confidence as a founder is critical going back to this communication piece of motivating people around you, right? I wouldn't want to join a company where the founder's completely not confident in their own idea, right? Because that is going to shine through. An investor isn't going to want to invest in someone who's completely not confident in that idea. So learning to first build your own confidence for what you're working on and then predicting that confidence to the people around you I think is critical. A lot of people will have doubt around you. And if you're not predicting that confidence, it's not clear that anybody else will if you're the founder. You're the one who's have to do it.\n\n(00:51:58):\nI think that you can predict that confidence while having a strong sense of humility towards the people around you. But I think when it comes to startups, learning to have that confidence is an important piece of the early days, right? And especially if you're building something that's very difficult, that takes a lot of work and a lot of money, protecting the confidence that you will succeed is critical for everybody that's doubting you. And those doubting you is a lot of people around you, right? And you just need to have an unnatural amount of confidence to prove them wrong. I think, again, this is not on the spectrum with humility. In fact, the most successful founders are often the most... You cannot inspire people around you if you don't have a strong sense of humility. People don't actually want to spend time with you, which means they don't want to work for you or invest in you. Again, you need both, but they serve different purposes when you get started.\n\nLenny (00:52:48):\nTough gig this founder life. You got to got to be everything. Let's see if there's a big difference in this next one. Execution and tactics versus focusing on strategy and kind of higher level stuff. How deep do founders go that you find that are most successful?\n\nGustaf Alströmer (00:53:02):\nSo this one actually I have a strong opinion about. I think that the reason that we talk about strategy a lot is because it goes back to business school. The origin of business school was to teach people to join the corporate world. And in the corporate world, strategy matters, right? So when you join a big company, you're employee number 2,010 or something, then you probably might have a business school job where thinking about corporate strategy is an important thing. When you are a small startup, strategy does not matter because there's not that much a strategy as about. Maybe later on you might be fruitful to think about strategy, but strategy kind of assumes that you can do multiple things at the same time, which small startups cannot. They can just do one thing at the same time. So execution is the thing that matters for companies.\n\n(00:53:51):\nWhenever someone wants to have a strategy conversation, it assumes that they don't understand their priorities. The priorities is always a list from top to bottom where there's one thing that's more important than the others. You can't really have a strategy session about the other things because there's only one thing to work on. So to me, a clear answer here is the good founders are execution-oriented and they just continually have one priority of what they're trying to go for. And then they're just hitting that priority all the time and then new priorities will come up and you don't really have time to have a discussion about company strategy. Company strategy also assumes that you have product market fit because you already have something that's working. If you don't have that, then getting to people wanting your product, that is your strategy. You don't have any other strategy.\n\nLenny (00:54:36):\nAwesome. Okay. We found one that's quite different one on the spectrum or the other. How about autocratic and kind of like, I don't know, I think of Steve Jobs-like versus kind of consensus, collaborative driven, if this is a spectrum at all. And then where do you find founders might fit that are most successful?\n\nGustaf Alströmer (00:54:53):\nI don't know if I have an answer to that question because I think when you work at early stage, you might look different than when you work at late stage. I don't spend a lot of time with founders that have thousands of employees and hearing how they are in the... When I talk to those founders, I talk to them one-one-one and I only hear from their perspective. So I don't actually know how they're peering in a large corporate setting. But when you're a small company, you're three people or five people or 10 people, you cannot be an autocratic decision maker. The founding team have a founding team decision making dynamic that could look different. Sometimes it's like everybody decides together on everything and sometimes you say, \"I have my area of expertise and you have yours. We split it up, the decision making.\" Either of those things are fine. I think you just have to have a process so you don't rehash every decision after you made them a million times.\n\n(00:55:42):\nI would say the thing that matters the most at that point is to be willing to adhere to the process that you and your founding team have come up with. Your individual nature could be different in a different role in a different company. But for a startup to work out, you have to have a specific process on how you make decisions. Those are on a very short sprints, like weekly or biweekly. And everyone needs to feel good about decisions after the fact. At least they feel good about the process. I don't think that you can just decide... You can't also be fully collaborative, everyone gets decided about everything. So small startups agree on how they decide together. So after that, everyone just follow the process. That's usually how things work out.\n\nLenny (00:56:27):\nOkay, I got one more. Cares more about the product or cares more about the distribution and growth strategy?\n\nGustaf Alströmer (00:56:33):\nWell, there's a lot of assumptions built into that, I would say, because caring about the product to me is caring about the customers. Sometimes if I would say something like, \"Oh, great founders care about the product,\" a lot of founders misinterpret that as in my personal perception of the product or my ideas of what the product is. And that's wrong. The right perception there is the expectations or the use of the product from the customers. So if you are meaning focus on the product or cares about the product in that sense that your customers care about, then absolutely, I think that's a really, really critical, important thing to have early on, like talking to users, doing things you don't scale. Once you get big, if you don't figure out a scalable distribution strategy, you won't succeed. And those are different for different companies, but they're not doing anything that don't scale.\n\n(00:57:27):\nDoing things that don't scale is not a scalable strategy. Eventually, something specific will be the things that work for you. If you're lucky, people will talk about your product and you'll have organic growth. But in many cases, that sales, that is some kind of consumer distribution strategy. You can't start with that. I've seen a lot, and this is when I had to reset my thinking coming from a growth team joining YC, is you can't start a startup ethic with a growth team mindset because that is just scalable things all the time. And really what you need to go back to is doing things that don't scale and unscale your way of thinking about customers. But it's really useful to have the growth mindset once something is working, right? Once you have thousands of people signing up everything every day, well how do you get to 2,000? Well, that's probably something that looks more like this thing that the growth team would do or distribution team would do.\n\n(00:58:21):\nI would say everyone has different experiences of this based on their prior experience, right? So if you work for a company that was infinitely successful, then you won't care so much about this. If you work for Google, you'll never even think about this because distribution is just the website. But if you work for a really small shitty product, then you think a lot about distribution because that's natural to you on how you succeed. So I think at the end of the day, talking to customers matters the most. That is what it means to care about a product to me. And then distribution is something that you will definitely invest a lot in once something is working.\n\nLenny (00:58:53):\nAwesome. All right. I have probably a hundred other questions I want to ask along these lines, but I want to make sure we get to another topic which I know is near and dear to your heart, which is climate tech. So my understanding is you're instrumental in pushing YC to focus on climate tech as a focus area. I believe you led the charge on their initial request for startups I think is the term where you all put out like, \"Here's who we want to fund.\" I think you've mentioned you funded a couple dozen climate tech companies and the last few batches, is that all generally correct?\n\nGustaf Alströmer (00:59:24):\nFirst, request for startup was actually Sam Altman and a few other folks that was kind of that one who drove that. The second one that we wrote into our actual request for startup, I wrote that one, was carbon removal specifically focused on. And then naturally the people that apply with climate tech ideas get in my reading queue of applications and I read them and I interview them. Not all of them, but many of them. I think today we funded over 130 plus companies that are focused on climate tech in some way or another.\n\nLenny (00:59:53):\nWow.\n\nGustaf Alströmer (00:59:54):\nThe trend line is that really ambitious people who want to start companies in this area. Some of them want to start the companies because the climate tech is the number one problem, but they don't view this as a nonprofit. Now I want to really make this as a distinction. People somehow think that starting a climate tech company is doing good for the world, but it probably doesn't have a lot more than that. The truth is that the world have decided. Because climate is one of the biggest problems that we're facing, if not the biggest, we've decided that we are going to stop doing the things that we're doing and we're going to change our entire energy system and change all the things that we do that emits carbon. And we have just decided, governments have decided this. The question is how it's going to happen, but this decision has been made.\n\n(01:00:42):\nIn that transition, we're talking about trillions of dollars of money moving from things that cause climate change to things that don't. The scale of this transition is not something we've seen recently. Like software is not that big in comparison. It actually is much smaller than this transition. So I think if you look at something like Tesla, which now has, I don't know, $600, $700 billion market cap, that's just one company that currently provides a couple percent of all the cars in the United States, new cars sold. And that is already one of the biggest companies in the world and has now the biggest, richest person in the world. We've only seen the beginning of this. The economical motivation be behind the decisions that people are making are just as strong as I want to fix climate change because this is just a really good business. This change have attracted a large set of software founders that you and me know that listen to this podcast that said, \"My skills is relevant here. There are a lot of things that I can do. And if not, I can learn those things.\"\n\n(01:01:43):\nBut most importantly, the skills of working for startups is really, really critical to join this transition. A lot of them have started companies or joining companies. I still get an email every week from some accomplished software engineer who asked me \"Which software companies should I work for to fix climate change?\" And I've gotten those emails for two or three years now. This thing just attracts really, really ambitious people. It's not stopping. It's accelerating. I feel lucky to work with so many of these great founders because they are uniquely interesting people.\n\nLenny (01:02:16):\nI've noticed exactly the same thing of just how many smart, driven, amazing people are like, \"I just want to move to a climate tech company. That's all I'm looking for now.\" Just to give you credit, I feel like you are ahead of the curve on the shift that's started to happen and pushed YC to focus on this really early. I always think like, \"Man, I know Gustaf and I feel like Gustaf has made such a massive impact on the investment and focus in startups on climate.\" And so I just want to give you huge props for doing that and being so at the forefront of a lot of this.\n\nGustaf Alströmer (01:02:50):\nThank you. I mean, sometimes I'd say that it matters to someone who has the credibility of YC to start accepting these companies. It does matter. I remember when I spoke to Diego from Pachama in 2018 when he was starting Pachama, we were whiteboarding in YC. He was a YC alumni starting a different company. The word climate tech did not exist. People were unsure if investors would fund companies like these. Pachama's raised $60 million to have, I don't know, lots of big customers, lots of employees and it's clearly doing really well. But I think at the time of 2018 it was kind of unknown. One of the reasons it was unknown is we had this previous bubble, clean tech bubble, in 2008, 2009, 2010 that didn't work out because of a number of specific reasons and investors were just afraid of funding things because they had some scar tissue or scars from this previous thing that happened.\n\n(01:03:46):\nNow that turned out upside down. The number of new investors that are investing in climate tech is as big of a trend as any other trend we've seen in the last decade or two decades. There's just an enormous focus on the investing side. Most recently, me and another guy, David Rusenko, wrote this request for startup, a new updated, very detailed list of... I hope we can post it in the show notes, a very detailed list of ideas or areas where we think it might be worth looking If you want to start a company.\n\n(01:04:18):\nNow, we don't know what good ideas look like. We don't know. But we can tell you where all the areas of opportunity exist. We should go and look for good ideas. We wrote this because in response to all these people that come to us and say, \"I want to work on climate tech. I don't have a good idea because I don't have any specific experience in this stuff.\" And then we're just like, \"Don't work on these three things but go and work on any of these 25 directions.\" It already has generate a good response. I think it'll generate more response. But I think it's important for YC to tell the world that we look and fund these things. And that's always been the reasons we had requests for startups, is to let people know that these are things that we actually want to fund.\n\nLenny (01:04:58):\nI actually moderated a panel a couple weeks ago. This organization called the Climate Draft put together PMs that are in climate tech. A lot of the questions were just like, \"What kind of background do I need to move into climate tech startup to start a climate tech company?\" It's interesting, every single one again and again just said like, \"Your actual regular PM skills is all we need.\" There's a lot of people already at the company that are experts in the science and that's okay if you have no experience. They just need the business experience, how to operate, how to execute, standard stuff that PMs learn. And so would you agree with that that you don't need to have this deep background in science and climate to move into the space?\n\nGustaf Alströmer (01:05:41):\nYeah, I would say if you were working on a software company and even some of the hardware companies, that's probably generally true. Absolutely. It's much more valuable to have that background than to have this specific domain expertise background. Those are complimentary. But as a PM, having the solid PM background is the more valuable piece I would say. Being a competitive PM, coming from a really good culture of product management, knowing what good looks like, that's invaluable to some of these companies because in the past they weren't able to hire these people. So I agree with that 100%.\n\n(01:06:14):\nIn terms of founders, I've seen everything, right? I've seen people having some domain expertise starting a company and really succeeding. I've seen people who had no domain expertise and learned everything they need to know. Maybe they partnered up with someone who had a domain expertise and then succeeded. I've seen all of it and I actually think that you can succeed in either of these categories. You don't need the deep expertise. It depends really on all the area you're in. But someone like Pachama, Diego and Tomas did not have expertise in forests besides the personal experience. They just had a willingness to solve the problem and it really worked out for them.\n\nLenny (01:06:51):\nYou mentioned you have this list of areas you're excited about. I know we'll share in the show notes, but is there any you want to highlight, just like here's areas you're most excited about and want to fund and/or are there companies you want to mention that are super interesting and super cool in the space that people should know about?\n\nGustaf Alströmer (01:07:04):\nWe wrote the list and we are highlighting companies in each of the categories. I don't know if I want to highlight any specific categories, but I can talk about some of the things that we've funded in the past that has real legs. So here's how I generally think about climate tech. So we have to decarbonize all the things we do that emits emissions. That means we have to change a bunch of things in the world, change transportation, change energy, change homes, all of these things, or change how we heat homes for example.\n\n(01:07:35):\nAnd then there is carbon removal. Carbon removal is sort of like, well even if we do all of this stuff really well, it's probably not going to be enough. And because there's an opportunity and there's some evidence to suggests that we can actually remove carbon from atmosphere in some way or another, a lot of companies are also working on this at the same time. I would say we need to do both and they're not in conflict. We probably are going to need to do both. Well, we certainly need to do the first one.\n\n(01:08:02):\nOn the decarbonization side, there's infinite number of categories of our society where we met a lot of carbon, right? So I'll give you an example. Shipping is a really big deal. A lot of carbon emissions come from freight ships around the world. That's not obvious solution how you would solve that because the kind of oil that they run on is really cheap and it's a very low margin business and they don't have a whole lot of incentives to change besides what is coming down regulatory. So it's not a national solution where someone to be a cool, someone will build a test off ships and it just work out. But the two companies to be funded in that area, one of them is Seabound who is building carbon capture and removal for ships.\n\n(01:08:41):\nThe other one is Fleetzero who build electrical ships. Electrification is on and again and again and again and again and whenever it's being applied, turn out to be a more efficient way of doing whatever thing that you were previous doing with the combustion industry. It is almost no maintenance. It's cheaper to build. The batteries are more expensive, it's cleaner and it fits the carbon coal you have. There's just a bunch of benefits there, but there has limitations. Usually the limitations on electrification has to do with batteries. It's like how far can you go? Now that category is what I call the... Which is a very important kind of critical one, which is the carbon accounting and sort of the recommendation systems that help big company account for the carbon that they have and they are giving some recommendations of what you do.\n\n(01:09:25):\nSo I'll give you three examples. We funded Unravel Carbon, which is carbon counting software in Singapore, focusing on Asia, probably the leading one there. There's company called Carbon Chain, which is focused specifically on supply chain and shipping and raw materials, stuff like that out of UK. And then there's ANAI here in the Bay area. Who knows how this market is going to play out, but this market has carbon accounting customer demand right now, right? So all the large companies of the world have other either promised the government, promised their shareholders or promised the public, or maybe even the employees, they go into decarbonize. They don't always have an idea how to do it. These software platforms is like the plug and play \"This is how you do it.\"\n\n(01:10:07):\nAnd then I'll give you two examples of things you can go into if you don't have any specific domain expertise and just a good software engineer. There's a company called Enode. They are basically building Plaid for EV chargers and Plaid for home energy system. So if we imagine that the future of all homes or future of all charging of EVs is going to be a bunch of energy appliances that are run by small computers, they're all wifi connected, you can connect to them and do things, tell them to turn on, turn off, turn on when it's cheap, turn off when it's whatever, all these different things that are valuable for the energy grid. Then you need a software platform to connect with all of them. And that's what Enode has been building.\n\n(01:10:48):\nAnother related company is called Static. Static is the Airbnb for EV charging in India. The reason that you need something like that is you don't have Tesla charging. You don't really have public charting in general. People don't have outlets in their garage and they don't have garages. So you need to build a new bottom up EV charging system or platform, and that's what Static has been doing. They're actually building out public charging infrastructure as well, but they have their own app. So if you use a Static app, they'll direct you to all the Static chargers. I believe that they're the biggest or the fastest growing EV charging network in India, which is the second-biggest country, if not the biggest country in the world right now.\n\n(01:11:25):\nSo the potential of these ideas, even that they're doing well now is just infinite, such an enormous market. If you succeed in one of these things, I don't think we are going to have as many gas stations as we have today and different networks. We're not going to have the same in EV charging. It's going to be a lot more consolidated around the use experience of the end user and the app they open to do this stuff. So I'm pretty convinced that there's real big opportunities for software entrepreneurs to figure this out. There's some companies in the current batch that are focused on this too.\n\n(01:11:59):\nThe final one I would mention is Heart Aerospace. We have a Heart Aerospace and the Right Electric and a few others focused on aviation. Aviation is another big difficult to decarbonize. Heart and Right are focusing on battery electric planes. So they're basically making commercial airplanes that fly on batteries and flying electric motors and it's incredible to see.\n\nLenny (01:12:22):\nWhat a killer list. We're definitely going to include links to all these companies in the show notes. Something I was thinking about is, so one narrative violation you mentioned is that there's actually money to be made in climate tech. It's not impact-oriented market anymore. It might be worth chatting about why that happened, but the question I want to get to is also things are going well. A lot of progress is being made. People see climate change and it's like we're dead, it's game over. But it feels like battery prices are coming down, solar's coming down, wind powers ramping up, all these startups are investing. So it'd be fun to just hear what's going well and maybe what is there to be optimistic about, but then also, \"Okay, yeah, things are going well, but there's still things that are not going so great and where we need to double down.\"\n\nGustaf Alströmer (01:13:10):\nTwo specific things that went well in the last say 12 or 24 months. First one was politics. So in the United States we got the IRA, which is like it's called the Inflation Reduction Act. It makes sense because shift into greener energy is going to actually reduce inflation because it reduces energy costs. But it's really a climate bill, right? It really is a bill that is focusing on onshoring, a lot of supply chain for the green economy and incentivizing a lot of this change that we just talked about. Whether it's carbon removal or home energy or home heating, whatever it might be, this bill addresses all of it and is massive. So that's one really good news. Politics didn't have a lot of good news in the US for a long time on this. Maybe not ever actually.\n\n(01:13:52):\nThe second good news, and it's such a good news that Europe is now trying to conquer. They're not trying to counter the IRA with their bill because they're seeing some of the battery companies saying, \"Well, I'll actually going to build the next factor in US, not Europe anymore. I changed my mind.\"\n\nLenny (01:14:05):\nOh, wow.\n\nGustaf Alströmer (01:14:05):\nSo Europe now has to counter with their incentives as well. The second good news is corporations are now customers. So maybe three or four years ago you went to a Fortune 100 company and you're like, \"Hey, do you want to buy my XYZ decarbonation solution?\" What it's like? The software platform or EVs or whatever it might be. They're like, \"Well, talk to this person on this floor. Maybe they can help you.\" And this person was not really empowered to make decisions. That has changed. Now they're like, \"Actually, we promised our shareholders to reduce emissions by 2% every year and we also promised the government and we promised whatever publicly to do that. So we got to do that.\" They're like, \"Where do we start? What's the first 2% that we got to decarbonize? Maybe that starts with energy. Oh, we'll change our energy providers.\" But they are now showing up as customers, not just with LOIs but paying actual for contracts, right? Doing investments in these companies because they've all see the future and they don't want to be behind.\n\n(01:15:02):\nThere's financials motivations for this. They want to get access to capital that has some strings attached to some of these things, but they don't want to fall behind. And then they don't want to be the Toyota to Tesla, where Toyota said, \"We are not going to do battery electric.\" It's just like they're still saying that sometimes and everybody else is like, \"Tesla is the one we got to copy because that's the one that's winning.\" All these corporations are really afraid of being the Toyota. They're really afraid of being the last one who's not changing and then the world will move past them and then they're going to die. So I think the motivation here is intrinsically survival and it's really about sort of adopting this because this is where the world is going.\n\n(01:15:39):\nI think these are the two best news. The thing that I think what... We also wrote about this in the request for startups. This is not a thing where you can convince everybody to just agree with you. And even if you did, people wouldn't know what to do. So you have to view this as an economical opportunity and say... We can't convince everybody that this is going to be the thing that's going to happen. It doesn't actually matter if you convince everybody. What matters is that sufficient amount of corporations are convinced that they change their habits. And then you can sell the things you're building to them.\n\n(01:16:09):\nAs sort of founder, just focus on your customers and focus on B2B. That's what most people I recommend to do here because that's where most of the change is going to happen. That's why I'm really optimistic about these startups and these founders. What they're doing is, in my opinion, more impactful than someone running a campaign trying to convince some other people that this is a big problem. Even when people know that climate change is problem, they don't know exactly what to do about it. But the startup founders, they know.\n\nLenny (01:16:37):\nYou touched on this, but it feels like one of the biggest shifts is capitalism is kicked in and is now leaning into climate tech startups and that's-\n\nGustaf Alströmer (01:16:46):\nYes. Absolutely. Absolutely.\n\nLenny (01:16:48):\n... making a big dent. Well, with that, we've reached the final part of our chat, which is the very exciting lightning round. I've got six questions for you. Are you ready?\n\nGustaf Alströmer (01:16:58):\nYeah.\n\nLenny (01:16:59):\nWhat are two or three books that you recommend most to other people?\n\nGustaf Alströmer (01:17:04):\nThe first one I recommend, I think I have it here. Yeah, this one. It's called The 100% Solution. It's written by Solomon Goldstein-Rose. It's for people who think climate change is a problem but don't know what to do about it, or they're just kind of in despair or think they're like, \"Ah, everyone are going to die,\" right? There are some books written where the outcome of the book is like, \"We're all going to die,\" but the truth is that we're not all going to die. This book is trying to cover the 100% of all the solutions in detail, kind of much more detailed version of the request of Sharp that we wrote. It gets you freely optimistic. And I give that to anybody who's cared about climate change because it really lays out this from a very optimistic way of looking at the world. And that's why I recommend that book more than almost anything else. That's probably my number one book.\n\nLenny (01:18:00):\nAmazing. I love that if just a one book, here's the book you got to read. I like that approach. What's a favorite recent movie or TV show that you've really enjoyed?\n\nGustaf Alströmer (01:18:10):\nOh, I watched so much. I don't know. I love Emily in Paris on Netflix. I think I have TV serves different purpose for me these days. It's just like entertainment.\n\nLenny (01:18:22):\nYeah, I get that.\n\nGustaf Alströmer (01:18:24):\nBut what else movie do I watch? We watch the Everything All at Once. I thought that was a really good movie. That was-\n\nLenny (01:18:30):\nYeah, it might win Best Picture.\n\nGustaf Alströmer (01:18:32):\nYeah.\n\nLenny (01:18:32):\nWe have a drinking game here. People say White Lotus, we drink. And so you did not, that's probably for the best. Favorite interview question that you like to ask YC founders when you're interviewing them?\n\nGustaf Alströmer (01:18:43):\nWhat have you done since you applied to YC on your product? What are specific things that you've accomplished since you applied? Because that usually is a month or two month months ago.\n\nLenny (01:18:51):\nThat's awesome. It connects so much with what you said earlier.\n\nGustaf Alströmer (01:18:55):\nI hope the answer is like, \"Here are all the things that we did.\"\n\nLenny (01:18:59):\nVersus we just prepared for this interview.\n\nGustaf Alströmer (01:19:01):\nExactly. Exactly.\n\nLenny (01:19:02):\nMost out there wild startup you have funded?\n\nGustaf Alströmer (01:19:07):\nI think I would say when I stepped onto the hangar floor of Heart Aerospace. I can send you a photo. Literally, I am looking at an airplane that's being made and I'm like there's no SaaS company's office you can walk into and you just open your mouth and you're like, \"What the hell is this?\" There are a few of those companies that are space companies or airspace companies or something like that where it's just a different feeling that you fund them and you can touch it. I have a lot of appreciation for things like that now because they're much harder to do, but when you succeed, they're much more tangible and you can be like, \"I have a tiny little piece in this space rocket or this airplane that we funded or this satellite above us.\" I really think that that's in some way a strong legacy compared to some other things that exist and just replace other softwares. And all these are better businesses, but there are strong legacies.\n\nLenny (01:20:06):\nWhat's a pro tip for applying to YC?\n\nGustaf Alströmer (01:20:09):\nPro tip. Number one thing is go to YouTube and type in like... I think there's a video that we've recorded which is about how you succeed with your application [inaudible 01:20:18]. It's an hour long video that gives you all the pro tips on how to do it. We told people in advance, \"Once you've watched that video, then see if you know anybody who've done YC and then reach out to them and maybe ask them if YC is right for you, but also ask them what's important for you to, as you kind of approaching applying to YC, writing the application, during the interview, what were the things that matter?\" Those are probably the two things I would do.\n\nLenny (01:20:46):\nFinal question, what's one pro tip for visiting Sweden?\n\nGustaf Alströmer (01:20:49):\nFirst of all, you should visit in the summer. It's a really good time to be there. It's a very different country in the winter. Try to go outside the cities into the nature and then prepare yourself for Swedes not all being Americans. They're a little bit more colder and have a little bit more distance to you and they don't randomly talk to you like I've learned to do here in America. I think you just have to go along with a little bit different of a vibe than you have here in the US. Most people actually love it. Most people who have just been, they love it, but most of them go in the summer.\n\nLenny (01:21:23):\nThis reminds me, I wanted to close it, but there's a tweet once about how in Sweden when you go to someone's house, they don't feed you. It's not expected that you will have food. You have to bring your own food. Is that true? And what's that about?\n\nGustaf Alströmer (01:21:35):\nIt's absolutely true. I actually gave an unconference talk about this topic. The unconference talk was all of the strange things to Swedish people do and why. If i would summarize it, yes, we do that. I actually experienced that. I went to a friend's house and they had dinner and I waited in my friend's room while they had dinner. That was just normal. And why did that happen? I think there's a strong sense of individual responsibility in Sweden, which kind of reaches over to unfriendliness, [inaudible 01:22:10] from an American or foreign lens because this is so crazy. But in Sweden it's just like, \"Well, you take care of your kids. I'll take care of my kids.\" And it's not really a question.\n\n(01:22:18):\nI think that a lot of the motivations of why Swedes are strange, one of them is we don't want to be indebted to someone else. So we never want to feel like... Which is why you wouldn't... For example, if you go to a bar in Sweden, you don't buy a round, you buy your own beer because maybe you have to figure out the money at the end of the day, things like that. I think it's just actually quite individualistic society, but it's individualistic with heart I would say. There's a warmth to it, but it will definitely be appeared strange to people who don't really understand this. They think people are cold and they're just like they don't understand that there's actually a heart behind this stuff.\n\nLenny (01:23:00):\nThat sounds really smart, to be honest, the system. I would be into it, but I can see how people are very confused. Gustaf, I can't thank you enough for doing this. This was incredible. I know that people listening to this are going to leave informed, inspired, motivated, hopefully motivated to move faster and make more progress. Two final questions. Where can folks find you online if they want to learn more or ask you may be follow up questions? And two, how can listeners be useful to you?\n\nGustaf Alströmer (01:23:27):\nI tweet sometimes on twitter.com/gustaf. The most useful things that I put out is probably on the YC's YouTube channel. So I record a couple videos on growth, on sales, on how to talk to customers. I actually send them to people all day long because the Start School videos that we made are a lot of preparation went into it and it answers most of the questions that people have. So watch those first, I would say. But yeah, sometimes I tweet other stuff that people can follow. That's fine. How can people be useful to me? I love hearing feedback from founders, what they're working on. I want to hear kind of questions they have about their companies. But I want to also really emphasize that to apply to YC, you don't need to know any of us. You don't need to reach out to us. It doesn't make any specific difference.\n\n(01:24:15):\nThe principles in YC is that you should be able to become an insider in YC in Silicon Valley without knowing anybody. That's kind of what the application process is about. So feel free to reach out to us if you have questions, but don't feel like that's required to be a good YC applicant. It's actually the opposite in that we read and treat all the applications equally. Thank you so much for listening to this podcast. I mean, it made me happy you made all the way to the end.\n\nLenny (01:24:44):\nYeah, extra credit for listening to the end. I also just want to say while you're saying that, it feels like YC is such a good force for the world. It just enables so much innovation and progress. And if technology is what drives the world forward, IC is so at the center of a lot of that. So just huge props to what YC is doing and what you're doing, Gustaf.\n\nGustaf Alströmer (01:25:02):\nWe feel a lot of responsibility towards that. That's for sure.\n\nLenny (01:25:05):\nAll right, I'll let you go. Again, Gustaf, thank you for doing this.\n\nGustaf Alströmer (01:25:09):\nThank you so much.\n\nLenny (01:25:12):\nThank you so much for listening. If you found this valuable, you can subscribe to the show on Apple Podcasts, Spotify, or your favorite podcast app. Also, please consider giving us a rating or leaving a review as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at lennyspodcast.com. See you in the next episode."
}
```

Episode 103: The science of product, big bets, and how AI is impacting the future of music | Gustav Söderström
Guest: Gustav Söderström

```json
{
  "id": "gustav-söderström",
  "guest": "Gustav Söderström",
  "title": "The science of product, big bets, and how AI is impacting the future of music | Gustav Söderström",
  "transcript": "# The science of product, big bets, and how AI is impacting the future of music | Gustav Söderström\n\n## Transcript\n\nGustav Söderström (00:00:00):\nThe internet started with curation, often user curation. So you took something, some good like people or books or music, and you digitize it and you put it online and then you ask users to curate it. And that was your Facebook, Spotify, and so forth. And then after a while, the world switched from curation to recommendation, where instead of people doing that work, you had algorithms. And that was a big change that required us and others to actually rethink the entire user experience and sometimes the business model as well. And I think what we're entering now is we're going from your curation to recommendation to generation. And I suspect it will be as big of a shift that you will eventually have to rethink your products. We have to rethink the user interface and the experience for recommendation first era. And so what does that mean in the generative era? No one really knows yet.\n\nLenny (00:00:47):\nWelcome to Lenny's Podcast where I interview world-class product leaders and growth experts to learn from their hard one experiences building and growing today's most successful products. Today, my guest is Gustav Söderström. Gustav is a product legend and he's now the co-president, chief product and chief technology officer at Spotify, where he's responsible for Spotify's global product and technology strategy and oversees the product design data and engineering teams at the company. I've had Gustav on my wish list of dream guests to have on this podcast since the day I launched the podcast and I'm so happy we made it happen.\n\nLenny (00:01:19):\nIn our conversation, we dig into what Gustav has learned about taking big bets and what to do when they don't work out, how Spotify moved away from squads and how they structure their teams now, how AI is already impacting their product, and also the future of music generated by AI. Also, why all great products need to pull some magic trick, how accurately succession represents Swedish business culture, and his hilarious analogy of peeing in your pants. Enjoy this episode with Gustav Söderström after a short word from our sponsors.\n\nLenny (00:01:51):\nThis episode is brought to you by Microsoft Clarity, a free easy to use tool that captures how real people are actually using your site. You can watch live session replays to discover where users are breezing through your flow and where they struggle. You can view instant heat maps to see what parts of your page users are engaging with and what content they're ignoring. You can also pinpoint what's bothering your users with really cool frustration metrics like rage clicks and dead clicks and much more. If you listen to this podcast, you know how often we talk about the importance of knowing your users, and by seeing how users truly experience your product, you can identify product opportunities, conversion wins, and find big gaps between how you imagine people using your product and how they actually use it.\n\nLenny (00:02:33):\nMicrosoft Clarity makes it all possible with a simple yet incredibly powerful set of features. You'll be blown away by how easy Clarity is to use and it's completely free forever. You'll never run into traffic limits or be forced to upgrade to a paid version. It also works across both apps and websites. Stop guessing, get Clarity. Check out Clarity at clarity.microsoft.com.\n\nLenny (00:02:59):\nThis episode is brought to you by Eppo. Eppo is a next generation AB testing platform built by Airbnb alums for modern growth teams. Companies like DraftKings, Zapier, ClickUp, Twitch, and Cameo rely on Eppo to power their experiments. Wherever you work, running experiments is increasingly essential, but there are no commercial tools that integrate with a modern grow team stack. This leads to waste of time building internal tools or trying to run your own experiments through a clunky marketing tool. When I was at Airbnb, one of the things that I loved most about working there was our experimentation platform, where I was able to slice and dice data by device types, country, user stage. Eppo does all that and more delivering results quickly, avoiding knowing prolonged analytic cycles, and helping you easily get to the root cause of any issue you discover.\n\nLenny (00:03:44):\nEppo lets you go beyond basic click-through metrics and instead use your north star metrics like activation, retention, subscription, and payments. Eppo supports test on front end, on the backend, email marketing, even machine learning claims. Check out Eppo at getE-P-P-O.com. That's geteppo.com. And 10X, your experiment velocity.\n\nLenny (00:04:08):\nGustav, welcome to the podcast.\n\nGustav Söderström (00:04:11):\nThanks for having me, Lenny. Pleasure to be here.\n\nLenny (00:04:13):\nIt's my pleasure to have you on. So at this point, you've been at Spotify for over 14 years, which is a rare feat in the tech world, and you've held a lot of different roles while you've been at Spotify. Can you just start off by giving us a sense of what these various roles and what you've done over the years at Spotify and then just what are you up to these days? What are you responsible for now?\n\nGustav Söderström (00:04:33):\nSo I came into Spotify in early 2009, late 2008. And my job then, I had been an entrepreneur, started some of my own companies in the, back then, very, very early feature phone, smartphone space. So I had a bunch of knowledge there. I had sold the company to Yahoo in the mobile space. I worked there for a while. I came back to Sweden and then I met through a mutual friend, Daniel Ek, the CEO and co-founder of Spotify. And they had built the desktop product already, the free streaming desktop product, and it was amazing and I could try it, but they needed someone to figure out what to do with mobile. And because I had been an entrepreneur in that space, I got that job.\n\nGustav Söderström (00:05:20):\nSo my job was to head up mobile for Spotify and figure out what the mobile offering would be, which was a challenge because, obviously, Spotify desktop was a free on-demand streaming application and back then, specifically with edge networks, you couldn't really stream at all in real time. The performance wasn't there and also you couldn't fund that with an ads model. So it was a product and business model innovation that was a lot of fun. So that's how I started. Then after a few years, I took on all of product development for Spotify. And then a few years later, I actually took on the technology responsibility, the CTO role for Spotify as well. And recently, my official title is co-president of Spotify together with Alex Nordstrom. So we run half of the company each. I run the product and technology side and he runs for the business and content side. So that's the super fast version.\n\nGustav Söderström (00:06:20):\nAside from getting more responsibilities taking on the technology department, it has been the same job by title. I've always reported to Daniel, but because Spotify's grown so much every six to 12 months, it's been starting at a new company. First, it was sort of a Swedish Nordic challenge and then it was a European challenge and then it was getting into the US and then we became a public company. So it's as if I had jumped around between a lot of jobs actually, even though it was largely the same title and role.\n\nLenny (00:06:55):\nYour story makes me think of the classic be careful what you're good at because you end up taking on more and more. And clearly, you've been given more and more responsibility over the years and so clearly things are going well and you're doing well.\n\nLenny (00:07:08):\nShifting a little bit. So you're on my podcast currently. You actually have your own podcast, which was this limited series on the product story of Spotify, which I listened to and loved, and it's surreal to listen to your voice in real time because I've been listening to that recently in preparation for this conversation. Two questions, just what made you decide to launch your own podcast knowing you had a full-time job and a lot going on and the production value for your podcast was very high for what I could tell? And then two, just what did you learn from that experience in terms of the product you ended up a building and just empathizing with the podcast creator side of?\n\nGustav Söderström (00:07:41):\nThere were a bunch of different reasons why I did that. One is, and not a small one, I think, like you, I love writing and I have this secret creator dream in me. I used to write blog posts a long time ago and I write internally a lot. You can't write that much externally when you work at a company like this.\n\nLenny (00:08:01):\nYeah.\n\nGustav Söderström (00:08:02):\nBut I love writing and talking and presenting. So there was certainly that. And then no small part was, from a product point of view, to empathize with one of our main constituents, the podcast creator. I'm, unfortunately, not a great musician. I try to play instruments and so forth, but I don't have any records. I don't sing very well. But I decided to make a podcast and that taught me a huge amount about what it's like to be a creator, creating different styles of podcast.\n\nGustav Söderström (00:08:33):\nFor example, we wanted to do a higher production cost podcast with music and then right away you run into a bunch of problems as Spotify is actually pretty well positioned to solve, but still, it's really hard to have music in a podcast from a rights perspective. So you understand all these problems that podcasters have and you can be better at solving them. But the biggest benefit and the real reason for doing the public podcast was that I have actually done an internal podcast through a hack where we could gate the podcast-only employees. And I tried to figure out internally how to build more culture around Spotify and help define for new employees and existing employees, who we are, the mistakes we did, the successes we had, and how we think about strategy specifically in product strategy because we were quite well known externally for technology and the squads and all of these things, not so much for product strategy.\n\nGustav Söderström (00:09:36):\nAnd because I love storytelling more than Google Docs, I decided to do an internal podcast and I went around and I interviewed actually Daniel's direct reports. So the CMO, the CHRO and CFO. And just ask them about a bunch of stuff. And the idea was to make them more approachable for employees because I felt listening to podcasts, even these people that have no idea who I am because I've never met them, I feel like I know them, I feel like I know how they think and I just like them much more. So the secret idea was, what if you could get to know your leaders much better than you do through occasional meetings or some town hall? So I did that internally, and because I'm a product person, we ended up talking a lot about product and product strategy. And people internally really like that.\n\nGustav Söderström (00:10:26):\nSo next time, the question was, what if people that don't even work at Spotify yet could feel as if they knew people at Spotify? That'd be great because most leaders in most companies are very opaque and appear as some otherworldly creatures that aren't really real, I think, when you see them in business papers or something. So what if you have heard them talk for an hour or so? So that was general idea. So a combination of recruitment tool, sharing more about how we think about product strategy and just because I think it was a lot of fun. I got to interview a bunch of smart and interesting people both externally and internally.\n\nLenny (00:11:09):\nDid it have the effect that you were hoping after looking back?\n\nGustav Söderström (00:11:12):\nI think it did. The podcast did well and, no, we did not give it our own promotion. I had to compete as everyone else, which also gives you a lot of empathy for the problem of like, okay, now you have a product, what about user acquisition? How do you actually get people to listen to it? So it did achieve what I wanted in the sense that we have this thing called intradays where especially in the past few years when we've hired a lot, we actually fly people to Stockholm for an onboarding session to learn about Spotify. And the leadership is on stage, talking about what they do and the departments and strategy and so forth. And it's very common that people come and tell me that, \"Oh, I listened to this podcast or this and the episode and it's at least one of the key reasons why I joined or sometimes the reason why I joined.\" So it's anecdotal, but it may be in the many tens of people, at least, who have said it. So that seems to work.\n\nLenny (00:12:12):\nThat's really interesting. Just again, and this comes up a few times on the podcast, is just the power of content in all these different ways for hiring for culture building. And it sounds like the original goal was just internally build this clinic culture and strategy.\n\nGustav Söderström (00:12:24):\nThat was the original goal, make senior leadership more approachable and reduce the distance and then also share more of the thinking in an entertaining way rather than just through docs that people end up not reading.\n\nLenny (00:12:38):\nI love that. So I was listening to it, as I said, and what was really interesting is I think episode four was actually all about AI, and I think your first attempts at leveraging machine learning in AI within Spotify. And I think that's what led to Discover Weekly and a few other tools. And that was years ago. And it's interesting listening to it now where AI is, again, a huge deal. And so I'm curious very tactically on the product team what you advise product managers and product teams on how to think about AI in their product thinking and also just in their day-to-day work.\n\nGustav Söderström (00:13:12):\nI can give a few examples there and I don't know that we're more sophisticated than anyone else, but we'll be doing at least the traditional machine learning for quite a long time. And I think in the podcast, I think I talked about the journey of the internet in stages. And one way to think about it is that the internet started with curation of the user curation. So you took something, some good, like people or books or music and you digitize it and you put it online, and then you ask users to curate it. And that was your Facebook, Spotify and so forth. And then after a while, the world switched from curation to recommendation, where instead of people doing that work, you had algorithms. And that was a big change that required us and others to actually rethink the entire user experience and sometimes the business model as well.\n\nGustav Söderström (00:14:04):\nAnd I think what we're entering now is we're going from your curation to recommendation to generation. And I suspect it will be as big of a shift that you will eventually have to rethink your products. So that's one lens. So I tend to talk to my teams about, even though it's all machine learning, I ask them to think of this as something completely different. The recommendation error was one type of machine learning. The generation error is a different type, so don't think of it as just more of the same, think of it as something actually completely new instead. And what we learned in ... Well, a few things. So if you look at this new era of large language models and the fusion models and so forth, there are two types of applications. As I said for the recommendation error, we had to rethink the user interface and the experience for recommendation first error.\n\nGustav Söderström (00:14:57):\nAnd so what does that mean in the generative area? No one really knows yet. As usual, there are a bunch of iterative improvements. So we use these large language models to improve our recommendations. You can have bigger vectors that can have more cultural knowledge. You can use it for safety classification on podcasts that no one has listened to yet and so forth. So there's lots of obvious improvements and we're doing those. But so far, we've only really done one real generative product in the hard definition, which is a product that couldn't have existed without generative AI, and that is the AI DJ. So that's a concept that we've been thinking about for a very long time. And the AI DJ is you press a button, a digitized person, there's a real person named X, digitized X. So he's now an AI, comes on and talks to you about music that you like and suggests music, and you can listen to it. And if you don't like it, you can just call him back and he says, \"Okay, now, let's listen to something maybe from a few summers ago,\" or \"Here's some new stuff that were trending yesterday in The Last of Us episode or something like that.\"\n\nGustav Söderström (00:16:10):\nSo that product couldn't have existed without generative AI, both generating the voice and generating the content of what the voice says. So you can have individualized, personalized voice at the scale of half a billion people. And so we had the use case we have seen for many, many years. Sometimes people call it the radio use case. We called it the zero intent use case internally when you actually don't know what you want to listen to at all.\n\nGustav Söderström (00:16:40):\nSpotify wasn't that good. Spotify was good when, at least roughly, you knew the use case of what you want to do, if it was a workout or dinner. We had lots of options for all of those. But if you really didn't know at all, it was hard to open Spotify and stare at it. And people used to say longingly that this was the one thing that radio was good at. Radio was quite bad, to be honest. I mean, it's not personalized to you at all. It's not on demand. You come in in the middle of things, it's actually terrible in many ways. But people still often say that there was something good about it. And I think that's something was the fact that you had a knob and you could just switch between contexts. It's like no, boring, boring, boring, boring, okay, this is good.\n\nGustav Söderström (00:17:23):\nAnd Spotify never had that mode of, I don't know what I want, but I want to cycle through things until I find something that I like. And I think with the AI DJ, that's actually the use case we managed to solve. So X comes on and says, \"I'm going to suggest something to you that you can listen to.\" And if you like it, you can keep listening, but if you don't like it, you bring him back again and you change channel. And for one reason or another, we tried to solve that for many times for a long time, but just starting to play a random song without any context as to why you would hear this, it just never worked. So that was our first foray into a product that couldn't exist before. And I think to your question of principles around that, there are a few pretty distinct principles that we've learned.\n\nGustav Söderström (00:18:09):\nOne that I really like that is not my principle at all, I think it is straight from Chris Dixon, is the principle of fault-tolerant user interfaces. So I can't say how many times during the early machine learning era when we said we're moving from curation to recommendation. I saw a design sketch that was a single big play button because clearly that is the simplest user interface you can do, but if you don't understand the performance of your machine learning, you can't design for it. The quality of your machine learning, if you're going to have a single play button, needs to be literally 100% or zero prediction error, and that's never the case. So let's say that you have a one in five hits, four out of five things are done, then you need a UI that probably at least shows five things at the same time on screen. So you have a one in five of something being relevant on screen.\n\nGustav Söderström (00:19:03):\nSo you need to understand the performance of your machine learning to design for it. It needs to be fault tolerant and often you need an escape hatch for the user. So you make a prediction. But if you were wrong, it needs to be super easy for the user say, \"No, you're wrong, I want to go to my library or to this or to that.\" So we have that principle of having fault-tolerant user interface and a user interface that corresponds to the current performance of your algorithms. And I think that is going to be true for generative machine learning as well. I think a very clear example actually is Mid Journey. If you think about the early Mid Journey user interface inside the Discord channel, actually generating an image was very, very slow.\n\nGustav Söderström (00:19:48):\nIt took a long time to generate high-quality image and they could have built the silver button thing where you put in a prompt, you wait for minutes, you get an image, and I think one out of four times, it's going to be bad. So you would've been disappointed three out of four times and it's a minute each, so like four minutes later, you'd be, \"This is a shitty product.\" What they did was they generated four simultaneous low-res images very quickly and you could say, \"So apparently, their performance was probably one in four, that's why they showed four and not six.\" And so one in four was usually pretty good. You click that one and either continue to iterate or scale it up. So that's also an example of, I think, people understanding where the performance of generative AI was when they built the UI. So that's something that I would be inspired by.\n\nGustav Söderström (00:20:37):\nAnd for the AI DJ specifically, another principle is to try to avoid this urge of just wanting to show off the technology and then have this voice that talk and talk and talk and talk. You have to remember that people came there for the music. So the principle for the AI DJ coming from the team, by the way, this was a bottoms-up product actually, it required a lot of support. We actually acquired big companies and so forth to be able to build it. But the idea had been built by teams bottom up. So the principle there was literally to do as little as possible and get out of the way. And I think that was really helpful. It's not telling you what the weather is and what happened in the news and going on and on and on about this band. It is trying to get you to the music and I think that's why it's working because it is working very well for us.\n\nLenny (00:21:24):\nI love this distinction between recommendation and generation. And this begs the question of, there's this trend that I imagine you're seeing of people autogenerating music using artists catalog. There's this Drake and The Weeknd thing that came out a week or two ago. Where do you think this ends up going and how do you think artists adjust to this world where music can just be autogenerated? This play button is all of it is generated versus just like the DJ in between the songs.\n\nGustav Söderström (00:21:53):\nFirst, big caveat, this is just super early. No one knows anything about how this is going to play out or the legal landscape and so forth, but I think it's going to have a lot of impact. And I think if we talk about two things, one is what it could do for music, the other is the right situation, and if rights-holders are getting compensated and so forth. So we talk about the first thing in isolation. I think an interesting example is right about when I grew up, Avicii came along. And it's interesting to think about because Avicii was not really considered by the existing music industry as a real artist because he couldn't really play an instrument and he couldn't sing, and he was just sitting with this computer in this DAW, digital audio workstation. And so it wasn't really considered real music. And I think now all of us consider it very real music and that he had tremendous real musical talent.\n\nGustav Söderström (00:22:51):\nSo I think right now, we're probably in the face where people say this isn't real music and it's somehow fake. I think the way to think about these diffusion models if and when they get good enough at generating music is probably the same like an instrument. It's just a much more powerful instrument and we'll probably see a new type of creator that wasn't proficient at any instrument and they couldn't assemble a full orchestra and do the thing that they had in their head and they can now generate very new things. I also think, by the way, that there is this distinction between AI music and real music that doesn't exist. For sure, very talented real musicians are using AI to get better and to help create new ideas. So that distinction doesn't really exist. It's all going to be AI. The question is what percentage, which makes the problem harder because you can't talk about if it should exist or not.\n\nGustav Söderström (00:23:50):\nYou have to talk about what percentage should exist and who gets to use it or not. But I think the way to think about it is probably as an instrument and that could help create a huge amount of art. And I think this is not news to you who probably use these things a lot, but I think if you don't use these generative models, there is the perception that you tell it to create a hit and you will get that. That's not how it works. Actually, what these models do is because they've been listening to a lot of music, they are very good at doing something that sounds very similar to what already exists. Actually being original is very hard. And from one point of view, as it now gets easier to create more generic music, it will actually be more difficult than ever to be truly unique.\n\nGustav Söderström (00:24:39):\nSo I still think there would be tremendous skill in creating something truly unique. And my hope would be that what happened with the DAW and that technology jump was you got a whole new genre like EDM that you couldn't really produce it with an orchestra or live. And maybe we'll see completely new music styles with these technologies. I think that would be very exciting. So that's on the positive side, but then you have the rights issue, which I have a lot of empathy for. And Spotify specifically has seen this before. So we had a different technology shift like this, which was the technology shift to online downloads of music and piracy and peer to peer. So first, it was a big technology shift in peer to peer and it was exciting for consumers. More consumers started listening to more music than ever. And I think that's where we are now with generative AI.\n\nGustav Söderström (00:25:31):\nThere's a new technology, but it also required a new business model before creators and industry could actually participate and benefit from this. And that's, obviously, self-serving to say because we were a big part of innovating that business model. But I still think that's what's necessary and I hope that that's what I and we could be part of. So I think we've seen the first part, the technology shift, and there will probably be a lot of discussion and chaos here which have a lot of empathy for, but I think we haven't seen the second part yet. What is a model where this could be a benefit? What actually happened after piracy is that the music industry got bigger than ever, not just as big but bigger than ever. And I think that could happen with this technology as well. But we're right in the beginning.\n\nLenny (00:26:20):\nSo along the same lines, something else you teach is this idea of all truly great products have to pull some magic trick. This comes up in your podcast a lot and I think you mentioned this other places, and thinking about all the stuff you're talking about here, it feels like, in a sense, everything's going to feel like magic because AI's baked into it.\n\nGustav Söderström (00:26:38):\nI think when we did the AI DJ, we did a small version of that. When people first listened to it, we could see that reaction in use of testing when they've wondered ... So the magic trick there was that how could they record this person saying so many different things because it's talking about my music. So the magic trick was, obviously, didn't record a person saying, it's generated, and that magic trick wears off. You hear it all the time now and so forth, but it was one of those magic tricks. So I still think that concept is important and it seems to correlate with products going viral and taking off.\n\nGustav Söderström (00:27:15):\nAnd I think it was the same using something like Dall-E or Stable Diffusion or Mid Journey the first time. It completely seemed like a magic trick. And, obviously, there is no magic, it's just data and statistics. But I think getting to that point and iterating a product to the point where it feels like magic the first time is very helpful. And it's often a question of just getting the performance to a certain level, scoping down, removing things. There's a lot of fine-tuning, I think, that makes you cross that line from it's cool and impressive but not magic to it feels like magic. I don't understand how this could be done.\n\nLenny (00:28:00):\nYeah, it reminds me of the launch of GBT which ended up being the biggest, most fastest growing product in history, and it's like the epitome of a magic trick. It feels like actual magic.\n\nGustav Söderström (00:28:09):\nAbsolutely, absolutely. And to most people, it is still very ... Actually to a lot of ascent, even to researchers, it's a little bit magical. No one really understands fully. So I guess there's maybe some magic left in the world.\n\nLenny (00:28:23):\nAbsolutely. And I think a lot of people are worried about not understanding what's going on there. Shifting to the way you all build product at Spotify. So Spotify is famous for popularizing this idea of squads and tribes. And correct me if I'm wrong, but you guys have moved away from that approach.\n\nGustav Söderström (00:28:39):\nYeah, that's right.\n\nLenny (00:28:41):\nOkay. So I'd love to understand just why you shifted and what you learned from that approach to building product, and then just like how do you organize the teams now? What do you do now?\n\nGustav Söderström (00:28:51):\nThis was something that we focused a lot on early and it turned out to be smart of us to name these things into squads and chapters and so forth. It wasn't really ... Well, maybe it was deliberately branding, but it wasn't for purposes of branding that we made it up. We made it up because we thought it was a good structure to use and we needed names for things and this was the early internet eras you were allowed to make things up. And so it was very good for where we were at the time and it certainly helped us in recruiting. It's become a little bit of a cost to us because people still think that we organize that way and it's not a very efficient way of being organized at this scale or maybe even if you started over right now because we've learned more.\n\nGustav Söderström (00:29:35):\nBut I think the big difference is the idea with the squad specifically was twofold. They were supposed to be small and full stack. So squad should be about seven people and it should have front and backend, mobile, QA, agile coaches and so forth, and it should be very autonomous was the idea. And that's really what we shifted. So, first of all, as you grow the company, scaling in increments of seven engineers just creates a ton of overhead. So, obviously, our teams now tend to be much bigger, maybe two, three times that at least per manager to maybe have 14 or something instead of seven and just less overhead roles. So that's one. It looks more traditional as you learn more and is reasonable as you scale. The second big thing I think we struggle with was back then when I joined, the average age at Spotify was ... I mean, I was the oldest and this was 14 years ago. So I think the average age was probably under 30 or something and it wasn't most tech companies.\n\nGustav Söderström (00:30:46):\nAnd so we had coming from Sweden, which is a different culture than the US, and I love a lot of things about Swedish culture and I think we managed to keep the best parts, but Sweden is a very bottoms-up autonomous culture. There's this famous drawing on how you make decisions in Sweden. In the US, I think it's just a hierarchy. In Sweden, it's a circle. It's in a circle, no one is in the middle, there is no leader and so forth.\n\nLenny (00:31:13):\nInteresting.\n\nGustav Söderström (00:31:14):\nSo I think by culture, we're very inspired by this super autonomous thing. And I think the idea with autonomy is very reasonable and the right one, which is we work and we are hiring the smartest people we can find and we pay high salaries for that. So if you're hiring smart people, one way to think about it is you're renting brain power.\n\nGustav Söderström (00:31:39):\nSo if you're renting all of this expensive brain power and then you give them no room to think for themselves, that doesn't sound smart, then you should actually hire less smart people and keep your costs down or something. So I think you have to give a bunch of autonomy to actually maximize the value of the investment you're making. So that's very reasonable that you would give a lot of space for people to use as much of their talent and capacity as possible. But the problem with that is if you put autonomy very far towards the leaves of the organization, and also if you combine that with having a very junior organization, which we did back then, there's a fair chance that you're just going to produce heat. You're going to have a hundred squads with a hundred strategies running in a hundred directions. And Spotify has been there in that camp.\n\nGustav Söderström (00:32:29):\nI mean, we managed to get somewhere, for sure, in spite of this, but I'd struggle to say we were efficient in doing that. So we've done a few things. The team structure is more traditional, larger teams, less overhead. And we've been specifically working with where in the org do we put the autonomy because the extremes are at the leaves and we were there. The other extreme may be at the top, let's say maybe some Twitter, there's one person. Both have problems. If you have it at the leaves, you're going to produce a lot of heat. If you have it at the top, you need someone with a lot of capacity and Elon has a lot of capacity, but you are, by definition, going to bottleneck. All decisions have to go through there. And Daniel, it's not his personality that he even wants to make all the decisions.\n\nGustav Söderström (00:33:19):\nHe wants to maximize throughput rather than to bottleneck the throughput. So the question is, if it's not at the top and not at the very bottom, where do you put it? And what we've found, which I don't think is very contrarian at all, I think this is the case in most companies, is around the VP level. So if you have Daniel, then you have the C level, myself and others, then you have the VP level, that is a good mix of instead of having one person in the company think, so only Daniel then and the rest just do, you have on the VP level in the company this many tens to maybe hundreds of people that have a lot of autonomy to think. So you get a good amount of freedom of thought and people thinking in different directions, but it's not like 8,000 people. And these people on the VP level are both quite a lot of them, but they're also usually quite senior. They have a lot of pattern recognition.\n\nGustav Söderström (00:34:16):\nSo I think that solves for, it's a good ... If you think of it as an optimization problem, it's a good optimization space. So the autonomy level in Spotify now tends to be quite high at the VP level and then lower around those levels.\n\nLenny (00:34:33):\nAnd when you say autonomy, what does that actually mean? Is it the VP of, say, the podcasting product has a lot of say over what happens and there's not a ton of ... I don't know how involved are people above? And I know Maya's the VP of product, I believe, for the podcast product.\n\nGustav Söderström (00:34:48):\nExactly.\n\nLenny (00:34:49):\nWho I think is going to come on the podcast someday. What does that mean in terms of Tommy for her, what practically?\n\nGustav Söderström (00:34:54):\nSo it means that I would ask Maya to define a strategy for what we do in podcasting, how are we going to be different, why would a podcaster want to be here? Whereas another company, I will make that strategy or another company, Daniel would make that strategy. Same with ... The AI DJ, for example, came from our personalization team. And so that was a bet that they made. So they have autonomy to make those kinds of bets and define strategies. Same with the user interface, we have an experienced team, can talk about the org structure later, but I put a lot of autonomy on the VP of experience to define and suggest what it is that we want to do. And in other companies, I would define all of that myself, for example.\n\nLenny (00:35:45):\nJust going even a little bit further here, I know you have just strong opinions on the way to organize teams and how the organization helps you optimize for specific things. What are your just thoughts along those lines and what have you learned about how the impact of organization and what you're optimizing for?\n\nGustav Söderström (00:36:03):\nYeah. So I talk about an idealized spectrum or maybe not idealized but exaggerated spectrum. Nothing is really true, but you create extremes to make a point. So on one spectrum, you have something like Amazon, which is known for two-pizza teams, no dependencies. You try to minimize dependencies so you can run in parallel. Teams compete with each other even on the same project and so forth. But they have direct access to the user.\n\nGustav Söderström (00:36:37):\nAnd so the benefit here is if you have an idea, the time to get the user is very low and it has worked for them. It's produced Kindle, it produced Alexa, it's produced a lot of very novel things. There are a few interesting downsides here. One downside that I'm extremely impressed with Jeff Bezos' foreseeing is if you have teams that compete with each other, the incentives are to hide your results, hide your code. And that should make for an organization that gets no platform leverage because no one's corporating. And I think either he had that insight or because he saw this, he had to do this, but he's well known for pushing extremely hard on hard APIs. If you don't create hard APIs to your technology, you're out. And if you think about it, it has to be that way because otherwise no one would do it.\n\nLenny (00:37:32):\nAnd a hard API is essentially everyone knows how to use this API and connect to this team to interface with.\n\nGustav Söderström (00:37:38):\nExactly. You have to expose your technology to others. You have to maintain those APIs and they have to be very structured because otherwise the whole thing would collapse as everyone's supposed to compete because there are no incentives. You have to centrally force that. And interestingly, even though theoretically then they're the worst position to have a structured platform, I think, because they forced it so hard, they were the ones who did Amazon Web Services because they had such hard defined APIs because of this rule that it was easier for them to turn it inside out and expose it the rest of the world. Whereas if you look at someone like Google, I think they struggled more with externalizing their APIs maybe because it is so friendly and soft. So they didn't need as hard APIs on the inside because there was no competition. People could just go into each other's code.\n\nGustav Söderström (00:38:18):\nSo it's interesting anecdote around it, but the main point is you're faster there, but it's going to be hard to corporate. And so you will see something like maybe exaggerating a bit. Sometimes you'll see multiple search boxes on the same page from different teams. And this has been true in Spotify, by the way, as well. You've seen multiple toasters on the Now Playing view coming up from different teams because they're working. When we were in the autonomous mode, everyone running. And then ... So you get the benefit of speed, but you get the drawback of shipping your org chart and shipping complexity to the end user. But clearly, that's been the right choice for Amazon because they're a trillion-dollar company. But then on the other spectrum, you have something like Apple who's also a trillion-dollar company. So clearly, both models work, where you would never see two search boxes from the same team popping up on an iPhone. That is centrally organized by something that is close to single individual.\n\nGustav Söderström (00:39:20):\nSo they are instead in what is probably the world's biggest largest functional org, they're doing as much. If you think about what goes into the Apple, I mean, they certainly do everything we do. They have music service, podcast service, audiobooks, and they have a billion other services. So it's not like they have an easier problem. And yet they build something that feels more like it was built by a single developer for a single user. So they centralize and they have this bottlenecking function that everything has to go through and be decided how it fits with everything else. And so that has the benefit of the user experience being simpler and not shipping the org chart and increasing complexity. But it also has the drawback of speed without having facts on it. I've heard people working at Apple have said, \"Yeah, it took seven years to get that thing to market,\" because you just had to wait in the pipeline.\n\nGustav Söderström (00:40:17):\nSo you have these extremes. And I think the most interesting example, I think, to think about is when you double click the power button on an iPhone, the Apple Pay comes up. That decision, how did that happen? You can imagine that all the services team would like to pop up when you double click that button. And so someone had to decide, should music come up, should payments come up, should something else come up? And so they have a different structure there. And on that spectrum of centralized versus decentralized, because of our strategy, which is we're a single application, trying to add or not trying to, we have added multiple types of content with actually very different business models on the backend, rev shares and royalties and book deals and so forth into single user experience. That is our strategy. We think the user experience in keeping that simple is the most important thing.\n\nGustav Söderström (00:41:12):\nSo we've chosen more of the centralized model, where these different vertical businesses, if you think about it, the music business, podcast, audiobooks business, they have it to go through a single recommendation organization because that's another problem. Which one do you recommend to which user? Should be a book or podcast or music? And how do you weigh them against each other? And also the user interface could easily get incredibly complicated if everyone built their own UI. The music team built their UI and then someone added features on top. So that's how we chose to optimize. But it is based on our strategy and I think both models work.\n\nLenny (00:41:50):\nThis episode is brought to you by Eco. Last month, Eco users earned an average of $84 in cashback rewards. How? With Eco, the future of personal finance. Eco is the update to a misaligned financial system. Providing an app that works just like your bank but removes almost all of the middleman, helping even the best money optimizers optimize in less time automatically. What if you earn rewards for paying your rent or got rewarded for ordering food and shopping online or even earn rewards for saving each month? And then imagine if you got rewarded again just for getting rewarded. With Eco, you can spend at some of your favorite merchants and automatically get 5% cashback, plus Eco's APY rewards look more like $80, not 80 cents. And then there are Eco points, the world's first open reward system. You earn them whenever you do almost anything in the Eco app.\n\nLenny (00:42:40):\nEco is working to make these points the most rewarding points ever. So it pays to be early. Sound too good to be true? Go to eco.com/lenny, sign up for an onboarding and find out why it isn't. Lenny's Podcast listeners who attend an Eco welcome session will get an exclusive 4% APY on deposits over $1,000. Learn more at eco.com/lenny, that's E-C-O.com/lenny.\n\nLenny (00:43:05):\nIt's interesting these two examples you gave, Apple and Amazon, they're two of the biggest companies in the world and they're like at the extremes of these two into the spectrum. And it's interesting, most companies are somewhere in the middle. I wonder if there's just a benefit to being in an extreme and that ends up being really important.\n\nGustav Söderström (00:43:21):\nI think so. In almost all industries, you have this smiling curve concept, where you want to be at the extremes on the smiling curve, and that's where big business opportunities are but not in the middle. So it's probably true in terms of organizational models as well.\n\nLenny (00:43:34):\nSpeaking of extremes, I want to talk a bit about taking big bets. So you guys had this big launch event recently where you basically redesigned the whole primary feed of Spotify to make it feel more like where apps are going, like TikTok reels feel of just stream, and you start hearing videos and music starts playing and some people loved it, some people did not. And I'm curious as a product leader, how you think about thinking long term and dealing with people that are just like, \"What the hell's ... I hate change, stop changing things.\" How do you think about that? Who do you listen to? Who do you ignore? How do you know to stay the course? How do you approach that?\n\nGustav Söderström (00:44:10):\nYeah, you're being very kind. There was a lot of negative feedback on Twitter on some of that. So let me actually dig into some detail because I think for product people listening to this, this is an interesting lesson that I think few companies talk about because you want talk about everything that went exactly as you thought they would and you don't want to talk about the things that didn't go exactly as you thought they would.\n\nGustav Söderström (00:44:39):\nSo I'll go through what we are trying to achieve and what we learned. So Spotify is mainly a background application, and for a long time, we've been considered very good at background music and podcast recommendation. When the phone is in your pocket and you're listening to an EDM playlist or pop playlist or something, we're really good at inserting another EDM track there or another pop track there or something like that in the background.\n\nGustav Söderström (00:45:09):\nWhat we hear from users again and again, though, is that they say that they get trapped in a taste bubble. So I love my Spotify, I love this, but I'm a little bit bored with EDM now and Spotify's not suggesting something completely new. And if you think about that problem, it may sound similar to the recommendation problem, it's just another recommendation problem, but it's actually fundamentally different because when you're recommending another EDM track inside the EDM playlist, you have a lot of signal from that user that they like EDM. But if you're going to recommend a completely new genre, by definition, you have no idea. Because if you had an idea, it wasn't new to them. So you can't know anything. So back to hit rate, your hit rate is going to be incredibly low when you suggest something completely new to the user.\n\nGustav Söderström (00:46:03):\nSo this problem of helping people get out of the taste bubble isn't that easy as it sounds. And we can't really take some genre that maybe isn't typical. So I'm a big fan of Reggaeton, for example. It's not typically ... It's not that common in Sweden. And if you would look the rest of my profile, it's EDM heaviness, you probably wouldn't have guessed it. And Spotify wouldn't have guessed it. So if I'm listening to my favorite EDM playlist in the background or maybe my metal playlist, metal is very big in Sweden, it's really hard for us to just insert a Reggaeton track in the middle of that. Most people are going to think Spotify's broken.\n\nLenny (00:46:41):\nYeah.\n\nGustav Söderström (00:46:41):\nWhat the hell are they thinking? So that doesn't really work. So in order to help people break out of their taste bubbles, you need something different. You need something where your hit ratio can be very low and you need people to expect it to be very low.\n\nGustav Söderström (00:47:00):\nSo when we recommend things in the background, our hit ratio needs to be at least nine out of 10, maybe one dud is okay, but if you get five duds, you're going to think we broke your playlist and your session. We need something where one out of 10 is a success. If you find one gem out of 10 tries, you're very happy. So you need a completely different paradigm. And you also need to be able to go through many candidates quickly because the hit rate is so low. You can't take three minutes per item. It's like, \"Okay, I didn't like this,\" and it's still like two minutes left before the next one comes on. You need to quickly say, \"No, no, no.\" So the obvious candidates for this are these feed-type experience, where you can go through lots of content, you're expecting the hit ratio to be much lower. And if you don't like it, the cost is very low, you just swipe.\n\nGustav Söderström (00:47:50):\nAnd then this is the reason why people have been ... When they want to break out of their taste bubbles or when they come into Spotify and listen to something completely new, it is usually because they found it on one of these services, like a TikTok or YouTube or something, where they get exposed to lots of new content. So people were asking us for these tools and so that's what we wanted to solve for. And so we built a bunch of features, feed-like structures, where you can go through either a new genre with many tracks or a podcast channel with genre with many episodes or even full playlists. And we implemented those and we put them in something called subfeeds. So in the current experience, and this is roll out worldwide, if you click the podcast subfeed, you get a feed of podcast episodes. Click the music subfeeds, you get a feed of playlists where you can go through many playlists. And if you don't understand the name, you can quickly hear what they sound like and check out a few tracks and understand if this is for you. And if you go through the search and browse page, you can find completely new genres that you can quickly go through.\n\nGustav Söderström (00:48:59):\nAnd so those are working as we intended. People go to them when they want to find new music. They browse through them and they save new songs. So they're working as we intended. The thing that didn't work as we intended was when users asked us for this again and again, we took the sum of these things and we put it on Home because people ask so much about discovery and we can see clearly how correlated discoveries with retention on Spotify and so forth. But what we misjudged or failed or rather learned about our own homepage is that the way it works right now, and this is what you can see in the Twitter comment, if you remove the angry voices and try to see what they're saying, they're saying the following, which is actually quite clear in the quantitative data as well, that if you look at what people do on Spotify's homepage, the current one, it is almost 90% what we call recall.\n\nGustav Söderström (00:49:59):\nSo it is either getting to a session that you're already in or a specific playlist that you know you want to get to or at least a specific use case. So you come in with a high intent, you actually knew what you wanted, and maybe only 10% of the time as a true discovery, like I don't know what I want. So if you think about, that is 90% recall and 10% discovery. When we tested the design ... So the subfeeds were working and not working, but when we tested some of them on Home, we switched it from 90/10 to 10/90. So 10% recall, 90% discovery. And while people want discovery, they probably don't want 90% discovery, instead of 90% recall. So if you then look at the comments on Twitter, what they're saying is like, \"Hey, I can't find my playlists anymore. Where are these things?\"\n\nGustav Söderström (00:50:47):\nThey're not really complaining about the discovery, they're complaining about the things they don't get anymore. And we can see this in the quant data as well. And you can see traffic shifting from home into search and into library, which is a clear sign people are trying to find the things they can't find anymore. And you can even see people then trying to use these discovery tools which are optimized for quickly understanding new things to do the recall. Where's that workout playlist I know I want? And it's actually very bad UI for recall, it's like a slot machine, right? Very unpredictable if you ever get to that workout playlist. It was optimized for finding new things, not for recall of existing things. When you do recall, you want the dense UI with many items on screen because you know what it is you're looking for. So you don't need a lot of real estate when you're doing discovery of new things. You want a lot of pixels and you probably want sound because you don't know what it is.\n\nGustav Söderström (00:51:40):\nSo what we learn about our UI, and I think there's maybe a little bit of product jealousy here, you always look at other experiences. And if you look around, it could be forgiven for thinking that most other products, if you look at something like YouTube, for example, their homepage is exactly that. It's a huge single-item discovery feed with only new items. And people don't seem to tweet angrily about how angry they are at you to say they love YouTube and it's a big product. And I think what we discovered was that we actually did something really well on our homepage, which was supporting you being inside a multiple sessions at the same time. So you could be in the middle of two podcasts and an audiobook and also them actually I just want to get to that workout playlist. I don't remember the name of it, but I know it's workout.\n\nGustav Söderström (00:52:31):\nWe actually did that part really well. I would venture just say much better than the other experiences where you literally have to go to some tab and into library and start browsing to get back to where you were. And so maybe it's path dependent. Because we have done recall pretty well, people got, I think, reasonably upset when they couldn't do the recall anymore. And we didn't want lose that because it was one of the things we did well and underestimated. And my takeaway is actually we do it better than other experiences. So we certainly want to keep that. So what we did was now we're just updating their hypothesis to achieve the same goal, which is these things are working and when people want to discover, they use them and they seem to work, they can also get better.\n\nGustav Söderström (00:53:20):\nYou're on this hill-climbing journey from a machine learning point of view, but the question is, how do you make sure that whenever people feel that they are in that I'm trapped in my taste bubble, they understand that these things are there and they're easy to use? So now we have a version of Home that we are also testing, obviously, where these things are very available but voluntary and you can still do all of the recall. And so from my point of view, this is the reason we A/B test because you want to be scientific about it and you want to learn as much as possible about your own product and your users. And now I'm sharing a lot of the learnings. Maybe we should keep them to ourselves, but my hunch is that it's going to make it a much better product.\n\nGustav Söderström (00:54:09):\nBut what I told my teams when we went into this, because I've done this a few times, agree to signing, I think there are two fundamentally different types of product development. One is designing a new feature. It is hard, but it's voluntary for people to use. So you do the AI DJ. Some people love it, that's fine. If you don't like it, it didn't make it worse for you. But when you redesign, it is much more tricky because it's not voluntary to participate in the redesign. So there's a cost even for people who don't like it. Then you have a very tricky problem here, which is there are going to be two types of feedback. One is you did something and it was right, but people are upset because you changed stuff. The other is you did something and it wasn't right, and people are also upset but for good reasons.\n\nGustav Söderström (00:55:08):\nAnd so how do you separate these two? Because I think I explained this to ... When we talk through this with my teams, I think the analogy to think about is you have your desktop, your physical desktop, you have your computer in one place, you have your pencil over here, you have your notebook over there, and I come in and I just rearrange all of it. And you have spent, in our case, maybe 12 years with that setup. It doesn't matter if I have a lot of quantitative data that my new setup is better, you're going to get upset because you are effective in this old setup. And it's hard to tell those apart. The most classic use case is the Facebook newsfeed, which people are very upset about when it became a single newsfeed. But it turned out to solve a lot of user problems that you didn't have to run around all of Facebook collecting events yourself.\n\nGustav Söderström (00:55:58):\nSo there are some ways of understanding if you made it better, but people's habits are broken or if it's not better. And one thing is, for example, to look at new user cohorts that don't have that behavior versus all user cohorts and so forth. So we went through all of this with the teams. Before we did it, I said, \"This is going to be painful.\" There's probably going to be a lot of tweets because chances that we get it exactly right are very low. So for that reason, it hasn't been very hard on the team. It is hard ... You want to respond to people, but the right way to do it is to listen, understand, try new hypothesis to really figure out what's going on. So I think I've done it maybe three or four times now. Three maybe. One unsuccessful, two successfully. So kind of knew what I was getting into.\n\nGustav Söderström (00:56:45):\nSo it's almost like you punish yourself, very painful, but also the most exciting things. And I think any product person knows that the easiest, the most straightforward thing to do is to iterate around where you are. There's no risk. You're not going to get fired, no user is going to get angry. But everyone also knows that eventually if you don't adapt new technologies, new paradigms, et cetera, you're going to get replaced. You have to find this balance of trying new things. And when you work in software, you have this tool of A/B testing and being scientific about it. When you build hardware, it's worse. If you're wrong, you're wrong. You can't update.\n\nLenny (00:57:26):\nI love this story. I so appreciate you sharing it. I imagine also with a big launch like this, you can't actually A/B test it ahead of time because of the press season. They're like, \"Oh my God, look what Spotify's doing.\" And so you're limited there. Imagine, right? You couldn't really test this ahead of time.\n\nGustav Söderström (00:57:40):\nThe hardest thing about this is if you're trying something completely new, the MVP needs to be very big so you can build a new IU, but if you didn't do algorithms for single item feed, you can't tell if it was the right idea but poor machine learning, right? UI poor machine learning. Or you have to build a lot and that gets quite expensive. That's actually ... The biggest why it's painful is not really the feedback from the outside. It is the cost you have to take on the inside. You incur a lot of costs as you're really hoping you're right.\n\nGustav Söderström (00:58:15):\nAnd in our cases, the changes on the homepage aren't that hard for us to do. The important thing is that the underlying hypothesis of, can we help you break out of your taste bubble actually works and then you update the acquisition funnels into that experience. But I think the problem is that you need to get so many things in place to be able to say, \"You might get a false negative,\" just because you didn't do it well or not. That's the biggest challenge, I think, with these big rewrites where everyone has to update everything before you can know if you're right or wrong.\n\nLenny (00:58:52):\nWhat was that process like of helping you understand what is not working and what is working and what you wanted to change? I imagine there's a bunch of data you're looking at, some tweets, things like that. What was the tactical, \"Oh, shoot, something's not going the way we expected, here's what we should do?\"\n\nGustav Söderström (00:59:08):\nWell, the feeds, we tested, but the home feed, we rolled out and tested afterwards. And we tested out on users, a few different variants of it. And then we got the data back and we looked more at the quantitative data. And we do a lot of user research where people sit and use the feeds to understand and build our own theorem mind of what is working and what is not working. And then, obviously, you look at user feedback, of course, and some users are very good at expressing what is of it that isn't working, others are not as good as expressing what isn't working. So it can be hard to parse that, but certainly, that's a factor as well.\n\nGustav Söderström (00:59:49):\nAnd so then once you do that, then you have quantitative data to look at. And then you sit in recent through, what do you think is right and wrong? What are the different hypotheses? What is working, what is not working? And then just update and test again and again until you prove or disprove your hypothesis. Trying to be as scientific as possible about it. And also I think the biggest risk also when you've invested so much time in something is getting precious about things. You have to just be brutal. You have to believe in things 100% until the data says no and then you believe in something else 100%. That sounds easy. It's very hard to do, to the extent that people get upset when you do it because, for some reason, people don't like when people change their minds. It is what we should want from everyone. I would love a politician who said, \"I'd looked at the data and I realized actually this is right and now I believe this.\" But we hate politicians that do that. They feel untrustworthy and we ridicule them.\n\nGustav Söderström (01:00:56):\nSo I think that's the biggest risk with anyone. You just have to be unemotional and just look at the proof and the data. And then if you do that, you just move on and then you get to where you want to be, and you solve the same problem but you adapt.\n\nLenny (01:01:14):\nI really like that philosophy. Essentially, it's the idea of strong opinions loosely held. Is that-\n\nGustav Söderström (01:01:19):\nExactly. Exactly what it is. And it sounds so easy, but it's hard.\n\nLenny (01:01:23):\nRight? Because to your point, people don't respect someone changing their mind. They're like, \"Oh, I see, they were wrong the whole time and they were so confident about being wrong.\"\n\nGustav Söderström (01:01:31):\nYeah, exactly. And it's unclear why it is what we should want, but I think it has something to do with human psychology. We actually tend to love profits and people who hold very strong opinions with very little data. Those are the people we like. People will look at a lot of data and actually that, we don't like. Not sure why.\n\nLenny (01:01:57):\nWe're flawed creatures.\n\nGustav Söderström (01:01:59):\nFor sure.\n\nLenny (01:02:00):\nIs there something that you've recently changed your mind about along these same lines that maybe comes to mind of like, \"Oh, yeah?\"\n\nGustav Söderström (01:02:06):\nNo, I think these learnings about the science system and homepage does really well, maybe better than others, that we don't want to wash out with a bath water or whatever the [inaudible 01:02:20] expression is. I think that's the biggest current learning I'm actually very happy about.\n\nLenny (01:02:26):\nYeah, I love learning that we're doing some really well that we didn't really realize necessarily and maybe we should lead into that more.\n\nGustav Söderström (01:02:34):\nExactly.\n\nLenny (01:02:35):\nGoing in a somewhat different direction. Shishir Mehrotra suggested to ask you something. He's on your board, I believe.\n\nGustav Söderström (01:02:41):\nYes.\n\nLenny (01:02:41):\nAnd he suggested to ask you about your 10% planning time. What is that about?\n\nGustav Söderström (01:02:46):\nThis is a concept that I think Shishir has used for a long time ever since he worked at YouTube. And the idea is that, roughly, you shouldn't be spending more than 10% of your time planning versus executing or building, which means that if you're working quarterly 10 weeks, you should spend one week planning as we work in six-month increment. So we try to spend two weeks planning and roughly successful. And this is ... Actually, when we talk about org models, give a shout-out to Brian Chesky at Airbnb, who is actually one of the first, I think, to have these more contrarian old models. He's much more applesque than most of Silicon Valley. He also works in six-month increments. He has a lot of experience in that as well. So that's what the 10% planning time is. And I think if you find yourself planning much more than that, you're either planning too much or your execution period is just too short for that amount of planning. It's a rule of thumb, but I find that it works.\n\nLenny (01:03:53):\nI asked a few PMs what I should ask you, PMs that work at Spotify actually, that I haven't told you. And someone pointed out that you always bring a lot of energy and clarity to a room. That's something they see you as really strong at. What have you learned about just the importance of that or just how to do that well as a leader?\n\nGustav Söderström (01:04:11):\nWell, that's great to hear. I didn't know that so I'm trying to figure out what to answer. I think that the energy, I don't know. I guess I'm just excited about what I do. I've always been excited about technology. I love seeing new things. My core drive is still this notion of you see something which I think you'll empathize with that doesn't exist yet. And you're like, \"Wow, I wonder if that could exist. That would be so cool.\" And then in order to get people to do it, you try to share that excitement. So I don't think I can bring a lot of energy for something I'm not excited about. So I have to work on things I actually believe in and that I'm excited about. And so maybe then the energy comes more naturally. Unfortunately, for me, so far, Spotify has been in this phase where a lot of innovation is allowed and I'm even asked to try to do new cool things.\n\nGustav Söderström (01:05:09):\nMaybe I would have less energy for a pure optimization phase. On the clarity, I've always liked trying to explain things. It's a well-known fact that the best way to understand something is to try to explain to someone else. So I go around explaining things to people who didn't ask for it and not to sound smart, but to see if I actually understood it. And so maybe it's that practice. And on that note, I actually do ask my leaders that work for me and I ask them to ask their leaders to always explain themselves. And I think when ... We talked a little bit about autonomy and so forth, we don't promise everyone that they have to agree, but I think the promise we should make to all employees is that even if they don't agree, they should be entitled to understand why you're making the decision.\n\nGustav Söderström (01:06:06):\nWhat I don't think is acceptable is to say, \"No, we're going to do it this way because I'm more senior. I've seen this a bunch of times. You are not smart enough.\" All of those things. I think you have to explain yourself so you owe an explanation. And I find that valuable back to the only way to understand something is to explain it because it usually turns out that if you can't explain it yourself, you probably don't really even understand it yourself. Sometimes I think it's possible that you can have product instincts that are good but you can't express them. But most of them, when people say there's something there but they can't explain it, they actually don't understand it themselves. And many times, there actually isn't anything there. And also if you can explain it as a product person, that knowledge is now shared. So it just becomes much more effective for the organization. So sometimes I try to provoke people a little bit and say ... When people ask how much is art versus science, I say, \"It's 0% art, 0% magic, and 100% science.\" And that's because I want to force people to try to explain it. I think we use the word art and magic. We have historically used the word art and magic for anything that we couldn't yet explain.\n\nGustav Söderström (01:07:33):\nGenetics was magic and art until it was science. And quantum physics was magic until it was science. And most recently, actually, intelligence and creativity was art and magic until it was statistics in an LLM. So I think I try to push people to say, \"Are you sure you can explain this?\" because that forces people to think through. So maybe I like it and I try to force it on people. So maybe that's why people think I sometimes bring clarity.\n\nLenny (01:08:07):\nI love that. Question along those lines, is there a system or an approach to explaining that you recommend? Is it just write it out in a document? Is it explaining in a certain style or is it just however is natural to the person?\n\nGustav Söderström (01:08:20):\nI used to write everything and then write and rewrite and make it more and more condensed. So that worked for me. I don't write as much anymore. Now, I tend to walk and talk in my head myself. What I actually do is I ... And I found this different for different people and a lot of people want to bounce something with someone else, that's how they think. You repeat the same thing again and again and you get some feedback on it. And so I used to write a lot. I sometimes do when it's an idea I want to understand better. And at some point in my life, I would love to write something real like a book or something. But what I do increasingly now is I do my one-on-ones with peers or people who report to me or something, and I just put on AirPods and do a distributed walk and talk.\n\nGustav Söderström (01:09:12):\nBoth people are walking but in different locations and you spend an hour discussing something. That has actually turned out to be very, very fruitful. So then you get the power of you're not alone so you get more brain power than your own. And I don't think there is strong evolutionary proof for this, but there's certainly indications that you're thinking better when you're walking, whether it's because you're oxygenating your brain or because it's evolutionary for some other reason, I'm not sure. But I found that walking, talking, and thinking actually even if you're not in person, just over AirPods, it's super effective. It was the pandemic that forced us. I thought we would get less creative and strategizing will suffer during the pandemic and I found the opposite. We had more of this than ever and I started thinking about why, and I think it's all of these walk and talks that we did.\n\nLenny (01:10:07):\nYou threw out there that you want to write a book someday. What do you think your book would be about?\n\nGustav Söderström (01:10:11):\nI have no idea. I have no idea. Statistically, it's probably going to be about something that I did a lot, so it has to be about something with technology or product or something. But I would love to write something fictional. That'd be a lot of fun.\n\nLenny (01:10:26):\nOh, boy. I'll pre-order as soon as that's up. Another concept I wanted to touch on that another PM suggested, which is he called it the P in the pants analogy. Does that ring a bell? And is that interesting to talk about?\n\nGustav Söderström (01:10:40):\nI don't know exactly which occasion this person is referring to, but I know I've used that analogy a few times.\n\nLenny (01:10:49):\nOkay. Promising.\n\nGustav Söderström (01:10:50):\nI don't know if it's like a Swedish analogy because I thought it was more widely known. But the idea is that you do something ... So the saying is that's like peeing in your pants in cold weather. It feels really warm and nice to begin with. And then after a while, you start to regret it. It's about being short term, basically. So now I just say that's like peeing in the pants inside because people know what I mean. It's a short-term thing.\n\nLenny (01:11:21):\nThat's a hilarious way of communicating that idea. Must be a Swedish thing.\n\nGustav Söderström (01:11:25):\nYes, I think Swedish people do it for some reason, apparently others don't.\n\nLenny (01:11:30):\nMaybe because it's cold a lot of times of the year.\n\nGustav Söderström (01:11:33):\nYes. That's probably it. This is a saying in cold climate. In the warm, it doesn't help. No one understands what you mean.\n\nLenny (01:11:39):\nSpeaking of Sweden, do you watch Succession?\n\nGustav Söderström (01:11:42):\nYes, I do.\n\nLenny (01:11:43):\nOkay. So Sweden's become a big part of the show, specifically the company trying to ... I guess I don't want to spoil, but there's a character that's really important. Yes, exactly. That is Swedish. And so I'm curious just what do you think of the way they portray the Swedish culture and Swedish business dealings?\n\nGustav Söderström (01:11:59):\nIt's super fun to see this as a Sweden. And I guess, first and foremost, like anyone or any person or any country that gets represented by super tall, well-built, great looking Alexander Skarsgård should probably be pretty happy. So that's good. Then I think there's this episode where they are in Norway, without giving away too much.\n\nLenny (01:12:25):\nYep.\n\nGustav Söderström (01:12:26):\nThere are elements that are authentic. There's a lot of, I think, paid brand positioning from a Swedish brand named Fjällräven, which I think means arctic fox, which is actually a very popular outdoor brand in Sweden. So that's authentic. The sauna things and so forth are authentic. So it's real, but it's exaggerated. Actually, the thing that isn't very authentic is his negotiation style. Swedish people tend to be serious, cautious, and this guy's more of a player. So he's not the typical Swedish businessman from a negotiation tactic point of view, I think.\n\nLenny (01:13:11):\nYeah, it doesn't make me think of the way you described it where in Sweden, people sit in a circle and no one's in the center.\n\nGustav Söderström (01:13:15):\nNo, exactly. He's very much in the center.\n\nLenny (01:13:19):\nAnd then when people go saunas, there are just like a chant, sauna, sauna [inaudible 01:13:23].\n\nGustav Söderström (01:13:23):\nExactly.\n\nLenny (01:13:24):\nThe last episode.\n\nGustav Söderström (01:13:25):\nIt's a great show. I love it.\n\nLenny (01:13:26):\nI love it. This season is insane. I'm so curious where it all goes. Maybe just the last question before very exciting lightning round. Spotify is, at this point, the biggest podcasting platform for me specifically and I think globally, and I love using it. It works great. I'm curious just what's next for Spotify and specifically Spotify podcasting.\n\nGustav Söderström (01:13:48):\nThere are two sides to it. It's for Spotify creators and for Spotify listeners. For Spotify creators, there are two things. One is, and this is what we talked about at Stream On, we talked about it also for music discovery, but it's the same problem and even harder for podcast. So we're still focused very heavily on helping podcast creators find more audience. This is ... Like I said, it's even a bigger problem to break out of your habits and your bubbles in podcasting. Such a big investment to find a new podcast. And so that is something, I think, we could and should do really well. So we keep investing a lot there. And as I said, you'll see more as we roll up more features now.\n\nGustav Söderström (01:14:37):\nThe other big need for creators is monetization and you can monetize today in many ways with DEI and Spotify SEI and so forth. But we're working hard to expand that and make it better because the industry is starting to mature and I think this is one of the biggest needs and the biggest things we could do for creators to help them monetize better, actually both free and paid. We also have paid podcast. So that's on the creator side.\n\nGustav Söderström (01:15:07):\nOn the consumer side, I don't want to share too much. We've shown that we're investing a lot in discovery. I want to keep some secrets for when they roll out, but we are investing a lot in the user expense itself. I think it's far from optimal yet what it could be. One thing that I can share that we're investing a lot in is just the ubiquity and playback across different devices and in cars and all these things that we've done well for music. But I think the listening experience can get a lot more seamless. I think search can get better. The data about podcasts and ... Well, I don't want to say too much, but looking at AI and generative technology, there is a lot that can be done.\n\nLenny (01:15:52):\nAll right. Well, I'll take what I can get. With that, we've reached our very exciting lightning round. I've got six questions for you, Gustav. Are you ready?\n\nGustav Söderström (01:16:01):\nI think I am. Let's do it.\n\nLenny (01:16:03):\nOkay, we'll find out. What are two or three books that you've recommended most to other people?\n\nGustav Söderström (01:16:08):\nOkay. This is why I try to squeeze in seven into two and three. So if we start on product, I think it's well known, but one that I would recommend product people to read is 7 Powers by Hamilton Helmer, and Netflix has used a lot. We use a lot. It's just if you're starting out, it's great to have a strategy framework. No strategy framework is right, but having one is better than none.\n\nGustav Söderström (01:16:31):\nAnother in the space of mental models and frameworks, I think, is The Complete Investor by Charlie Munger. So, yes, it's about investment, but really it's a bunch of mental models that he uses. And I think the key takeaway is you have a problem, you should always apply three different models to it because what models do is they simplify and reduce dimensionality. The world has probably infinite dimensions and they reduces to maybe three or four. And the risk with that is you happen to get rid of a really important dimension, maybe pandemic diseases or something. But if you use three models that have different dimensions and was reduced in different ways, statistically, and it comes to the same conclusion, even the second model you apply vastly increases your chances that you're right. So that was a good book to read.\n\nGustav Söderström (01:17:24):\nThen I think if we go outside of product, I'm very interested in just science and mathematics. So a few quick ones. The Mystery of the Aleph, an amazing book. Something Deeply Hidden by Sean Carroll on the interpretation of quantum mechanics. Helgoland by Carlo Rovelli on the relational interpretation of quantum mechanics. The Beginning of Infinity and The Fabric of Reality by David Deutch. The Case Against Reality by Donald Hoffman on the evolution versus truth and that evolution doesn't optimize for seeing the truth, just for fitness. Gödel's Proof, I think, is an amazing book on his incompleteness theorem, that in any axiomatic systems, there will be true statements that can never be proven, which is a weird thing to think about. And then maybe one of my favorites is The Demon in the Machine by Paul Davies that, I think, is lesser known on how information is really just entropy and this concept of information engines, that you can power something by just information and exhaust is also information. That was not a quick list.\n\nLenny (01:18:39):\nNo, I was just going to say you've set the record for the most number of books, but it also shows how you've become so insightful and wise just reading books like these. And so I think if people are looking to get to a place that you're at now, I think there's the lesson.\n\nGustav Söderström (01:18:55):\nI'll keep the artist much shorter, I promise.\n\nLenny (01:18:57):\nIt's all good. We got time. Okay, next question. What's a favorite recent movie or TV show?\n\nGustav Söderström (01:19:03):\nSo we talked about Succession and it is a recent favorite. So I'll just frivolously take something that isn't recent but is an absolute favorite, which is Halt and Catch Fire, which I think is on FX. Amazing show. If you ever worked in technology, kind of starts out in the Silicon Prairie in the '80s and follows up to present day. Amazing show.\n\nLenny (01:19:24):\nHalt and Catch Fire. Yeah, I watched some of it. I actually fell off of it, but it's a good reminder to go check it out.\n\nGustav Söderström (01:19:29):\nGot to go back.\n\nLenny (01:19:30):\nI'm going to go back. What's a favorite recent interview question you like to ask?\n\nGustav Söderström (01:19:34):\nI don't ask it, but my favorite question is Lex Fridman's small ending question that is usually something like, so what's the meaning of it all? I like that. It's a tough question to get.\n\nLenny (01:19:46):\nI'm so tempted to ask you, but-\n\nGustav Söderström (01:19:48):\nNo, don't.\n\nLenny (01:19:50):\nOkay. Let's move on. That'll be another ... That'll be our second take at this.\n\nGustav Söderström (01:19:53):\nYes.\n\nLenny (01:19:55):\nWhat are some favorite products you've recently discovered that you love?\n\nGustav Söderström (01:19:58):\nThe obvious one is ChatGPT GPT-4 and just playing around with that, trying to create bots for yourself that do different things for you and so forth. But I don't think that's probably true for everyone. The other really favorite is something you've written about and talked about, which is Duolingo, which I think is both very impressive from a product point of view, the execution and what they've done. It is also insanely used in my family. We have a family account and everyone is using it and competing every day. So I'm both impressed by the product and I also use the product quite a lot.\n\nLenny (01:20:35):\nWhat languages are folks learning within your family?\n\nGustav Söderström (01:20:38):\nIn my family, it's Spanish right now.\n\nLenny (01:20:41):\nHow's it going?\n\nGustav Söderström (01:20:41):\nBien.\n\nLenny (01:20:44):\nYou get a gold star.\n\nGustav Söderström (01:20:47):\nI only have a few thousand XP. I'm not that good yet.\n\nLenny (01:20:51):\nNo, I don't know if that's good. That sounds pretty good. Next question, what's something relatively minor you've changed in your product development process that's had a tremendous impact on your team's ability to execute?\n\nGustav Söderström (01:21:01):\nI'm not sure I've done anything minor that had a tremendous impact. Usually, it takes something bigger to get a big impact. I think maybe one thing that I've tried to do back to clarity and so forth is this thing I mentioned about I'm trying to push a lot for what I call Socratic debate, where the idea is obviously that the best idea wins, not the most senior idea and so forth. And trying to push for this notion of having people explain themselves, not saying I think there's something there or have a feeling or something like that. And apparently, as you said, that has had some impact because people apparently say that about me. So that's probably the biggest thing.\n\nLenny (01:21:53):\nFinal question, what is one fun ritual of the Spotify product team, and is it saunas?\n\nGustav Söderström (01:21:59):\nSo Spotify is so big now that it's quite local actually, different parts of Spotify, different product rituals. I accidentally created one ritual many years ago, maybe 12 years ago, when we talked about which phase a product is in. And it was ... We needed some definition. So I think off the cuff, I said, \"Well, it's four phases.\" It's think it, build it, ship it, tweak it.\" And then the think it phase, it should be cheap, not a lot of money spent. In the build it phase, you're going to start spending a lot of money. So then you must have reduced the risk in the think it phase that you're right. And then you have the ship it phase and then you go over and tweak it. And it was something that wasn't that thought through, but it's funny because I still hear it sometimes even from other companies like, \"Oh, we're in the think it phase,\" or \"We're in the tweak it phase.\" So it stuck. I don't know if it's very good, but it's stuck.\n\nLenny (01:22:57):\nIt is catchy. I think anything getting stuck in people's head is a success. Gustav, thank you so much for being here. We are two for two for Swedish people. Gustaf, with an F, Alströmer was on the podcast.\n\nGustav Söderström (01:23:10):\nWho is also an amazing person.\n\nLenny (01:23:12):\nAlso an amazing person. I feel very jealous of people that get to work with you and for you. Thank you again for being here. Two final questions, where can folks find you online if they want to learn more, maybe reach out, ask some questions.\n\nGustav Söderström (01:23:23):\n[inaudible 01:23:23] @GustavS.\n\nLenny (01:23:24):\nOkay. Say it again\n\nGustav Söderström (01:23:28):\n@GustavS.\n\nLenny (01:23:29):\nAwesome. And then final question is just how can listeners be useful to you?\n\nGustav Söderström (01:23:33):\nJust reach out. I do read feedback and I try to remove the angry comments and understand what they're actually thinking and why they're upset or what's not working.\n\nLenny (01:23:44):\nAnd then the reaching out, would you recommend an angry tweet at you or more of a email to that email address you shared?\n\nGustav Söderström (01:23:50):\nWell, the @GustavS is the Twitter handle, so just tweet at me.\n\nLenny (01:23:54):\nOkay.\n\nGustav Söderström (01:23:56):\nYou can be nice as well.\n\nLenny (01:23:57):\nOkay.\n\nGustav Söderström (01:23:57):\nIt's okay.\n\nLenny (01:23:58):\nAmazing. Gustav, thank you so much for being here.\n\nGustav Söderström (01:24:01):\nThank you for having me, Lenny. It's been a pleasure.\n\nLenny (01:24:03):\nBye, everyone.\n\nLenny (01:24:05):\nThank you so much for listening. If you found this valuable, you can subscribe to the show on Apple Podcast, Spotify, or your favorite podcast app. Also, please consider giving us a rating or leaving a review as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at lennyspodcast.com. See you in the next episode."
}
```

Episode 104: Zigging vs. zagging: How HubSpot built a $30B company | Dharmesh Shah (co-founder/CTO)
Guest: Hamel+Shreya

```json
{
  "id": "hamelshreya",
  "guest": "Hamel+Shreya",
  "title": "Zigging vs. zagging: How HubSpot built a $30B company | Dharmesh Shah (co-founder/CTO)",
  "transcript": "# Zigging vs. zagging: How HubSpot built a $30B company | Dharmesh Shah (co-founder/CTO)\n\n## Transcript\n\nLenny Rachitsky (00:00:00):\nTo build great AI products, you need to be really good at building evals. It's the highest ROI activity you can engage in.\n\nHamel Husain (00:00:05):\nThis process is a lot of fun. Everyone that does this immediately gets addicted to it. When you're building an AI application, you just learn a lot.\n\nLenny Rachitsky (00:00:12):\nWhat's cool about this is you don't need to do this many, many times. For most products, you do this process once and then you build on it.\n\nShreya Shankar (00:00:18):\nThe goal is not to do evals perfectly, it's to actionably improve your product.\n\nLenny Rachitsky (00:00:23):\nI did not realize how much controversy and drama there is around evals. There's a lot of people with very strong opinions.\n\nShreya Shankar (00:00:28):\nPeople have been burned by evals in the past. People have done evals badly, so then they didn't trust it anymore, and then they're like, \"Oh, I'm anti evals.\"\n\nLenny Rachitsky (00:00:36):\nWhat are a couple of the most common misconceptions people have with evals?\n\nHamel Husain (00:00:39):\nThe top one is, \"We live in the age of AI. Can't the AI just eval it?\" But it doesn't work.\n\nLenny Rachitsky (00:00:45):\nA term that you used in your posts that I love is this idea of a benevolent dictator.\n\nHamel Husain (00:00:49):\nWhen you're doing this open coding, a lot of teams get bogged down in having a committee do this. For a lot of situations, that's wholly unnecessary. You don't want to make this process so expensive that you can't do it. You can appoint one person whose taste that you trust. It should be the person with domain expertise. Oftentimes, it is the product manager.\n\nLenny Rachitsky (00:01:09):\nToday, my guests are Hamel Husain and Shreya Shankar. One of the most trending topics on this podcast over the past year has been the rise of evals. Both the chief product officers of Anthropic and OpenAI shared that evals are becoming the most important new skill for product builders. And since then, this has been a recurring theme across many of the top AI builders I've had on. Two years ago, I had never heard the term evals. Now it's coming up constantly. When was the last time that a new skill emerged that product builders had to get good at to be successful?\n\n(00:01:41):\nHamel and Shreya have played a major role in shifting evals from being an obscure, mysterious subject to one of the most necessary skills for AI product builders. They teach the definitive online course on evals, which happens to be the number one course on Maven. They've now taught over 2,000 PMs and engineers across 500 companies, including large swaths of the OpenAI and Anthropic teams along with every other major AI lab.\n\n(00:02:07):\nIn this conversation, we do a lot of show versus tell. We walk through the process of developing an effective eval, explain what the heck evals are and what they look like, address many of the major misconceptions with evals, give you the first few steps you can take to start building evals for your product, and also share just a ton of best practices that Hamel and Shreya have developed over the past few years. This episode is the deepest yet most understandable primer you'll find on the world of evals. And honestly, it got me excited to write evals, even though I have nothing to write evals for. I think you'll feel the same way as you watch this.\n\n(00:02:41):\nIf this conversation gets you excited, definitely check out Hamel and Shreya's course on Maven. We'll link to it in the show notes. If you use the code LENNYSLIST when you purchase the course, you'll get 35% off the price of the course. With that, I bring you Hamel Husain and Shreya Shankar.\n\n(00:02:58):\nThis episode is brought to you by Fin, the number one AI agent for customer service. If your customer support tickets are piling up, then you need Fin. Fin is the highest-performing AI agent on the market with a 65% average resolution rate. Fin resolves even the most complex customer queries. No other AI agent performs better. In head-head bake-offs with competitors, Fin wins every time. Yes, switching to a new tool can be scary, but Fin works on any help desk with no migration needed, which means you don't have to overhaul your current system or deal with delays in service for your customers.\n\n(00:03:31):\nAnd Fin is trusted by over 5,000 customer service leaders and top AI companies like Anthropic and Synthesia. And because Fin is powered by the Fin AI engine, which is a continuously improving system that allows you to analyze, train, test, and deploy with ease, Fin can continuously improve your results too. So if you're ready to transform your customer service and scale your support, give Fin a try for only 99 cents per resolution. Plus, Fin comes with a 90-day money-back guarantee. Find out how Fin can work for your team at fin.ai/lenny. That's fin.ai/lenny.\n\n(00:04:05):\nThis episode is brought to you by Dscout. Design teams today are expected to move fast, but also to get it right. That's where Dscout comes in. Dscout is the all-in-one research platform built for modern product and design teams. Whether you're running usability tests, interviews, surveys, or in-the-wild fieldwork, Dscout makes it easy to connect with real users and get real insights fast. You can even test your Figma prototypes directly inside the platform. No juggling tools, no chasing ghost participants. And with the industry's most trusted panel plus AI-powered analysis, your team gets clarity and confidence to build better without slowing down. So if you're ready to streamline your research, speed up decisions, and design with impact, head to dscout.com to learn more. That's dscout.com. The answers you need to move confidently. Hamel and Shreya, thank you so much for being here, and welcome to the podcast.\n\nHamel Husain (00:05:04):\nThank you for having us.\n\nShreya Shankar (00:05:05):\nYeah, super excited.\n\nLenny Rachitsky (00:05:07):\nI'm even more excited. Okay, so a couple years ago, I had never heard the term evals. Now it's one of the most trending topics on my podcast, essentially, that to build great AI products, you need to be really good at building evals. Also, it turns out some of the fastest-growing companies in the world are basically building and selling and creating evals for AI labs. I just had the CEO of Mercor on the podcast. So there's something really big happening here. I want to use this conversation to basically help people understand this space deeply, but let's start with the basics. Just what the heck are evals? For folks that have no idea what we're talking about, give us just a quick understanding of what an eval is, and let's start with Hamel.\n\nHamel Husain (00:05:49):\nSure. Evals is a way to systematically measure and improve an AI application, and it really doesn't have to be scary or unapproachable at all. It really is, at its core, data analytics on your LLM application and a systematic way of looking at that data, and where necessary, creating metrics around things so you can measure what's happening, and then so you can iterate and do experiments and improve.\n\nLenny Rachitsky (00:06:22):\nSo that's a really good broad way of thinking about it. If you go one level deeper just to give people a very, even more concrete way of imagining and visualizing what we're talking about, even if you have a example to show would be even better, what's an even deeper way of understanding what an eval is?\n\nHamel Husain (00:06:36):\nLet's say you have a real estate assistant application and it's not working the way you want. It's not writing emails to customers the way you want, or it's not calling the right tools, or any number of errors. And before evals, you would be left with guessing. You would maybe fix a prompt and hope that you're not breaking anything else with that prompt, and you might rely on vibe checks, which is totally fine.\n\n(00:07:11):\nAnd vibe checks are good and you should do vibe checks initially, but it can become very unmanageable very fast because as your application grows, it's really hard to rely on vibe checks. You just feel lost. And so evals help you create metrics that you can use to measure how your application is doing and kind of give you a way to improve your application with confidence. That you have a feedback signal in which to iterate against.\n\nLenny Rachitsky (00:07:44):\nSo just to make very real, so imagining this real estate agent, maybe they're helping you book a listing or go see an open house. The idea here is you have this agent talking to people, it's answering questions, pointing them to things. As a builder of that agent, how do you know if it's giving them good advice, good answers? Is it telling them things that are completely wrong?\n\n(00:08:04):\nSo the idea of evals, essentially, is to build a set of tests that tell you, how often is this agent doing something wrong that you don't want it to do? And there's a bunch of ways you could define wrong. It could be just making up stuff. It could be just answering in a really strange way. The way I think about evals, and tell me if this is wrong, just simply is like unit tests for code. You're smiling. You're like, \"No, you idiot.\"\n\nShreya Shankar (00:08:29):\nNo, that's not what I was thinking.\n\nLenny Rachitsky (00:08:31):\nOkay. Okay, okay, tell me. Tell me, how does that feel as a metaphor?\n\nShreya Shankar (00:08:35):\nOkay. I like what you said first, which is we had a very broad definition. Evals is a big spectrum of ways to measure application quality. Now, unit tests are one way of doing this. Maybe there are some non-negotiable functionalities that you want your AI assistant to have, and unit tests are going to be able to check that. Now, maybe you also, because these AI assistants are doing such open-ended tasks, you kind of also want to measure how good are they at very vague or ambiguous things like responding to new types of user requests or figuring out if there's new distributions of data like new users are coming and using your real estate agent that you didn't even know would use your product. And then all of a sudden, you think, \"Oh, there's a different way you want to kind of accommodate this new group of people.\"\n\n(00:09:24):\nSo evals could also be a way of looking at your data regularly to find these new cohorts of people. Evals could also be like metrics that you just want to track over time, like you want to track people saying, \"Yes. Thumbs up. I liked your message.\" You want very, very basic things that are not necessarily AI-related but can go back into this flywheel of improving your product. So I would say, overall, unit tests are a very small part of that very big puzzle.\n\nLenny Rachitsky (00:09:56):\nAwesome. You guys actually brought an example of an eval just to show us exactly what the hell we're talking about. We're talking in these big ideas. So how about let's pull one up and show people, \"Here's what an eval is.\"\n\nHamel Husain (00:10:06):\nYeah, let me just set the stage for it a little bit. So to echo what Shreya said, it's really important that we don't think of evals as just tests. There's a common trap that a lot of people fall into because they jump straight to the test like, \"Let me write some tests,\" and usually that's not what you want to do. You should start with some kind of data analysis to ground what you should even test, and that's a little bit different than software engineering where you have a lot more expectations of how the system is going to work. With LLMs, it's a lot more surface area. It's very stochastic, so you kind of have a different flavor here.\n\n(00:10:47):\nAnd so the example I'm going to show you today, it's actually a real estate example. It's a different kind of real estate example. It's from a company called Nurture Boss. I can share my screen to show you their website just to help you understand this use case a little bit, so let me share my screen. So this is a company that I worked with. It's called Nurture Boss, and it is a AI assistant for property managers who are managing apartments, and it helps with various tasks such as inbound leads, customer service, booking appointments, so on and so forth. It's like all the different sort of operations you might be doing as a property manager, it helps you with that. And so you can see kind of what they do. It's a very good example because it has a lot of the complexities of a modern AI application.\n\n(00:11:40):\nSo there's lots of different channels that you can interact through the AI with like chat, text, voice, but also, there's tool calls, lots of tool calls for booking appointments, getting information about availability, so on and so forth. There's also RAG retrieval, getting information about customers and properties and things like that. So it's pretty fully fleshed in terms of an AI application. And so they have been really generous with me in allowing me to use their data as a teaching example. And so we have anonymized it, but what I'm going to walk through today is, okay, let's do the first part of how we would start to build evals for Nurture Boss. Why would we even want to do that?\n\n(00:12:36):\nSo let's go through the very beginning stage, what we call error analysis, which is, let's look at the data of their application and first start with what's going wrong. So I'm going to jump to that next, and I'm going to open an observability tool. And you can use whatever you want here. I just happen to have this data loaded in a tool called Braintrust, but you can load it in anything. We don't have a favorite tool or anything in the blog post that we wrote with you. We had the same example but in Phoenix Arize, and I think Aman, on your blog post, used Phoenix Arize as well. And there's also LangSmith. So these are kind of like different tools that you can use.\n\n(00:13:29):\nSo what you see here on the screen, this is logs from the application, and let me just show you how it looks. So what you see here is, and let me make it full screen, this is one particular interaction that a customer had with the Nurture Boss application, and what it is is a detailed log of everything that happened. So it's called a trace, and it's just the engineering term for logs of a sequence of events. The concept of a trace has been around for a really long time, but it's especially really important when it comes to AI applications.\n\n(00:14:12):\nAnd so we have all the different components and pieces and information that the AI needs to do its job, and we are logged all of it and we're looking at a view of that. And so you see here a system prompt. The system prompt says, \"You are an AI assistant working as a leasing team member at Retreat at Acme Apartments.\" Remember, I said this is anonymized, so that's why the name is Acme Apartments. \"Your primary role is to respond to text messages from both current residents and prospective residents. Your goal is to provide accurate, helpful information,\" yada, yada, yada. And then there's a lot of detail around guidelines of how we want this thing to behave.\n\nLenny Rachitsky (00:14:56):\nIs this their actual system prompt, by the way, for this company?\n\nHamel Husain (00:14:58):\nIt is. Yes, it is.\n\nLenny Rachitsky (00:14:58):\nAmazing. That's so cool.\n\nHamel Husain (00:14:59):\nIt's a real system prompt.\n\nLenny Rachitsky (00:15:01):\nThat's amazing because it's rare you see a actual company product's system prompt. That's like their crown jewels a lot of times, so this is actually very cool on its own.\n\nHamel Husain (00:15:08):\nYeah. Yeah, it's really cool. And you see all of these different sort of features that are different use cases, so things about tour scheduling, handling applications, guidance on how to talk to different personas, so on and so forth. And you can see the user just kind of jumps in here and asks, \"Okay, do you have a one-bedroom with study available? I saw it on virtual tours.\" And then you can see that the LLM calls some tools. It calls this get individual's information tool, and it pulls back that person's information. And then it gets the community's availability. So it's querying a database with the availability for that apartment complex.\n\n(00:16:01):\nAnd then finally, the AI responds, \"Hey, we have several one-bedroom apartments available, but none specifically listed with a study. Here are a few options.\"\n\n(00:16:12):\nAnd then it says, \"Can you let me know when one with a study is available?\"\n\n(00:16:16):\nAnd then it says, \"I currently don't have specific information on the availability of a one-bedroom apartment.\"\n\n(00:16:23):\nUser says, \"Thank you.\"\n\n(00:16:25):\nAnd the AI says, \"You're welcome. If you have any more questions, feel free to reach out.\" Now, this is an example of a trace, and we're looking at one specific data point. And so one thing that's really important to do when you're doing data analysis of your LLM application is to look at data. Now, you might wonder, \"There's a lot of these logs. It's kind of messy. There's a lot of things going on here. How in the hell are you supposed to look at this data? Do you want to just drown in this data? How do you even analyze this data?\"\n\n(00:17:07):\nSo it turns out there is a way to do it that is completely manageable, and it's not something that we invented. It's been around in machine learning and data science for a really long time, and it's called error analysis. And what you do is, the first step in conquering data like this is just to write notes. Okay? So you got to put your product hat on, which is why we're talking to you, because product people have to be in the room and they have to be involved in sort of doing this. Usually a developer is not suited to do this, especially if it's not a coding application.\n\nLenny Rachitsky (00:17:47):\nAnd just to mirror back, why I think you're saying that is because this is the user experience of your product. People talking to this agent is the entire product essentially, and so it makes sense for the product person to be super involved in this.\n\nHamel Husain (00:17:59):\nYeah. So let's reflect on this conversation. Okay, a user asked about availability. The AI said, \"Oh, we don't really have that. Have a nice day.\" Now, for a product that is helping you with lead management, is that good? Do you feel like this is the way we want it to go?\n\nLenny Rachitsky (00:18:30):\nNot ideal.\n\nHamel Husain (00:18:32):\nYes, not ideal, and I'm glad you said that. A lot of people would say, \"Oh, it's great. The AI did the right thing. It looked, it said, 'We didn't have available,' and it's not available.\" But with your product hat on, you know that's not correct. And so what you would do is you would just write a quick note here. You would say, \"Okay.\" You might pop in here, and you can write a note. So every observability application has ability to write notes, and you wouldn't try to figure out if something is wrong. In this case, it's kind of not doing the right thing, but you just write a quick note, \"Should have handed off to a human.\"\n\nLenny Rachitsky (00:19:19):\nAnd as we watch this happening, it's like you mention this and you'll explain more. You're doing this, this feels very manual and unscalable, but as you said, this is just one step of the process and there's a system to this. That was just the first one.\n\nHamel Husain (00:19:30):\nYeah, and you don't have to do it for all of your data. You sample your data and just take a look, and it's surprising how much you learn when you do this. Everyone that does this immediately gets addicted to it and they say, \"This is the greatest thing that you can do when you're building an AI application.\" You just learn a lot and you're like, \"Hmm, this is not how I want it to work. Okay.\" And so that's just an example.\n\n(00:19:58):\nSo you write this note, and then we can go on to the next trace. So this is the next trace. I just pushed a hot key on my keyboard. Let me go back to looking at it.\n\nLenny Rachitsky (00:20:09):\nAnd these tools make it easy to go through a bunch and add these notes quickly.\n\nHamel Husain (00:20:13):\nYes. And so this is another one. Similar system prompt. We don't need to go through all of it again. We'll just jump right into the user question. \"Okay, I've been texting you all day.\" Isn't that funny? And the user says, \"Please.\" Okay, yeah, this one is just like an error in the application where this is a text message application, sorry, the channel through which the customer is communicating is through text message, and you're just getting really garbled. And you can see here that it kind of doesn't make sense. The words are being cut off like, \"In the meantime,\" and then the system doesn't know how to respond, because you know how people text message, they write short phrases. They split their sentence across four or five different turns. So in this case-\n\nLenny Rachitsky (00:21:16):\nYeah, so what do you do with something like that?\n\nHamel Husain (00:21:18):\nYeah, so this is a different kind of error.\n\nLenny Rachitsky (00:21:19):\nMm.\n\nHamel Husain (00:21:19):\nThis is more of, \"Hey, we're not handling this interaction correctly. This is more of a technical problem,\" rather than, \"Hey, the AI is not doing exactly what we want.\" So we would write that down too.\n\nLenny Rachitsky (00:21:20):\nWhich is still really cool.\n\nHamel Husain (00:21:20):\nYeah.\n\nLenny Rachitsky (00:21:31):\nIt's amazing you're catching that, too, here. Otherwise, you'd have no idea this was happening.\n\nHamel Husain (00:21:35):\nYeah, you might not know this is happening, right? And so you would just say, \"Okay.\" You would write a note like, \"Oh, conversation flow is janky because of text message.\"\n\nLenny Rachitsky (00:21:51):\nAnd I like that, I like that you're using the word janky. It shows you just how informal this can be at this stage.\n\nHamel Husain (00:21:56):\nYeah, it's supposed to be chill. Just don't overthink it. And there's a way to do this. So the question always comes up, how do you do this? Do you try to find all the different problems in this trace? What do you write a note about? And the answer is, just write down the first thing that you see that's wrong, the most upstream error. Don't worry about all the errors, just capture the first thing that you see that's wrong, and stop, and move on. And you can get really good at this. The first two or three can be very painful, but you can do a bunch of them really fast.\n\n(00:22:38):\nSo here's another one, and let's skip the system prompt again. And the user asks, \"Hey, I'm looking for a two- to three-bedroom with either one or two baths. Do you provide virtual tours?\"\n\n(00:22:51):\nAnd a bunch of tools are called and it says, \"Hi Sarah. Currently, we have three-bedroom, two-and-a-half-bathroom apartment available for $2,175. Unfortunately, we don't have any two-bedroom options at the moment. We do offer virtual tours. You can schedule a tour,\" blah, blah. It just so happens that there is no virtual tour, right?\n\nLenny Rachitsky (00:23:16):\nMm-hmm. Nice.\n\nHamel Husain (00:23:16):\nSo it is hallucinating something that doesn't exist. Then you kind of have to bring your context as an engineer, or even product content, and say, \"Hey, this is kind of weird. We shouldn't be telling a person about virtual tour when it's not offered.\"\n\n(00:23:32):\nSo you would say, \"Okay, offered virtual tour,\" and you just write the note. So you can see there's a diversity of different kinds of errors that we're seeing, and we're actually learning a lot about your application in a very short amount of time.\n\nShreya Shankar (00:23:55):\nOne common question that we get from people at this stage is, \"Okay, I understand what's going on. Can I ask an LLM to do this process for me?\"\n\nLenny Rachitsky (00:24:04):\nMm, great question.\n\nShreya Shankar (00:24:04):\nAnd I loved Hamel's most recent example because what we usually find when we try to ask an LLM to do this error analysis is it just says the trace looks good because it doesn't have the context needed to understand whether something might be bad product smell or not. For example, the hallucination about scheduling the tour, right? I can guarantee you, I would bet money on this, if I put that into chat GPT and asked, \"Is there an error?\" it would say, \"No, did a great job.\"\n\n(00:24:34):\nBut Hamel had the context of knowing, \"Oh, we don't actually have this virtual tour functionality,\" right? So I think, in these cases, it's so important to make sure you are manually doing this yourself. And we can talk a little bit more about when to use LLMs in the process later, but number one pitfall right here is people are like, \"Let me automate this with an LLM.\"\n\nLenny Rachitsky (00:24:55):\nDo you think we'll get to a place where an agent can do this, where it has that context?\n\nShreya Shankar (00:24:58):\nOh, no. No, no, no. Sorry. There are parts of error analysis that an LLM is suited for, which we could talk about later in this podcast. But right now, in this stage of free form, note-taking is not the place for an LLM.\n\nLenny Rachitsky (00:25:13):\nGot it. And this is something you call open coding, this step?\n\nShreya Shankar (00:25:14):\nYes, absolutely.\n\nLenny Rachitsky (00:25:17):\nCool. Another term that you used in your posts that I love and that fits into this step is this idea of a benevolent dictator. Maybe just talk about what that is, and maybe, Shreya, cover that.\n\nShreya Shankar (00:25:27):\nYeah, so Hamel actually came up with this term.\n\nLenny Rachitsky (00:25:29):\nOkay, maybe Hamel cover that, actually.\n\nHamel Husain (00:25:33):\nNo problem. And we'll actually show the LLM automation in this example, because we're going to take this example, we're going to go all the way through.\n\nLenny Rachitsky (00:25:40):\nAmazing.\n\nHamel Husain (00:25:41):\nAnd so benevolent dictator is just a catchy term for the fact that when you're doing this open coding, a lot of teams get bogged down in having a committee do this. And for a lot of situations, that's wholly unnecessary. People get really uncomfortable with, \"Okay, we want everybody on board. We want everybody involved,\" so on and so forth. You need to cut through the noise. And a lot of organizations, if you look really deeply, especially small, medium-sized companies, you can appoint one person whose tastes that you trust. And you can do this with a small number of people and often one person, and it's really important to make this tractable. You don't want to make this process so expensive that you can't do it. You're going to lose out.\n\n(00:26:36):\nSo that's the idea behind benevolent dictator, is, \"Hey, you need to simplify this across as many dimensions as you can.\" Another thing that we'll talk about later is when it goes to building an LLM as a judge, you need a binary score. You don't want to think about, \"Is this like a 1, 2, 3, 4, 5?\" Like, assign a score to it. You can't. That's going to slow it down.\n\nLenny Rachitsky (00:26:59):\nJust to make sure this benevolent dictator point is really clear, basically, this is the person that-\n\nLenny Rachitsky (00:27:00):\nMake sure this benevolent dictator point is really clear. Basically, this is the person that does this note-taking, and ideally they're the expert on the stuff. So if it's law stuff, maybe there's a legal person that owns this, it could be a product manager. Give us advice on who this person should be?\n\nHamel Husain (00:27:16):\nYeah. It should be the person with domain expertise. So in this case, it would be the person who understands the business of leasing, apartment leasing, and has context to understand if this makes sense. It's always a domain expert, like you said. Okay. For legal, it would be a law person. For mental health, it would be the mental health expert, whether that's a psychiatrist or someone else.\n\nLenny Rachitsky (00:27:41):\nCool.\n\nHamel Husain (00:27:42):\nThough oftentimes, it is the product manager.\n\nLenny Rachitsky (00:27:44):\nCool. So the advice here is pick that person. It may not feel so super fair that they're the one in charge and they're the dictator, but they're benevolent. It's going to be okay.\n\nHamel Husain (00:27:52):\nYeah. It's going to be okay. It's not perfection. You're just trying to make progress and get signal quickly so you have an idea of what to work on because it can become infinitely expensive if you're not careful.\n\nLenny Rachitsky (00:28:07):\nYeah. Okay, cool. Let's go back to your examples.\n\nHamel Husain (00:28:09):\nYeah, no problem. So this is another example where we have someone saying, \"Okay. Do you have any specials?\" And the assistant or the AI responds, \"Hey, we have a 5% military discount.\" User responds, and it switches the subject, \"Can you tell me how many floors there are? Do you have any one-bedrooms available or one-bedrooms on the first floor?\" And the AI responds, \"Yeah, okay. We have several one-bedroom apartments available.\" And then the user wants to confirm, \"Any of those on the first floor and how much are the one-bedrooms?\" And then also, it's a current resident, so they're also asking, \"I need a maintenance request.\"\n\n(00:28:56):\nYou could see the messiness of the real world in here, and the assistant just calls a tool that says transfer call, but it doesn't say anything. It just abruptly does transfer call, so it's pretty jank, I would say. It's just not-\n\nLenny Rachitsky (00:29:13):\nAnother jank.\n\nHamel Husain (00:29:14):\nAnother kind of jank, a different kind of jank. So when you write the open note, you don't want to say jank, because what we want to do is we want to understand, and when we look at the notes later on, we want to understand what happened.\n\n(00:29:24):\nSo you just want to say, \"Did not confirm call transfer with user.\" And it doesn't have to be perfect. You just have to have a general idea of what's going on.\n\nLenny Rachitsky (00:29:39):\nCool.\n\nHamel Husain (00:29:39):\nSo, okay. So let's say we do, and Shreya and I, we recommend doing at least 100 of these. The question is always, \"How many of this do you do?\" And so there's not a magic number. We say 100 just because we know that as soon as you start doing this, once you do 20 of these, you will automatically find it so useful that you will continue doing it.\n\n(00:30:07):\nSo we just say 100 to mentally unblock you, so it's not intimidating. It's like, \"Don't worry, you're only going to do 100.\" And there is a term for that, so the right answer is, \"Keep looking at traces until you feel like you're not learning anything new.\" Maybe Shreya should talk about-\n\nShreya Shankar (00:30:30):\nYeah. So there's actually a term-\n\nHamel Husain (00:30:31):\n... that.\n\nShreya Shankar (00:30:31):\n... in data analysis and qualitative analysis called theoretical saturation. So what this means is when you do all of these processes of looking at your data, when do you stop? It's when you are theoretically saturating or you're not uncovering any new types of notes, new types of concepts, or nothing that will materially change the next part of your process.\n\n(00:30:57):\nAnd this kind of takes a little bit of intuition to develop, so typically, people don't really know when they've reached theoretical saturation yet. That's totally fine. When you do two or three examples or rounds of this, you will develop the intuition. A lot of people realize, \"Oh, okay. I only need to do 40, I only need to do 60. Actually, I only need to do 15.\" I don't know. Depends on the application and depends on how savvy you are with error analysis for sure.\n\nLenny Rachitsky (00:31:25):\nAnd your point about you're going to want to do a bunch. I imagine it's because you're just like, \"Oh, I'm discovering all these problems. I got to see what else is going on here.\"\n\nShreya Shankar (00:31:33):\nExactly.\n\nLenny Rachitsky (00:31:34):\nIs that right?\n\nShreya Shankar (00:31:34):\nAnd promise, at some point, you're not going to discover new types of problems.\n\nLenny Rachitsky (00:31:39):\nYeah. Awesome. So let's say you did 100 of these, what's the next step?\n\nHamel Husain (00:31:42):\nYeah. Okay. So you did 100 of these. Now you have all these notes. So this is where you can start using AI to help you. So the part where you looked at this data is important, like we discussed. You don't want to automate this part too much.\n\nLenny Rachitsky (00:31:59):\nHumans will still have jobs. This is the takeaway here. That's great.\n\nHamel Husain (00:32:02):\nYes.\n\nLenny Rachitsky (00:32:02):\nJust reviewing traces. At least there's one job left for now. Great.\n\nHamel Husain (00:32:06):\nSo, yeah. Exactly. And so, okay. You have all these notes. Now, to turn this into something useful, you can do basic counting. So basic counting is the most powerful analytical technique in data science because it's so simple and it's kind of undervalued in many cases, and so it's very approachable for people.\n\n(00:32:33):\nAnd so the first thing you want to do is take these notes, and you can categorize them with an LLM, and so there's a lot of different ways to do that. Right before this podcast, I took three different coding agents or AI tools in how to categorize these notes. So one is, \"Okay, I uploaded into a cloud project, I uploaded a CSV of these notes, and I just exported them directly from this interface.\" There's a lot of different ways to do this, but I'm showing you the simple, stupid way, the most basic way of doing things.\n\n(00:33:13):\nAnd so I dumped the CSV in here and I said, \"Please analyze the following CSV file.\" And I told it there's a metadata field that has a note in it, but what I said is I used the word open codes, and I said, \"Hey, I have different open codes,\" and that's a term of art. LLMs know what open codes are and they know what axial codes are because it is a concept that's been around for a really long time, so those words help me shortcut what I'm trying to do.\n\nLenny Rachitsky (00:33:46):\nThat's awesome. And the end of the prompt is telling it to create axial codes?\n\nHamel Husain (00:33:50):\nYes. Creating axial codes, so what it does is-\n\nShreya Shankar (00:33:54):\nSo maybe it's worth talking about what are axial codes or what's the point here? You have a mess of open codes, and you don't have 100 distinct problems. Actually, many of them are repeats, but because you phrased them differently, and that you shouldn't have tried to create your taxonomy of failures as you're open coding. You just want to get down what's wrong and then organize, \"Okay, what's the most common failure mode?\"\n\n(00:34:19):\nSo the purpose, axial code basically is just a failure mode. It's the label or category. And what our goal is, is to get to this clusters of failure modes and figure out what is the most prevalent, so then you can go and run and attack that problem.\n\nLenny Rachitsky (00:34:36):\nThat is really helpful. Basically, just synthesizing all these-\n\nShreya Shankar (00:34:36):\nAbsolutely.\n\nLenny Rachitsky (00:34:39):\n... into categories and themes. Super cool. And we'll include this prompt in our show notes for folks so they don't have to sit there and screenshot it and try to type it up themselves.\n\nHamel Husain (00:34:49):\nYeah. Great idea. And so Claude went ahead and analyzed the CSV file and decided how to parse it, blah, blah, blah. We don't need to worry about all that stuff, but it came up with a bunch of axial codes. Basically, axial codes are categories, like Shreya said. So one is, okay, capability limitations, misrepresentation, process and protocol violations, human handoff issues, communication, quality. It created these categories.\n\n(00:35:18):\nNow, do I like all the categories? Not really. I like some of them. It's a good first stab at it. I would probably rename it a little bit because some of them are a bit too generic. Like what is capability limitation? That's a little bit too broad. It's not actionable. I want to get a little bit more actionable with it so that if I do decide it's a problem, I know what to do with it, but we'll discuss that in a little bit. So you can do this with anything, and this is the dumbest way to do it, but dumb sometimes is a good way to get started, so-\n\nLenny Rachitsky (00:35:49):\nAnd this is what LLMS are really good at, taking a bunch of information and synthesizing it.\n\nShreya Shankar (00:35:53):\nAbsolutely. Synthesizing for us to make sense of, right? Note that it's not automatically proposing fixes or anything, that's our job, but now, we can wade through this mess of open codes a lot easier.\n\n(00:36:05):\nAnother thing that's interesting here in this prompt to generate the axial codes is you can be very detailed if you want, right? You can say, \"I want each axial code to actually be some actionable failure mode,\" and maybe the LLM will understand that and propose it, or, \"I want you to group these open codes by what stage of the user story that it's in.\" So this is where you can be creative or do what's best for you as a product manager or engineer working on this, and that will help you do the improvement later.\n\nLenny Rachitsky (00:36:40):\nSo there's no definitive prompt of, \"Here's the one way to do it\"?\n\nShreya Shankar (00:36:42):\nAbsolutely.\n\nLenny Rachitsky (00:36:43):\nYou're saying you can iterate, see what works for you?\n\nShreya Shankar (00:36:46):\nAbsolutely.\n\nLenny Rachitsky (00:36:46):\nIt's interesting the tools don't do this, or do they try and they just don't do a great job?\n\nShreya Shankar (00:36:50):\nNo, I don't think they do it. We've been screaming from the rooftops, \"Please, please-\"\n\nLenny Rachitsky (00:36:54):\nOh, wow.\n\nShreya Shankar (00:36:55):\n\"... do this.\" I do think it's a little bit hard, right? Part of this whole experience with the eval scores Hamel and I are teaching are a lot of people don't actually know this, so maybe it's that people don't know this and they don't know how to build tools for it. And hopefully, we can demystify some of this magic.\n\nLenny Rachitsky (00:37:13):\nAnd just to double-click on this point, this is not a thing everyone does or knows. This is something you two developed based on your experience doing data analysis and data science at other companies?\n\nShreya Shankar (00:37:23):\nWell, I want to caveat that we didn't invent error analysis. We don't actually want to invent things. That's bad signal. If somebody is coming to you with a way to do something that's entirely new and not grounded in hundreds of years of theory and literature, then you should, I don't know, be a little bit wary of that.\n\n(00:37:42):\nBut what we tried to do was distill, \"Okay, what are the new tools and techniques that you need to make sense of the LLM error-out analysis?\" And then we created a curriculum or structured way of doing this. So this is all very tailored to LLMs, but the terms open coding, axial coding, are grounded in social science.\n\nLenny Rachitsky (00:38:04):\nAmazing. Okay. What's funny about you guys doing this is I just want to go do this somewhere. I don't have any AI product to do this on, but it's just like, \"Oh, this would be so fun.\" Just sit there and find all the problems I'm running into and categorize them and then try to fix them.\n\nShreya Shankar (00:38:18):\nI love that.\n\nLenny Rachitsky (00:38:19):\nHamel pulled up a video. What do you got going on here?\n\nHamel Husain (00:38:22):\nYeah. So I pulled up a video just to drive home Shreya's point. We are not inventing anything, so what you see on the screen here is Andrew Ng, one of the famous machine learning researchers in the world who have taught a lot of people, frankly, machine learning. And you can see this is an eight-year-old video, and he's talking about error analysis.\n\n(00:38:45):\nAnd so this is a technique that's been used to analyze stochastic systems for ages, and it's something that it was just using the same machine learning ideas and principles, just bringing them into here, because again, these are stochastic systems.\n\nLenny Rachitsky (00:39:01):\nAwesome. Well, one thing, we're working on getting Andrew on the podcast, we're chatting, so that will-\n\nShreya Shankar (00:39:01):\nNice.\n\nLenny Rachitsky (00:39:05):\n... be really fun. Two, I love that my podcast episode just came out today is in your feed there, and it's standing out really well in that feed, so I'm really happy about that [inaudible 00:39:13].\n\nHamel Husain (00:39:13):\nVery nice. Yeah. The recommendation algorithm is quite good.\n\nLenny Rachitsky (00:39:15):\nYes. Here we go. Hope you click on that. Don't screw my algorithm. Okay, cool. So we've done some synthesis. I know we're not going to go through the entire step. This is you have a whole course that takes many days to learn this whole process. What else do you want to share about how to go about this process?\n\nHamel Husain (00:39:31):\nOkay. So you can do this through anything, and the same thing works just fine in ChatGPT, the same exact prompt. You can see it made axial codes. I really like using Julius AI. It's one of my favorite tools.\n\n(00:39:45):\nJulius is kind of this third-party tool that uses notebooks. I personally like Jupiter notebooks a lot, and so it's more of a data science thing, but a lot of product managers that are kind of learning notebooks nowadays, and it's kind of cool. It's like a fun playground where you can write code and look at data. But we don't have to go deeply into that. Just wanted to mention, you can use a lot. AI is really good at this.\n\n(00:40:10):\nSo let's go to the fun part. Here we go. So now we have these axial codes. So the first thing I like to do, I have these open codes, and I have the axial codes, let's say, that we assigned from the cloud project or the ChatGPT. And so what I do is I collect them first and I take a look, like, \"Does these axial codes make sense?\" And I look at the correspondence between the different axial codes and the open codes, and I go through an exercise and I say, \"Hmm. Do I like these codes? Can I make them better? Can I refine them? Can I make them more specific?\" Instead of being generic, I make them very specific and actionable.\n\n(00:40:59):\nSo you see the ones that I came up with here are tour scheduling, rescheduling issues, human handoff or transfer issue, formatting error with an output, conversational flow. We saw the conversational flow issue with the text messages. Making follow-up promises not kept.\n\n(00:41:18):\nAnd so basically, what I can do, what you can do now is you have these axial codes, and so I just collect them into a list, so this is an Excel formula. Just collect these codes into a list, and now we have a comma-separated list of these codes. And then what you can simply do is you could take your notes that you have, those open codes, and you can tell an AI, and this is using Gemini and AI just for simplicity, this is, again, we're trying to keep it simple, categorize the following note into one of the following categories as always.\n\nLenny Rachitsky (00:41:56):\nFor folks watching, I like all these different prompts and formulas you're sharing. This is the Google Sheets AI prompt.\n\nShreya Shankar (00:42:04):\nHuge fan.\n\nHamel Husain (00:42:07):\nAnd so basically, what you could do is you can categorize your traces into one of the buckets, and that's what we have here. We have categorized all those problems that we encountered into one of these things.\n\nShreya Shankar (00:42:22):\nAnd this is automatic, which is very exciting. I mean, the AI is doing it. So this also drives home the point that your open codes have to be detailed, right? You can't just say janky because if the AI is reading janky, it's not going to be able to categorize it. Even a human wouldn't, right? It would have to go and remember why you said janky, so it's important to be somewhat detailed in your open code.\n\nLenny Rachitsky (00:42:45):\nOkay. So avoid the word janky. It's a good rule of thumb.\n\nShreya Shankar (00:42:48):\nYeah. Or have it with 10 other words.\n\nLenny Rachitsky (00:42:48):\nOh, okay. What is-\n\nHamel Husain (00:42:48):\nYeah. I was being funny.\n\nLenny Rachitsky (00:42:52):\nYeah, okay. What are some of those other words that people often use that you think are not good?\n\nShreya Shankar (00:42:57):\nI don't think it's specific words. I think it's just people are not detailed enough in the open code, so it's hard to do the categorization.\n\nLenny Rachitsky (00:43:04):\nGreat. And by the way, the reason you have to map them back is because, say, Claude or ChatGPT gave you suggestions and you change them and iterated on them, so you can't just go back and say, \"Cool, whatever,\" in each bucket?\n\nHamel Husain (00:43:16):\nYeah, yeah.\n\nLenny Rachitsky (00:43:17):\nGreat.\n\nHamel Husain (00:43:17):\nThat's a really good question, actually. It's good to iterate and think about it a little bit like, \"Do I like these open codes? Do these actually make sense to me?\" Just like anything that AI does, it's really good to kind of put yourself in the middle just a little bit.\n\nLenny Rachitsky (00:43:32):\nIt's in the loop. Still space for us. Great.\n\nShreya Shankar (00:43:34):\nOne of the things that I like to do with this step if I'm trying to use AI to do this labeling, is also have a new category called none of the above. So an AI can actually say, \"None of the above,\" in the axial code, and that informs me, \"Okay, my axial codes are not complete. Let's go look at those open codes, let's figure out what some new categories are or figure out how to reword my other axial codes.\"\n\nLenny Rachitsky (00:44:00):\nAwesome. And what's cool about this is you don't need to do this many, many times.\n\nShreya Shankar (00:44:03):\nNo.\n\nLenny Rachitsky (00:44:04):\nFor most products, you do this process once, and then you build on it, I imagine, and you just tweak it over time?\n\nShreya Shankar (00:44:09):\nAbsolutely. And it gets so fast. People do this once a week, and you can do all of this in 30 minutes, and suddenly your product is so much better than if you were never aware of any of these problems.\n\nLenny Rachitsky (00:44:23):\nYeah. It's absurd to feel like you wouldn't know this is happening. Watching this happening, I'm like, \"How could you not do this to your product?\"\n\nShreya Shankar (00:44:31):\nA lot of people have no idea.\n\nLenny Rachitsky (00:44:31):\nMost people. Yeah. We'll talk about that. There's a whole debate around this stuff that we want to talk about. Okay, cool. So you have the sheet. What comes next?\n\nHamel Husain (00:44:40):\nOkay. So here's sort of the big unveil. This is the magic moment right now. So we have all these codes that we applied, the ones that we like on our traces. Now, you can do the ta-da, you can count them.\n\n(00:44:56):\nSo here's a pivot table, and we just can do pivot table on those, and we can count how many times those different things occurred. So what do we find? Find on these traces that we categorized? We found 17 conversational flow issues. And I really like pivot tables because you can do cool things. You can double-click on these. You can say, \"Oh, okay. Let me take a look at those,\" but that's going into an aside about pivot tables, how cool they are.\n\n(00:45:25):\nBut now, we have just a nice, rough cut of what are our problems? And now, we have gone from chaos to some kind of thinking around, \"Oh, you know what? These are my biggest problems. I need to fix conversational issues, maybe these human handoff issues.\" It's not necessarily the count is the most important thing. It might be something that's just really bad and you want to fix that, but okay. Now, you have some way of looking at your problem, and now you can think about whether you need evals for some of these.\n\n(00:46:07):\nSo there might be some of these things that might be just dumb engineering errors that you don't need to write an eval for because it's very obvious on how to fix them. Maybe the formatting error with output, maybe you just forgot to tell the LLM how you want it to be formatted, and you didn't even say that in the prompt. So just go ahead and fix the prompt maybe, and we can decide, \"Okay, do you want to write an eval for that?\" You might still want to write an eval for that because you might be able to test that with just code. You could just test the string, does it have the right formatting potentially? Without running an LLM.\n\n(00:46:53):\nSo there's a cost-benefit trade-off to evals. You don't want to get carried away with it, but you want to usually ground yourself in your actual errors. You don't want to skip this step. And so the reason I'm kind of spending so much time on this is this is where people get lost. They go straight into evals like, \"Let me just write some tests,\" and that is where things go off the rails.\n\n(00:47:24):\nOkay. So let's say we want to tackle one of these things. So for example, let's say we want to tackle this human handoff issue, and we're like, \"Hmm, I'm not really sure how to fix this. That's a kind of subjective sort of judgment call on should we be handing off to a human? And I don't know immediately how to fix it. It's not super obvious per se. Yeah. I can change my prompt, but I'm not sure. I'm not 100% sure.\"\n\n(00:47:56):\nWell, that might be sort of an interesting thing for an LLM as a judge, for example. So there's different kinds of evals. One is code-based, which you should try to do if you can because they're cheaper. LLM as a judge is something, it's like a meta eval. You have to eval that eval to make sure the LLM that's judging is doing the right thing, which we'll talk about in a second.\n\n(00:48:25):\nSo, okay. LLM as a judge, that's one thing. Okay. How do you build an LLM as a judge?\n\nLenny Rachitsky (00:48:31):\nBefore we get into that actually, just to make sure people know exactly what you're describing there, these two types of evals. One is you said it's code-based and one is LLM as judge. Maybe Shreya, just help us understand what code-based eval even is? It's essentially a unit test? Is that a simple way to think about it?\n\nShreya Shankar (00:48:46):\nYeah. Maybe eval is not the right term here, but think automated evaluator. So when we find these failure modes, one of the things we want is, \"Okay. Can we now go check the prevalence of that failure mode in an automated way without me manually labeling and doing all the coding and the grouping, and I want to run it on thousands and thousands of traces, I want to run it every week.\" That is, okay. You should probably build an automated evaluator to check for that failure mode.\n\n(00:49:12):\nNow, when we're saying code-based versus LLM-based, we're saying, \"Okay. So maybe I could write a Python function or a piece of code to check whether that failure mode is present in a trace or not.\" And that's possible to do for certain things like checking the output is JSON, or checking that it's markdown, or checking that it's short. These are all things you can capture in code or you could approximately capture in code.\n\n(00:49:38):\nWhen we're talking about LLM judge here, we're saying that this is a complex failure mode and we don't know how to evaluate in an automated way. So maybe we will try to use an LLM to evaluate this very, very narrow, specific failure mode of handoffs.\n\nLenny Rachitsky (00:49:56):\nSo just to try to mirror back what you're describing, you want to test what your, say, agent or AI product is doing. You ask it a question, it gets back with something.\n\n(00:50:05):\nOne way to test if it's giving you the right answer is if it's consistently doing the same thing, that you could write a code to tell you this is true or false. For example, will it ever say there's a virtual tour? So you could ask it.\n\nShreya Shankar (00:50:18):\nYes.\n\nLenny Rachitsky (00:50:18):\n\"Do you provide virtual tours?\" It says yes or no, and then you could write code to tell you if it's correct based on that specific answer.\n\n(00:50:27):\nBut if you're asking about something more complicated and it's not binary, in one world, you need a human to tell you this is correct. The solution to avoid humans having to review all this every time automatically is LLMs replacing human judgment, and you'd call it an LLM as judge. The LLM as being the judge if this is correct or not.\n\nShreya Shankar (00:50:47):\nAbsolutely. You nailed it.\n\nLenny Rachitsky (00:50:48):\nGreat.\n\nShreya Shankar (00:50:49):\nSo people always think, \"Oh, this is at least as hard as my problem of creating the original agent.\" And it's not, because you're asking the judge to do one thing, evaluate one failure mode, so the scope of the problem is very small and the output of this LLM judge is pass or fail. So it is a very, very tightly scoped thing that LLM judges are very capable of doing very reliably.\n\nLenny Rachitsky (00:51:18):\nAnd the goal here is just to have a suite of tests that run before you ship to production that tell you things are going the way you want them to? The way your agent is interacting is correct?\n\nShreya Shankar (00:51:28):\nThe beautiful thing about LLM judges, you can use them in unit tests or CI, sure, but you could also use it online for monitoring, right? I can sample 1000 traces every day, run my LLM judge, real production traces, and see what the failure rate is there. This is not a unit test, but still now we get an extremely specific measure of application quality.\n\nLenny Rachitsky (00:51:53):\nCool. That's a really great point because a lot of people just see evals for being this not-real-life thing. It's a thing that you test before it's actually in the real world. And what's actually happening in the real world, you're saying you should actually do exactly that?\n\nShreya Shankar (00:52:04):\nYeah.\n\nLenny Rachitsky (00:52:04):\nTest your real thing running in production? And it's a daily, hourly sort of thing you could be running?\n\nShreya Shankar (00:52:09):\nTotally.\n\nLenny Rachitsky (00:52:10):\nAwesome. Okay. Hamel's got an example of an actual LLM as a judge eval here, so let's take a look.\n\nHamel Husain (00:52:16):\nI love how Shreya really teed it up for me, so thank you so much. So what we have is a LLM as a judge prompt for this one specific failure. Like Shreya said, you would want to do one specific failure and you want to make it binary because we want to simplify things. We don't want, \"Hey, score this on a rating of one to five. How good is it?\" That's just in most cases, that's a weasel way of not making a decision. Like, \"No, you need to make a decision. Is this good enough or not? Yes or no?\"\n\n(00:52:50):\nIt can be painful to think about what that is, but you should absolutely do it. Otherwise, this thing becomes very untractable, and then when you report these metrics, no one knows what 3.2 versus 3.7 means, so.\n\nShreya Shankar (00:53:03):\nYeah. We see this all the time also, and even with expert-curated content on the internet where it's like, \"Oh, here's your LLM judge evaluator prompt. Here's a one-to-seven scale.\"\n\n(00:53:15):\nAnd I always text Hamel like, \"Oh, no. Now, we have to fight the misinformation again because we know somebody is going to try it out and then come back to us and say, 'Oh, I have 4.2 average,'\" and we're going to be like, \"Okay.\"\n\nLenny Rachitsky (00:53:31):\nIt's wild how much drama there is in the evals space. We're going to get to that. Oh, man.\n\n(00:53:37):\nThis episode is brought to you by Mercury. I've been banking with Mercury for years, and honestly, I can't imagine banking any other way at this point. I switched from Chase, and holy moly, what a difference. Sending wires, tracking spend, giving people on my team access to move money around, so freaking easy. Where most traditional banking websites and apps are clunky and hard to use, Mercury is meticulously designed to be an intuitive and simple experience.\n\nLenny Rachitsky (00:54:00):\nMeticulously designed to be an intuitive and simple experience, and Mercury brings all the ways that you use money into a single product, including credit cards, invoicing, bill pay, reimbursements for your teammates and capital. Whether you're a funded tech startup looking for ways to pay contractors and earn yield on your idle cash, or an agency that needs to invoice customers and keep them current, or an e-commerce brand that needs to stay on top of cash flow and access capital, Mercury can be tailored to help your business perform at its highest level. See what over 200,000 entrepreneurs love about Mercury. Visit mercury.com to apply online in 10 minutes. Mercury is a fintech, not a bank. Banking services provided through Mercury's FDIC insured partner banks. For more details, check out the show notes.\n\nHamel Husain (00:54:45):\nOkay, so this is your judge prompt. There's no one way to do it. It's okay to use an LLM to help you create it, but again, put yourself in the loop. Don't just blindly accept what the LLM does, and in all of these cases, that's what we did. With the axial codes, we iterated on this. You can use an LLM to help you create this prompt, but make sure you read it, make sure you edit it, whatever. This is not necessarily the perfect prompt. This is just the stupid, keeping it very simple just to show you the idea. It's like, \"Okay, for this handoff failure,\" I said, \"Okay, I want you to output true or false,\" it's a binary judge. That's what we recommend. Then I just go through and say, \"Okay, when should you be doing a handoff?\" And I just list them out.\n\n(00:55:33):\nOkay, explicit human requests ignored or looped, some policy-mandated transfer, sensitive resident issues, tool data, unavailability, same day walk-in or tour requests. You need to talk to a human for that, so on and so forth. The idea is, now that I know that this is a failure from my data, I'm interested in iterating on it, because I know this is actually happening all the time. Like Shreya said, it would be nice to have a way not only to evaluate this on the data I have, but also on production data, just to get a sense of, what scales is this happening? Let me find more traces, let me have a way to iterate on this. We can take this prompt and I'm going to use the spreadsheet again. The first step is, okay, when I'm doing this judge... I wrote the prompt.\n\n(00:56:28):\nNow, a lot of people stop there and they say, \"Okay, I have my judge prompt. We're done. Good, let's just ship it,\" and the prompt says... If the judge says it's wrong, it's wrong. They just accept it as the gospel, be like, \"Okay, the LLM says it's wrong, it must be wrong. Don't do that, because that's the fastest way that you can have evals that don't match what's going on, and when people lose trust in your evals, they lose trust in you. It's really important that you don't do that, so before you release your LLM as a judge, you want to make sure it's aligned to the human. How do you do that? You have those axial codes and you want to measure your judge against the axial code, and say like, \"Hey, does it agree with me? My own judge, does it agree with me?\" Just measure it.\n\n(00:57:18):\nWhat we have here is, okay, I say, \"Assess this LLM trace.\" Again, I'm using just spreadsheets here, \"Assess this LM trace according to these rules,\" and the rules are just the prompt that I just showed you. I ask it, \"Okay, is there a handoff error, true or false?\" Then this column, let me just zoom in a bit. Column H, I have, \"Okay, did this error occur?\" Column G is whether I thought the error occurred or not. You can see-\n\nLenny Rachitsky (00:57:53):\nYou're going through manually, you do that.\n\nHamel Husain (00:57:55):\nYeah, yeah, which we already did. We already went through it manually. It's not like we have to do it again, because we have that cheat code from the axial coding, we already did it. You might have to go through it again if you need more data, and there's a lot of details to this on how to do this correctly. You want to split your data and do all these things, so that you're not cheating, but I just want to show you the concept. Basically, what you can do is measure the agreement. Now, one thing you should know, as a product manager, is a lot of people go straight to this agreement. They say, \"Okay, my judge agrees with the human some percentage of the time.\"\n\n(00:58:41):\nNow that sounds appealing, but it's a very dangerous metric to use, because a lot of times, errors, they only happen on the long tail and they don't happen as frequently, so if you only have the error 10% of the time, then you can easily have 90% agreement by just having a judge say it passes all the time. Does that make sense? 90% agreement look good on paper, but it might be misleading.\n\nLenny Rachitsky (00:59:15):\nIt's rare, it's a rare error. Yeah.\n\nHamel Husain (00:59:18):\nAs a product manager or someone, even if you're not doing this calculation yourself, if someone ever reports to you agreement, you should immediately ask, \"Okay, tell me more.\" You need to look into it. They give you more intuition, here is like a matrix of this specific judge in the Google sheet, and this is, again, a pivot table, just keeping it dumb and simple. \"Okay, on the rows I have, what did the human think? What did I think? Did it have an error, true or false? Then did my judge have an error, true or false?\"\n\nShreya Shankar (00:59:56):\nThe intuition here is exactly what Hamel said, where you need to look at each type of error. When the human said false, but the judge said true, or vice versa, so those non-green diagonals here, and if they're too large, then go iterate on your prompt, make it more clear to the LLM judge, so that you can reduce that misalignment. You want to get to a point where most... You're going to have some misalignment, that's okay. We talk about in our course, also how to code correct that misalignment, but in this stage, if you're a product manager and the person who's building the LLM judge eval has not done this, they're saying like, \"It agrees 75% of the time, we're good.\" They don't have this matrix and they haven't iterated to make sure that these two types of errors have gone down to zero, then it's a bad smell. Go and ask them to go fix that.\n\nLenny Rachitsky (01:00:52):\nAwesome. That's a really good tip, what to look for when someone's doing this wrong.\n\nShreya Shankar (01:00:56):\nYeah.\n\nLenny Rachitsky (01:00:56):\nActually, can you take us back to the LLM as judge prompt? I just want to highlight something really interesting here. I've had some guests on the podcast recently who've been saying, \"Evals are the new PRDs,\" and if you look at this, this is exactly what this is. Product managers, product teams, here's what the product should be, here's all the requirements, here's the how it should work. They built a thing and then they test it. Manually, often. What's cool about this is this is exactly that same thing, and it's running constantly. It's telling you, \"Here's how this agent should respond,\" and it's very specific ways. \"If it's this, this, this, do that. If it's this, this, that, do that.\" It's exactly what I've been hearing again and again, you could see right here. This is the purest sense of what a product requirements document should be, is this eval judge that's telling you exactly what it should be, and it's automatic and running constantly.\n\nShreya Shankar (01:01:45):\nYeah, absolutely. It's derived from our own data, so of course, it's a product manager's expectations. What I find that a lot of people miss is they just put in what their expectations are before looking at their data, but as we look at our data, we uncover more expectations that we couldn't have dreamed up in the first place, and that ends up going into this prompt.\n\nLenny Rachitsky (01:02:05):\nThat is interesting. Your advice is not skip straight to evals and LLM as judge prompts before you build the product, still write traditional one-pagers PRDs to tell your team what we're doing, why we're doing it, what success looks like. But then at the end, you could probably pull from that and even improve that original PRD if you're evolving the product using this process.\n\nShreya Shankar (01:02:28):\nI would go even further to say you're going to improve... It's going to change. You're never going to know what the failure modes are going to be upfront, and you're always going to uncover new vibes that you think that your product should have. You don't really know what you want until you see it with these LLMs, so you got to be flexible, have to look at your data, have to... PRDs are a great abstraction for thinking about this. It's not the end all, be all. It's going to change.\n\nLenny Rachitsky (01:02:58):\nI love that, and Hamel's pulling up some cool research report. What's this about?\n\nHamel Husain (01:03:04):\nThis is one of the coolest research reports you can possibly read if you want to know about evals. It was authored by someone named Shreya Shankar.\n\nShreya Shankar (01:03:13):\nOh, my God.\n\nHamel Husain (01:03:15):\nAnd her collaborators. It's called \"Who Validates the Validated?\"\n\nLenny Rachitsky (01:03:20):\nThat's the best name for a researcher.\n\nShreya Shankar (01:03:21):\nThank you, thank you.\n\nHamel Husain (01:03:24):\nI should let Shreya talk about this. I think one of the most important things to pay attention in this paper are the criteria drift, and what she found.\n\nShreya Shankar (01:03:35):\nWe did this super fun study when we were doing user studies with people who were trying to write LLM judges or just validate their own LLM outputs. I think this was before evals was extremely popular, I feel like, on the internet. We did this project late 2023 was when we started it. But then the thing that really was burning in my mind as a researcher is like, \"Why is this problem so hard? We've been having machine learning and AI for so long, it's not new, but suddenly, this time around, everything is really difficult.\" We just did this user study with a bunch of developers and we realized, \"Okay, what's new here is that you can't figure out your rubrics upfront. People's opinions of good and bad change as they review more outputs, they think of failure modes only after seeing 10 outputs they would never have dreamed of in the first place,\" and these are experts. These are people who have built many LLM pipelines and now agents before, and you can't ever dream up everything in the first place. I think that's so key in today's world of AI development.\n\nLenny Rachitsky (01:04:50):\nThat is a really good point. That's very much reinforcing what we were just talking about and that's why I'll pull this up, is just... Okay-\n\nShreya Shankar (01:04:56):\nThe research behind it.\n\nLenny Rachitsky (01:04:58):\nYeah, okay, great. You still got to do product the same way, but now you have this really powerful tool that helps you make sure what you've built is correct. It's not going to replace the PRD process. Cool. How many, say, I don't know, LLM as judge prompts, do you end up with usually say... I don't know. I know, obviously, depends complexity to the product, but what's a number in your experience?\n\nShreya Shankar (01:05:19):\nFor me, between four and seven.\n\nLenny Rachitsky (01:05:22):\nThat's it.\n\nShreya Shankar (01:05:23):\nIt's not that many, because a lot of the failure modes, as Hamel said earlier, can be fixed by just fixing your prompt. You just didn't think to put it in your prompts, so now you put it in your... You shouldn't do an eval like this for everything, just the pesky ones that you've described your ideal behavior in your agent prompt, but it's still failing.\n\nLenny Rachitsky (01:05:43):\nGot it. Say you found a problem, you fixed it. In traditional software development, you'd write a unit test to make sure it doesn't happen again. Is your insight here is, \"Don't even bother writing an eval around that if it's just gone\"?\n\nShreya Shankar (01:05:54):\nI think you can if you want to, but the whole game here is about prioritizing. You have finite resources and finite time, you can't write an eval for everything, so prioritize the ones that are the more pesky areas.\n\nLenny Rachitsky (01:06:07):\nProbably the ones that are most risky to your business if they say something like Mecha Hitler, Grok.\n\nShreya Shankar (01:06:07):\nYikes.\n\nLenny Rachitsky (01:06:15):\nCool. Okay, so that's very relieving, because this prompt was a lot of work to really think through all these details.\n\nShreya Shankar (01:06:21):\nBut it's a lot of one-time cost. Right now, forever, you can run this on your application.\n\nHamel Husain (01:06:30):\nOkay, data analysis is super powerful, is going to drive lots of improvements very quickly to your application. We showed the most basic kind of data analysis, which is counting, which is accessible to everyone. You can get more sophisticated with the data analysis. There's lots of different ways to sample, look at data. We made it look easy in a sense, but there's a lot of skills here to do to it well. Building an intuition and a nose for how to sort through this data. For example, let's say I find conversational issues, this conversational flow issues. Maybe if I was trying to chase down this problem further, I would think about ways to find other conversational flow issues that I didn't code. I would maybe dig through the data in several ways, and there's different ways to go about this. It's very similar, if not almost exactly similar as traditional analytics techniques that you would do on any product.\n\nLenny Rachitsky (01:07:41):\nGive us just a quick sense of what comes next and then let's talk about the debate around evals and a couple more things.\n\nShreya Shankar (01:07:48):\nWhat comes next after you've built your LLM judge? Well, we find that people just try to use that everywhere they can, so they'll put the LLM judge in unit tests and they will build, \"Here are some example traces where we saw that failure, because we labeled it. Now we're going to make those part of unit tests and make sure that, every time we push a change to our code, these tests are going to pass.\" They also use it for online monitoring. People are making dashboards on this, and I think that's incredible. I think the products that are doing this, they have a very sharp sense of how well their application is performing, and people don't talk about it, because this is their moat. People are not going to go and share all of these things, because it makes sense. If you are an email-writing assistant, and you're doing this and you're doing it well, you don't want somebody else to go and build an email-writing assistant and then get you out of business.\n\n(01:08:41):\nI really want to stress the point that it's try to use these artifacts that you're building wherever possible online, repeatedly use them to drive improvements to your product. Oftentimes, Hamel and I will tell people how to do this up to this very point, and it clicks for people and then they never come back again. Either they have, I don't know, quit their jobs, they're not doing AI development anymore, or they know what to do from here on out. I think it's the latter, but I think it's very powerful.\n\nLenny Rachitsky (01:09:15):\nJust watching you do this really opened my eyes to what this is and how systematic the process is. I always imagine you just sit on a computer, \"Okay, what are the things I need to make sure work correctly?\" What you're showing us here is it's a very simple step-by-step based on real things that are happening in your product, how to catch them, identify them, prioritize them, and then catch them if they happen again and fix them.\n\nShreya Shankar (01:09:38):\nYeah, it's not magic. Anyone can do this, you're going to have to practice the skill, like any new skill, you have to practice, but you can do it. I think what's very empowering now is that product managers are doing this and can do this, and can really build very, very profitable products with this skill set.\n\nLenny Rachitsky (01:09:57):\nOkay, great segue to a debate that we got pulled into that was happening on X the other day. I did not realize how much controversy and drama there is around evals. There's a lot of people with very strong opinions. How about Shreya? Give us just a sense of the two sides of the debate around the importance and value of evals, and then give us your perspective.\n\nShreya Shankar (01:10:19):\nYeah. All right, I'll be a little bit placating and I say I think everyone is on the same side. I think the misconception is that people have very rigid definitions of what evals is. For example, they might think that evals is just unit tests or they might think that evals is just the data analysis part and no online monitoring or no monitoring of product-specific metrics, like actually number of chats engaged in or whatnot. I think everyone has a different mindset of evals going in, and the other thing I will say is that people have been burned by evals in the past. I think people have done evals badly. One concrete example of this is they've tried to do an LLM judge, but it has not aligned with their expectations. They only uncovered this later on and then they didn't trust it anymore, and then they're like, \"I'm anti evals.\"\n\n(01:11:14):\nI 100% empathize with that, because you should be anti Likert scale LLM judge. I absolutely agree with you, we are anti that as well. A lot of the misconception stems from two things, like people having a narrow definition of evals and then people not doing it well and then getting burned and then wanting to avoid other people making that mistake. Then, unfortunately, X or Twitter is a medium where people are misinterpreting what everybody is saying all the time, and you just get all these strong opinions of, \"Don't do evals, it's bad. We tried it, it doesn't work. We're Claude Code,\" or whatever other famous product, \"And we don't do evals.\" There's just so much nuance behind all of it, because a lot of these applications are standing on the shoulders of evals. Coding agents is a great example of that, Claude Code. They're standing on the shoulders of Claude base model... Not base, but the fine-tuned Claude models have been evaluated on many coding benchmarks. Can't argue against that.\n\nLenny Rachitsky (01:12:24):\nJust to make clear exactly what you're talking about there, one of the heads, I think maybe the head engineer of Claude Code, went on a podcast and he's like, \"We don't do evals, we just vibe. We just look at vibes,\" and vibes meaning they just use it and feel if it's right or wrong.\n\nShreya Shankar (01:12:37):\nI think that works. There's two things to that, right? One is they're standing on the shoulders of the evals that their colleagues are doing for coding.\n\nLenny Rachitsky (01:12:45):\nOf the Claude foundational model.\n\nShreya Shankar (01:12:47):\nAbsolutely, right? We know that they report those numbers, because we see the benchmarks, we know who's doing well on those. The other thing is they are actually probably very systematic about the error analysis to some extent. I bet you that they're monitoring who is using Claude, how many people are using Claude, how many traps are being created, how long these chats are. They're also probably monitoring in their internal team, they're dogfooding. Anytime something is off, they maybe have a cue or they send it to the person developing Claude Code, and this person is implicitly doing some form of hair error analysis that Hamel talked about. All of this is evals, right? There's no world in which they're just being like, \"I made Claude Code, I'm never looking at anything,\" and unfortunately, when you don't think about that or talk about that, I think that the community...\n\n(01:13:39):\nMost of the community is beginners or people who don't know about evals and want to learn about it, and it sends the wrong message there. Now, I don't know what Claude Code is doing, obviously, but I would be willing to bet money that they're doing something in the form of evals.\n\nHamel Husain (01:13:53):\nWe'll also say that coding agents are fundamentally very different than other AI products, because the developer is the domain expert, so you can short circuit a lot of things, and also, the developer is using it all day long, so there's a type of dogfooding and type of domain expertise that is... You can collapse the activities, you don't need as much data, you don't need as much feedback or exploration, because you know, so your eval process should look different.\n\nLenny Rachitsky (01:14:31):\nBecause you're seeing the code, you see the code it's generating. You can tell, \"This is great, this is terrible.\"\n\nHamel Husain (01:14:35):\nYeah, yeah. I think a lot of people had generalized coding agents, because coding agents are the first AI product released into the wild, and I think it's a mistake to try to generalize that at large.\n\nShreya Shankar (01:14:51):\nThe other thing is, yeah, engineers have a dogfooding personality. There are plenty of applications where people are trying to build AI in certain domains and they don't have dogfooding for doctors, for example, or not out there trying to get all the most incorrect advice from AI and be tolerant and receptive to that. It's very important to keep, I think these nuanced things in mind.\n\nLenny Rachitsky (01:15:16):\nWhat I'm hearing from you, Shreya, interestingly, is that if humans on the team are doing very close data analysis, error analysis, dogfooding like crazy, and essentially, they're the human evals and you're describing that as that's within the umbrella of evals. You could do it that way if you have time and motivation to do that, or you could set these things up to be automatic.\n\nShreya Shankar (01:15:40):\nAbsolutely, it's also about the skills. People who work at Anthropic are very, very highly skilled. They've been trained in data analysis or software engineering or AI, and whatnot. You can get there, anyone can get there, of course, by learning the concepts, but most people don't have that skill right now.\n\nHamel Husain (01:16:02):\nDogfooding is a dangerous one, only because a lot of people will say they're dogfooding. They're like, \"Yeah, we dogfooded,\" but are they, really? A lot of people aren't really dogfooding it at that visceral level that you would need to close that feedback loop. That's the only caveat I would add.\n\nLenny Rachitsky (01:16:24):\nThere's also this, feels like, straw man argument of evals versus A-B tests. Talk about your thoughts there, because that feels like a big part of this debate. People are having like, \"Do you need evals if you have A-B tests that are testing production level metrics?\"\n\nShreya Shankar (01:16:38):\nA-B tests are, again, another form of evals ,I imagine, right? When you're doing an A-B test, you have two different experimental conditions and then you have a metric that quantifies the success of something, and you're comparing the metric. Again, an eval in our mind is systematic measurement of quality, some metric. You can't really do an A-B test without the eval to compare, so maybe we just have a different weird take on it.\n\nLenny Rachitsky (01:17:06):\nYeah, okay. What I'm hearing is you consider A-B tests as part of the suite of evals that you do. I think when people think A-B tests, it's like we're changing something in the product, we're going to see if this improves some metric we care about. Is that enough? Why do we need to test every little feature? If it's impacting a metric we care about as a business, we have a bunch of A-B tests that are just constantly running.\n\nShreya Shankar (01:17:27):\nThis is now a great point. I think a lot of people prematurely do A-B tests, because they've never done any error analysis in the first place. They just have hypothetically come up with their product requirements and they believe that, \"We should test these things,\" but it turns out, when you get into the data, as Hamel showed, that the errors that you're seeing are not what you thought what the errors might be. They were these weird handoff issues or, I don't know, the text message thing was strange. I would say that, if you're going to do A-B tests and they're powered by actual error analysis as we've shown today, then that's great, go do it. But if you're just going to do them, which we find that people try to do, just want to do them based on what you hypothetically think is what is important, then I would encourage people to go and rethink that and ground your hypotheses.\n\nLenny Rachitsky (01:18:23):\nDo you have thoughts on what Statsig is going to do at OpenAI? Is there anything there that's interesting? That was a big deal, a huge acquisition. A- B test company people are like, \"A-B test, the future.\" Thoughts?\n\nHamel Husain (01:18:34):\nJust to add to the previous question a little bit, why is there this debate, A-B testing versus evals? I think, fundamentally, evals is... People are trying to wrap their head around how to improve their applications and fundamentally need to do... Data science is useful in products. Looking at data, doing data analytics. There's many different suite of tools, and you don't need to invent anything new. Sure, you don't need necessarily the whole breadth of data science, and it looks slightly different, just slightly, with LLMs. Your tactics might be different, so really what it is is using analytic tools to understand your product. Now, people say the word \"Evals,\" trying to carve out this new thing, and saying evals and then A-B testing, but if you zoom out, it's the same data science as before, and I think that's what's causing the confusion is, \"Hey, we need data science thinking,\" and AI product is helpful to have that thinking in AI products like it is in any product is my take on that.\n\nLenny Rachitsky (01:19:50):\nThat's a really good take, I think just the word \"Evals\" triggers people now.\n\nShreya Shankar (01:19:53):\nYeah.\n\nLenny Rachitsky (01:19:53):\nIf you just call it, \"We're just doing error analysis, doing data science to understand where our product breaks and just setting up tests to make sure we know-\"\n\nShreya Shankar (01:20:00):\nThat's boring, sounds boring. No, no, no. We need a mysterious term, like \"Evals,\" to really get the momentum going. Your question about Statsig, I think it's very exciting. To be honest, I don't know much about it, because I just imagine that they're this company that... There's a tool that many people use, and maybe it just so happened that OpenAI acquired them. I'm sure they've been using them in the past, I'm sure OpenAI's competitors are using Statsig as well, so maybe there is something strategic in that acquisition. I have no idea, I don't know anything there, but I think those are really the bigger questions for me than, \"Is this fundamentally changing A-B testing or making evals more of a priority?\" I think they've always been a priority, I think OpenAI has always been doing some form of them, and OpenAI has gone so far, historically speaking, as to go and look at all the Twitter sentiment and try to do some retrospective on that, and then tie that back to their products. Certainly, they're doing-\n\nShreya Shankar (01:21:00):\nThen, tie that back to their products. Certainly, they're doing some amount of evals before they ship their new foundation models, but they're going so much beyond and being like, \"Okay, let's find all the tweets that are complaining about it, all the Reddit threads that are complaining about it, and go try to figure out what's going on.\" It goes to show that evals are very, very important. No one has really figured it out yet. People are using all the available sources signal that they can to improve their products.\n\nHamel Husain (01:21:26):\nWhat I'll say is I'm really hopeful that it might shift or create a focus within OpenAI, hopefully. Up until now, a lot of the big labs understandably focused on general benchmarks like MMLU score, human eval, things like that, which are very important for foundation models. Those not very related to product specific evals, like the ones we talked about today, but handoff and stuff like that, they tend not to correlate.\n\nShreya Shankar (01:22:01):\nYeah, they don't correlate with math problem-solving, sorry to say.\n\nHamel Husain (01:22:06):\nExactly. If you look at the eval products, let's say the ones up until recently that some of the big labs have, they don't have error analysis. They have a suite of generic tools, cosine similarity, hallucination score, whatever, and that doesn't work. It's a good first stab at it. It's okay. At least you're doing something, getting people, maybe it's like getting people look at data. But eventually, what we hope to see is, okay, a bit more data science thinking in this eval process. That's hopefully the tools we'll get to.\n\nShreya Shankar (01:22:44):\nYeah, Pamela and I should not be the only two people on the planet that are promoting a structured way of thinking about application specific evals. It's mind-boggling to me. Why are we the only two people doing this the whole world? What's wrong? I hope that we're not the only people and that more people catch on.\n\nLenny Rachitsky (01:23:04):\nThe fact that your course on Maven is the number one highest grossing course in Maven, clearly there's demand and interest, and there's more people I think on your side. Interestingly, just as an example you've been sharing on Twitter that I think is informative, everyone's been saying how cloud code doesn't care about evals. They're all about vibes, and everyone's like, and they're the best coding agent out there, so clearly, this is right. More recently, there's all this talk about Codex, OpenAI Codex being better and everyone's switching and they're so pro evals.\n\nShreya Shankar (01:23:33):\nI know.\n\nLenny Rachitsky (01:23:34):\nYeah.\n\nShreya Shankar (01:23:38):\nIt gets me every time. The Internet's so inconsistent. My favorite thing was yesterday, I believe, a couple of lab mates and I were out getting dessert or something, and somebody said like, \"Oh, do you like Codex or Claude better or whatever?\" The other person said, \"Oh, I like Claude.\" Then, someone else said, \"But the new version of Codex is better.\" Then, the first person said, \"Oh, but the last I checked was two days ago, so maybe my thoughts, maybe I'm not up-to-date.\" I was like, \"Oh, my God.\"\n\nLenny Rachitsky (01:24:14):\nSo true, so true. This is the world we live in. Oh, my God. Okay. I want to ask about just top misconceptions people have with evals and top tips and tricks for being successful. Maybe just share one or two each of each. Let me just start with misconceptions, and maybe I'll go to the Hamel first. Just what are a couple of the most common misconceptions people have with eval still?\n\nHamel Husain (01:24:31):\nThe top one is, \"Hey, I can just buy a tool, plug it in, and it'll do the eval for you. Why do I have to worry about this? We live in the age of AI. Can't the AI just eval it?\" That's the most common misconception, and people want that so much that people do sell it, but it doesn't work. That's the first one.\n\nLenny Rachitsky (01:24:55):\nShoot, many humans are still great. I think that's great news.\n\nHamel Husain (01:25:00):\nThe second one that I see a lot is, \"Hey, just not looking at the data.\" In my consulting, people come to me with problems all the time, and the first thing I'll say is, \"Let's go look at your traces.\" You can see their eyes pop open and be like, \"What do you mean?\" I'm like, \"Yeah, let's look at it right now.\" They're surprised that I am going to go look at individual traces, and it always 100% of the time learn a lot and figure out what the problem is. I think people just don't know how powerful looking at the data is like we showed on this podcast.\n\nShreya Shankar (01:25:48):\nI would agree with that.\n\nLenny Rachitsky (01:25:50):\nThose are the top two? Okay.\n\nShreya Shankar (01:25:51):\nYes.\n\nLenny Rachitsky (01:25:51):\nIs there anything else or those are the ones solve those problems.\n\nShreya Shankar (01:25:55):\nOh, those are definitely... Then, I guess the third one I would add is, there's no one correct way to do evals. There are many incorrect ways of doing evals, but there are also many correct ways of doing it. You got to think about where you are at with your product, how much resources you have, and figure out the plan that works best for you. It'll always involve some form of error analysis as we showed today, but how you operationalize those metrics is going to change based on where you're at.\n\nLenny Rachitsky (01:26:28):\nAmazing. Okay. What are a couple of just tips and tricks you want to leave people with as they start on their eval journey or just try to get better at something they're already doing?\n\nShreya Shankar (01:26:37):\nTip number one is just don't be alarmed or don't be scared of looking at your data. The process, we try to make it as structured as possible. There are inevitably questions that are going to come up. That's totally fine. You might feel like you're not doing it perfectly. That's also fine. The goal is not to do evals perfectly, it's to actionably improve your product. We guarantee you, no matter what you do, if you're doing parts of these process, you're going to find ways of actionable improvement, and then you're going to iterate on your own process from there.\n\n(01:27:14):\nThe other tip that I would say is, we are very pro-AI. Use LLMs to help you organize any thoughts that you have throughout this entire process. This could be everything ranging from initial product requirements. Figure out how to organize them for yourself. Figure out how to improve on that product requirements doc based on the open codes that you've created. Don't be afraid to use AI in ways that present information better for you.\n\nLenny Rachitsky (01:27:44):\nSweet, so don't be scared. Use LLMs as much as you can throughout the process.\n\nShreya Shankar (01:27:48):\nBut not to replace yourself.\n\nLenny Rachitsky (01:27:51):\nRight. Okay, great. There's still jobs. It's great. Hamel.\n\nHamel Husain (01:27:55):\nYeah. Let me actually share my screen, because I want to show something. To piggyback of what Shreya said is, if you heard any phrase in this podcast, you've probably heard look at your data more than anything else. It's so important that we teach that you should create your own tools to make it as easy as possible. I showed you some tools when we're going through the live example of how to annotate data. Most of the people I work with, they realize how important this is and they vibe code their own tools, or we shouldn't say vibe code. They make their own tools, and it's cheaper than ever before because you have AI that can help you.\n\n(01:28:40):\nAI is really good at creating simple web applications that can show you data, that can write to a database. It's very simple. For the Nurture Boss use case, we wanted to remove all the friction of looking at data. What you see here is just some screenshots of what the application that they created looks like. It's just, \"Okay, they have the different channels, voice, email, text. They have the different threads, they hid the system prompt by default.\" Little quality of life improvements. Then, they actually have this axial coding part here where you can see in red the count of different errors. They automated that part in a nice way and they created this within a few hours. It's really hard to have a one size fits all thing for looking at your data. You don't have to go here immediately, but something to think about is make it as easy as possible because, again, it's the most powerful activity that you can engage in. It's the highest ROI activity you can engage in. With AI, yeah, just remove all the friction.\n\nLenny Rachitsky (01:29:56):\nThat's amazing. Again, I think that ROI piece is so important. We haven't even touched on this enough. The goal here is to make your product better, which will make your business more successful. This isn't just a little exercise to catch bugs and things like that. This is the way to make AI products better because the experience is how users interact with your AI.\n\nHamel Husain (01:30:16):\nAbsolutely. If any, we teach our students, \"Hey, when you're doing these evals, if you see something that's wrong, just go fix it.\" The whole point is not to have evals, a beautiful eval suite, where you can point at it, edit it and say, Oh, look at my evals.\" No, just fix your application, make it better. If it's obvious, do it. Totally agree with you.\n\nLenny Rachitsky (01:30:38):\nAmazing. A question I didn't ask, but this is I think something people are thinking about. How long do you spend on this? How long does it usually take to do? The first time\n\nShreya Shankar (01:30:45):\nI can answer for myself for applications that I work with. Usually, I'll spend three to four days really working with whoever to do initial rounds of error analysis. A lot of labeling, feel like we're in a good place to create the spreadsheet that Hamel had and everyone's on-board and convinced, and even a few LLM judge evaluators. But this is one-time cost. Once I figured out how to integrate that in unit tests, or I have a script that automatically runs it on samples and I'll create a Cron Job to just do this every week. I would say it's like, I don't know, I find myself probably spending more time looking at data because I'm just data hungry like that. I'm so curious.\n\n(01:31:23):\nI'm like, I've gained so much from this process and it's put me above and beyond in any of my collaborations with folks, so I want to keep doing it, but I don't have to. I would say maybe 30 minutes a week after that.\n\nLenny Rachitsky (01:31:41):\nIt's a week essentially, a week essentially upfront, and then 30 minutes to keep improving on adding to your suite?\n\nShreya Shankar (01:31:47):\nYeah, it's really not that much time. I think people just get overwhelmed by how much time they spend up front and then thinking that they have to keep doing this all the time.\n\nLenny Rachitsky (01:31:56):\nAmazing. Is there anything else that you wanted to share or leave listeners with? Anything else you wanted to double down as a point before we get to a very exciting lightning round?\n\nHamel Husain (01:32:06):\nI would say this process is a lot of fun, actually. It's like, okay, you're looking at data. Oh, it sounds like you're annotating things. Okay. Actually, I was just looking at a client's data yesterday, the same exact process. It's a application that sends emails, recruiting emails to try to get candidates to apply for a job. We decided to start looking at traces. We jumped right into it. \"Hey, let's look at your traces.\" We looked at a trace, the first thing I saw was this email that is worded, \"Given your background, blah, blah, blah, blah, blah.\" I asked the person right away, and this is where putting your product hat on and just being critical, and this is where the fun part is.\n\n(01:32:55):\nI said, \"You know what? I hate this email. Do you like the email, given your background?\" When I receive a message given your background, comma, I just delete that. I'm like, \"What is this, given your background with machine learning and blah blah?\" I'm like, \"This is a generic thing.\" I asked the person like, \"Hey, can we do better than this? This sounds like generic recruiting.\" They're like, \"Oh, yeah, maybe.\" Because they were proud of it, they're like, \"The AI is doing the right thing, it's sending this email with the right information, with the right link, with the right name, everything.\" That's where the fun part is, is put your product hat on and get into, is this really good?\n\nLenny Rachitsky (01:33:38):\nSomething I want to make sure we cover before we get to a very exciting lightning round is, this is just scratching the surface of all the things you need to know to do this well. I think this is the best primer I've ever seen on how to do this well.\n\nShreya Shankar (01:33:51):\nNice.\n\nLenny Rachitsky (01:33:51):\nBut I think we did it. But you guys teach a course that goes much, much deeper for people that really want to get good at this and take this really seriously. Share what else you teach in the course that we didn't cover, and what else you get as a student being part of the course you teach at Maven.\n\nShreya Shankar (01:34:07):\nYeah, I can talk about the syllabus a little bit, and then Hamel can talk about all the perks. We go through a lifecycle of error analysis, then automated evaluators, then how to improve your application, how do you create that flywheel for yourself? We also have a few special topics that we find pretty much no one has ever heard of or taught before, which is exciting. One is, how do you build your own interfaces for error analysis? We go through actual interfaces that we've built and we also live code them on the spot for new data. We show how we use Claude code cursor, whatever we're feeling in the moment that day to build these interfaces.\n\n(01:34:49):\nWe also talk about broadly cost-optimization as well. A couple of people that I've worked with, they get to a point where their evals are very good, their product is very good, but it's all very expensive because they're using state-of-the-art models. How can we replace certain uses of the most expensive GPT-5, with 5-nano, 4-mini whatnot and save a lot of money, but still maintain the same quality? We also give some tips for that. Hamel, you're on. We also have many perks.\n\nLenny Rachitsky (01:35:23):\nYeah. Talk about the perks.\n\nHamel Husain (01:35:24):\nOkay, the perks. My favorite perk is there's 160 page book that's meticulously written, that we've created, that walks through the entire process in detail of how to do evals that supplement the course. You don't have to sit there and take all these notes. We've done all the hard work for you and we have documented it in detail and organize things. That is really useful. Another really interesting thing, and something that I got the idea from you, Lenny, is, okay, this is an AI course. Education shouldn't be this thing where you are only watching lectures and doing homework assignments. Students should have access to an AI that also helps them. What we have done is we've, just like there's the LennyBot that you have.\n\nLenny Rachitsky (01:36:19):\nDot com.\n\nHamel Husain (01:36:20):\nYeah, lennybot.com, we have made the same thing with the same software that you're using, and we have put everything we've ever said about evals into that. Every single lesson, every office hours, every Discord chat, any blogs, papers, anything that we've ever said publicly and within our course, we've put it in there. We've tested it with a bunch of students and they've said it's helpful. We're giving all students 10 months free unlimited access to that alongside the course.\n\nLenny Rachitsky (01:36:56):\nAmazing. Then, you'll charge for that later down the road?\n\nHamel Husain (01:37:01):\nI have no idea. I just take one month at a time. I don't know where we're going with that.\n\nLenny Rachitsky (01:37:04):\nEight months and then we'll have to figure it out. I was thinking this whole interview should have just been our bots talking to each other.\n\nShreya Shankar (01:37:09):\nThat's amazing. I would watch that, only for 10 minutes then I don't know what they're talking about.\n\nLenny Rachitsky (01:37:14):\nYeah, maybe 30 seconds. Do you guys train it on the voice mode, by the way? That's my favorite feature of Delphi's product. If not, you should do that.\n\nHamel Husain (01:37:22):\nOh, I think, I can't remember, I should look at it.\n\nLenny Rachitsky (01:37:26):\nYou definitely should. Now that we have this podcast episode, you could use this content to train it. It's 11Labs powered. It's so good. Okay, so how do they get to... I guess that's okay. They get to that once they become, enter your course.\n\nShreya Shankar (01:37:38):\nYeah, sign up for the course and then you'll get a bunch of emails. Everything will be clear, hopefully.\n\nLenny Rachitsky (01:37:43):\nAmazing. Okay.\n\nShreya Shankar (01:37:44):\nWe also have a Discord of all the students who have ever taken the class. That Discord is so active. I can't go on vacation without getting notified on the plane.\n\nLenny Rachitsky (01:37:55):\nBittersweet, bittersweet. Incredible. Okay. With that, we've reached our very exciting lightning round. I've got five questions for you. Are you ready?\n\nShreya Shankar (01:38:04):\nYes. Let's go.\n\nLenny Rachitsky (01:38:05):\nLet's do it. Okay. I'm going to bounce between you two. Share something if you want. You can pass if you want. First question, Shreya, what are two or three books that you find yourself recommending most to other people?\n\nShreya Shankar (01:38:17):\nI like to recommend a fiction book because life is about more than evals. Recently, I read Pachinko by Min Jin Lee. A really great book. Then, I also am currently reading Apple in China, which the name of the author is slipping my mind, but this is more of an exposition, written by a journalist on how Apple did a lot of manufacturing processes in Asia over the last couple, several decades. Very eye-opening.\n\nLenny Rachitsky (01:38:49):\nAmazing. Hamel.\n\nHamel Husain (01:38:52):\nYeah, I have them right here. I'm a nerd. Okay, so I'm not as cool as Shreya is. I actually have textbooks, which are my favorite. This one is a very classic one, Machine Learning by Mitchell. Now, it's theoretical, but the thing I like about it is it really drives home the fact that Occam's razor is prevalent not only in science, but also in machine learning and AI. A lot of times the simplest, and also engineering, so a lot of times the simpler approach generalizes better. That's the thing I internalize deeply from that book. I also really like this one. Another textbook. I told you I'm a nerd. This is also a very old one, and this is Norvig algorithms. I really like it because it's just human ingenuity and it's lots of clever useful things in computing.\n\nShreya Shankar (01:39:49):\nThey're down the street, him and Berkeley.\n\nLenny Rachitsky (01:39:54):\nThe people that did that research?\n\nShreya Shankar (01:39:57):\nYeah, textbook authors.\n\nLenny Rachitsky (01:39:58):\nSuper cool. Oh, man, nerds, I love it. Okay, next question. Favorite recent movie or TV show? I'll jump to Hamel first.\n\nHamel Husain (01:40:06):\nOkay, so I'm a dad of two parents. I have two parents. Sorry, two kids. Yeah, I'm a dad of two kids, and I don't really get the time to watch any TV or movies, so I watch whatever my kids are watching. I've watched Frozen three times in the last week.\n\nLenny Rachitsky (01:40:25):\nOnly three? Oh, okay. In the last week. Okay.\n\nHamel Husain (01:40:30):\nThat's my life.\n\nLenny Rachitsky (01:40:30):\nGreat, Hamel. Frozen. I love it. Okay, Shreya.\n\nShreya Shankar (01:40:32):\nYeah, I don't have kids, so I can give all these amazing answers. Actually, so my husband and I have been watching The Wire recently. We never actually saw it growing up, so we started watching it and it's great.\n\nLenny Rachitsky (01:40:46):\nI feel like everyone goes through that. Eventually in their life they decide, I will watch The Wire.\n\nShreya Shankar (01:40:51):\nI know, so we are in that right now.\n\nLenny Rachitsky (01:40:51):\nIt's like a year of your life. It's great. It's such a great show. Oh, man. But it's so many episodes and everyone's an hour long.\n\nShreya Shankar (01:40:58):\nI know. I know.\n\nLenny Rachitsky (01:40:58):\nIt's such a commitment.\n\nShreya Shankar (01:40:59):\nWe get through two or three a week, so we're very slow.\n\nLenny Rachitsky (01:41:03):\nWorth it. Okay, next question. Do you have a favorite product you've recently discovered that you really love? We'll start with Shreya.\n\nShreya Shankar (01:41:10):\nYeah. I really like using Cursor, honestly. Now, Claude Code. I'll say why. I'm a researcher more so than anything else. I write papers, I write code, I build systems, everything, and I find that a tool... I'm so bullish on AI assisted coding because I have to wear a lot of hats all the time. Now, I can be more ambitious with the things that I build and write papers about, so I'm super excited about those. Cursor was my entry point into this, but I'm starting to find myself always trying to keep up with all these AI assisted coding tools.\n\nLenny Rachitsky (01:41:48):\nHamel?\n\nHamel Husain (01:41:49):\nYeah, I really like Claude Code and I like it because I feel like the UX is outstanding. There's a lot of love that went into that. It's just really impressive as a terminal application that is that nice.\n\nLenny Rachitsky (01:42:04):\nIronic that you two both love Claude Code when it's just built on vibes.\n\nShreya Shankar (01:42:09):\nI think it's false. It's not just built on vibes.\n\nLenny Rachitsky (01:42:13):\nThere we go. Okay, two more questions. Hamel, do you have a favorite life motto that you find yourself using in coming back to in work or in life?\n\nHamel Husain (01:42:21):\nKeep learning in. Think like a beginner.\n\nLenny Rachitsky (01:42:26):\nBeautiful. Shreya?\n\nShreya Shankar (01:42:27):\nI like that. For me, it's to always try to think about the other side's argument. I find myself sometimes just encountering arguments on the internet, like this race to eval debates and really think, \"Okay, put myself in their shoes. There's probably a generous take, generous interpretation.\" I think we're all much stronger together than if we start picking fights. My vision for evals is not that Hamel and I become billionaires. It is that everyone can build AI products, and we're all on the same page\n\nLenny Rachitsky (01:42:59):\nSlash everyone becomes billionaires.\n\nShreya Shankar (01:43:02):\nYes.\n\nLenny Rachitsky (01:43:04):\nAmazing. Final question. When I have two guests on, I always like to ask this question and I'll start with Hamel. What's something about Shreya that you like most? What do you like most about Shreya? I'm going to ask her the same question in reverse.\n\nHamel Husain (01:43:18):\nYeah. Shreya is one of the wisest people that I know, especially for being so young relative to me. I feel like she's much wiser than I am, honestly, seriously. She's very grounded and has a very even perspective on things. I'm just really impressed by that all the time.\n\nLenny Rachitsky (01:43:18):\nShreya?\n\nShreya Shankar (01:43:43):\nYeah. My favorite thing about Hamel is his energy. I don't know anybody who consistently maintains momentum and energy like Hamel does. I often think that I would start carrying much less about evals, if not for Hamel. Everyone needs a Hamel in their life, for sure.\n\nLenny Rachitsky (01:44:06):\nWell, we all have a Hamel in our life now. This was incredible. This was everything I'd hoped it'd be. I feel like this is the most interesting in-depth consumable primer on evals that I've ever seen. I'm really thankful you two made time for this. Two final questions. Where can folks find you? Where can they find the course and how can listeners be useful to you? I'll start with Shreya.\n\nShreya Shankar (01:44:29):\nYeah, you can reach me via email. It's on my website. If you Google my name, that is the easiest way to get to my website. You can find the course if you Google AI Evals for engineers and product managers, or just AI Evals course, you'll find it. We'll send some links hopefully after this, so it's easy. How to be helpful? Two things always for me. One is ask me questions when you have them. I'll try to get to the respond as soon as I can. The other one is tell us your successes. One of the things that keeps us going is somebody tells us what they implemented or what they did, a real case study. Hamel and I gets so excited from these and it really keeps us going, so please share.\n\nHamel Husain (01:45:16):\nYeah, it's pretty easy to find me. My website is Hamel.dev. I'll give you the link. You can find me on social media, LinkedIn, Twitter. The thing that's most helpful is to echo what Shreya said, we would be delighted if we are not the only people teaching evals. We would love other people teach evals. Any kind of blog posts, writing, especially that as you go through this and learn this that you want to share, we would be delighted to help re-share that or amplify that.\n\nLenny Rachitsky (01:45:54):\nAmazing. Very generous. Thank you two, so much for being here. I really appreciate it, and you guys have a lot going on, so thank you.\n\nShreya Shankar (01:46:01):\nThanks, Lenny, for having us and for all the compliments.\n\nLenny Rachitsky (01:46:05):\nMy pleasure. Bye everyone. Thank you so much for listening. If you found this valuable, you can subscribe to the show on Apple Podcasts, Spotify, or your favorite podcast app. Also, please consider giving us a rating or leaving a review as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at Lennyspodcast.com. See you in the next episode."
}
```

Episode 105: Monetizing passions, scaling marketplaces, and stories from a creator economy vet | Camille Hearst
Guest: Hamilton Helmer

```json
{
  "id": "hamilton-helmer",
  "guest": "Hamilton Helmer",
  "title": "Monetizing passions, scaling marketplaces, and stories from a creator economy vet | Camille Hearst",
  "transcript": "# Monetizing passions, scaling marketplaces, and stories from a creator economy vet | Camille Hearst\n\n## Transcript\n\nHamilton Helmer (00:00:00):\nWarren Buffett famously said, in business.\n\nAudio (00:00:02):\nI look for economic castles protected by unbreachable moats.\n\nHamilton Helmer (00:00:04):\nPower requires a benefit and a barrier, so he's taking care of the benefit part by saying a castle, you have to have a pretty good understanding of why it's a castle and not a shack.\n\nLenny Rachitsky (00:00:15):\nSo in a lot of decks it's like, \"Oh, we have the most amazing team. We move the fastest.\" You mention how rarely is that actually a power?\n\nHamilton Helmer (00:00:20):\nYou're on a treadmill and if you stop running on that treadmill, you get creamed, but it's not power. The things that drive operational excellence can be mimicked.\n\nLenny Rachitsky (00:00:30):\nLet's actually talk about achieving these powers.\n\nHamilton Helmer (00:00:32):\nThere's a thing called power progression. There are times when certain types of power are available. The path to power is where the rubber meets the road.\n\nLenny Rachitsky (00:00:44):\nToday my guest is Hamilton Helmer. Hamilton is a legend in the world of strategy. He's the author of 7 Powers, which outlines a framework for identifying and developing sustainable competitive advantage. It is widely considered to be the best book on strategy and people like Patrick Carlson, Peter Thiel, Reed Hastings, Daniel Eck, and so many more leaders credit the book and Hamilton's teachings for helping them build durable lasting companies. In our conversation, Hamilton shares what sources of power startups can start developing early, which types of power companies often think they have but they don't, how power relates to strategy and moats, when to start thinking about power as a startup and also what individual product managers and non-leaders can do about these insights about power. Also, how Hamilton sees AI impacting various entry and the sources of power. He also gives a preview of his new book that he's working on currently and so much more.\n\n(00:01:40):\nWith that, I bring you Hamilton Helmer after a short word from our sponsors. And if you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It's the best way to avoid missing future episodes and it helps the podcast tremendously. This episode is brought to you by WorkOS. If you're building a SaaS app, at some point your customers will start asking for enterprise features like SAML authentication and SCIM provisioning. That's where WorkOS comes in, making it fast and painless to add enterprise features to your app. Their APIs are easy to understand so that you can ship quickly and get back to building other features. Today, hundreds of companies are already powered by WorkOS, including ones you probably know like Vercel, Webflow and Loom. WorkOS also recently acquired Warrant, the fine-grained authorization service. Warrant's product is based on a groundbreaking authorization system called Zanzibar, which was originally designed for Google to power Google Docs and YouTube.\n\n(00:02:42):\nThis enables fast authorization checks at enormous scale while maintaining a flexible model that can be adapted to even the most complex use cases. If you're currently looking to build role-based access control or other enterprise features like single sign-on, SCIM or user management, you should consider WorkOS. It's a drop-in replacement for Auth0 and supports up to 1 million monthly active users for free. Check it out at WorkOS.com to learn more. That's WorkOS.com.\n\n(00:03:13):\nThis episode is brought to you by Vanta. When it comes to ensuring your company has top-notch security practices, things get complicated fast. Now you can assess risk, secure the trust of your customers and automate compliance for SOC 2, ISO 27001, HIPAA and more with a single platform Vanta. Vanta's market-leading trust management platform helps you continuously monitor compliance alongside reporting and tracking risks. Plus you can save hours by completing security questionnaires with Vanta AI. Join thousands of global companies that use Vanta to automate evidence collection, unify risk management, and streamline security reviews. Get $1,000 off Vanta when you go to vanta.com/Lenny. That's Vanta.com/Lenny. Hamilton Helmer, thank you so much for being here. Welcome to the podcast.\n\nHamilton Helmer (00:04:12):\nHi Lenny, a pleasure to be here.\n\nLenny Rachitsky (00:04:14):\nIt's even more my pleasure. I want to start by talking about when power becomes important. When do you recommend founders start thinking about power in terms of pre-product market fit, post-product market fit? Is it worth spending time thinking about power early? Obviously it's good to think about a little bit, but how much and how seriously should founders be thinking about it before they found something that people actually want?\n\nHamilton Helmer (00:04:38):\nOne of the things that has really surprised me in the last five years has been that my understanding of the answer to that question has changed. And before I thought it was you do product market fit and then you do strategy, and if you try and put strategy before product market fit, it is not much you can do with it. That's wrong. Actually, I mentioned before one of the great pleasures for me in my work is being able to talk to company founders and one of the things that has surprised me is that conversations with them, even at an early stage about strategic matters have a richness and relevance to me that was unexpected.\n\n(00:05:41):\nAnd founders are practical people. I mean it's very hard to do what they're doing and they have to be extremely focused and choose what's worthwhile spending time on. And so in observing their reactions and that dialogue, I could see that there was something going on that was meaningful to them. And this second book we're working on, we'll tease out why that's so more, but the answer to your question is when should you thinking about this? The answer is always. And that's an odd answer. And so even before you have product-market fit, it's worth thinking about strategy. Now it's not strategy in the sense of this fully articulated strategic planning, we're going to do this, these are going to be the competitors, this is how we're going to answer them. This is how we'll price. Not like that at all. At the earlier stage, you can imagine wildly more degrees of freedom and the questions are of the business propositions that you're thinking of in trying to get to product-market fit, what are the underlying characteristics that might tilt them towards the availability of power or not?\n\n(00:07:10):\nAnd those actually are meaningful conversations and certainly by no means certain. You're tilting probabilities, you're not creating a determinative things. And then later on, once you've already have product-market fit, then you have to understand your source of power to understand what competitive position is because then you have to establish that. And then later on in a more stable phase when you're in a stability phase of business, you have to know what your source of power is if you have one, because you have to know how to defend it. And then also it is also the foundational knowledge that you need for if there's another step. Because another thing that's quite surprising about iconic businesses is they often have a second act or a third act or fourth act. I mean think of AWS or Intel going into CPUs or Apple going into iPhones, all not the origin of business. And that's actually common, not unusual, and that's starting the process all over again.\n\nLenny Rachitsky (00:08:24):\nAwesome. You mentioned this word strategy. I want to set a little foundation here. How does strategy relate to power when people are thinking about these two concepts? And then do you have just a nice definition of what strategy is? Everyone's always just like, \"What the heck do you mean when you're talking about the strategy?\"\n\nHamilton Helmer (00:08:41):\nAs I said, I'm a concept person and so when you develop concepts, you have to be very highly constrained by their usefulness. I'm a great fan of the great mathematician, John von Neumann, and he had a view which really irritated a lot of mathematicians, which is that if mathematics wasn't guided by what was useful, it would become, I think the word he used was aesthetic to aesthetic. And so any concept developments like that, it needs to be guided by usefulness. And so in dealing with strategy, the question is what domain of things do you want to include in that conversation? Because the term is ubiquitous in business. I mean do a Google search sometime. I've done one recently on Google Scholar for the word strategy and you'll get a million, I'm not exaggerating, a million hits. And so strategy for some people might mean everything that gets to the pile, it gets to the top of a pile in terms of what you have to do that year.\n\n(00:09:58):\nEverything is... And that's a perfectly legitimate definition, but what I have found is that there is a very important narrowing that makes it much more useful. So that's coming back to what I was saying [inaudible 00:10:17] makes it much more useful to the business. So my view is that you want to focus, it's very useful for a business to focus on the fundamental determinants of business value and that's an arbitrary choice. It could be something else, but I can tell you from decades of business experience that that narrowing is very useful. And then so once you make that... Once you reach that understanding, it tells you some important things. So if you understand what drives business value, I mean if you do the math of it right, it's NPV of cash flow, right? Expected cash flow. And what that tells you is that strategy is a long time concept. You're looking far out in the future.\n\n(00:11:10):\nSo think of Pearl Harbor, Pearl Harbor for the Japanese was this enormous tactical success. They just destroyed the US ability, naval ability in the Pacific Ocean and the worst possible strategic move because it completely solved Franklin Delano Roosevelt's problem of how he could get the US citizens on board to attacking Hitler. The strategy of it was US's industrial might would eventually win the war and population. And the difference there is time constant, tactical short, strategy long. And so if you focus on value that narrows what you think about and allows you to get rather concise and offer up advice to founders about what they need to pay attention to.\n\nLenny Rachitsky (00:12:09):\nSo then how specifically is power informing strategies? Like the way you think about it, focus on your power and that will inform your strategy?\n\nHamilton Helmer (00:12:17):\nEarlier I talked about these economic structures that provide durability of return in terms of refuge from withering arbitrage of everybody who wants to eat your lunch. And so that's what power is. So you have to understand what is an economics? You're asking me questions that get pretty deep into theory or I hope you don't, this is too conceptual. But you have to say, you have to understand how competition takes place and say what is it that creates some kind of refuge? And what it is is there's something in what you do that gives you either a cost or price advantage over others. So let's say you're lower cost and that's the benefit. And the barrier side is that there's something that is durable about that that makes it over time that you can't, the competitors can't take away from you. So benefit and a barrier, we call it the to be or not to be test. And if you have that, you can think of that immediately translating into a value because that will give you good margins out into the foreseeable future, which is what you're after.\n\n(00:13:44):\nI don't know if that's... We're getting into the weeds here, but that's what it's about. And so power is those structures. Let me give you a quick example. So one I use in the book. So Netflix with scale economies, they have more subscribers. The cost of their content is a very large fixed cost, about 50% of their cost structure every year. They can take that fixed cost and spread it over more subscribers so their cost per subscriber is less versus somebody with fewer subscribers. So if they face the same prices for subscriptions as their competitors, they will be more profitable. And that's a scale economy and that's an example of a type of power and a common one I'd say. But these things are hard to achieve, right? Because there's the holy grail.\n\nLenny Rachitsky (00:14:47):\nSo let's actually talk about achieving these powers. Essentially the argument here is your power informs everything you do because this is the thing that'll allow you to stay durable and competitive and last. There's seven powers. We're not going to talk about all of them. If you want to go understand each of them, go read the book.\n\nHamilton Helmer (00:15:07):\nYeah. Let's not. That conversation always goes too long.\n\nLenny Rachitsky (00:15:08):\nYeah, exactly. So what I'm wondering is, okay, so say you're looking at this list of seven powers and you're a specific type of startup, do you have a heuristic that tells you here's most likely the power and set of subsets that will most likely be an option for you? Like B2B SaaS companies, is there a smaller subset? Probably one of these has to be one of your options versus B2C.\n\nHamilton Helmer (00:15:32):\nThat's a great question Lenny, because I think the path to power is where the rubber meets the road and it's very complicated and nuanced. But I'll give you a few thoughts along it and frankly our next book, the Second Invention, that's what it's about. It's about power, the entire book.\n\nLenny Rachitsky (00:15:56):\n[Inaudible 00:15:56] I'll try to read it.\n\nHamilton Helmer (00:15:58):\nSo in a book there's a thing called power progression, which says there are... It tells over the cycle of a business, there are times when certain types of power are available and the converse of that is times when they're not available. And so there's some that only are really available in when you reach a stability phase of a business pretty far out there.\n\n(00:16:25):\nAnd so if you're starting a company, take those off the table. So those two are branding and process power. So often I find that there's a confusion about this because brand recognition for a startup may be incredibly important, but you can get brand recognition by buying an ad in the Super Bowl. That's not power, that you paid for it. So take branding and process power and process power is really operational excellence on steroids and usually is imitable, so it's usually not.\n\n(00:17:05):\nSo take those off the table and then a resource type of power, which is you have something that is of value that if you transferred it to somebody else it would be of value to them, but you own the rights. So the barrier is law for example, or lack of knowledge from others. And there are classes of businesses that are like that. So prescription pharmaceuticals, if you are the first person to come up with Viagra, that's worth a lot of money. If you took that license and gave it to somebody else, it'd be worth a lot of money to them. But that's a different class and it's usually not the types of things that I'm dealing with and it's obvious to everybody. So the key challenge there is can you invent a pharmaceutical that's effective for a large market. So you can take those three off the table and that then leaves counter positioning, scale economies, switching costs and network economies.\n\n(00:18:13):\nAnd the important thing to keep in mind there is that they're sequenced. So almost every startup that you want to deal with starts with counter positioning because remember what product market fit is primarily is a substitution. You are coming up with a way to satisfy a more or less existing need in a novel way that creates more value. Now sometimes you tap into entirely new needs, but it's not so often. I mean Amazon was up against brick and mortar stores, Google was up against Yahoo and so on, and so you're usually substituting and that substitution, so your competition at that point is functional competition.\n\n(00:19:19):\nAnd if you don't have counter positioning at that point you're pretty high risk from an incumbent who already has the capabilities necessary to do that. They just have to extend their product or do this or that, but counter positioning is the refuge from that. And then you go into the other three types of power scale economies, switching costs and network economies, and those depend on your scale relative to competitors. This is pretty cursory, but that's what I'd say. I'll focus on those four and I would recommend that you think pretty hard about whether you think you have counter positioning to start.\n\nLenny Rachitsky (00:20:08):\nAwesome. I actually want to double down on that thread, but just to summarize, I have the list here. Basically you're saying if you're an early stage startup, the four to really that are actually potential powers for you at least early on, counter positioning, which your point is you could just start with that. That's essentially positioning and business model design, which happens at the beginning and then you can start to think about network economy, scale economy, switching costs as powers.\n\nHamilton Helmer (00:20:33):\nRight. And you've done the right thing by not having me go through and define each of those. But your listeners will need to go back to my book and see what those things are to get... We're going through the shorthand as we should, but all of what we've said won't be completely obvious to them.\n\nLenny Rachitsky (00:20:54):\nYeah, I think a simple Google search I find just gives you a very simple definition of these.\n\nHamilton Helmer (00:20:58):\nYeah, that's right. There's some people who have done some really good summaries of this.\n\nLenny Rachitsky (00:21:02):\nOr ChatGPT, even better in a lot of cases.\n\nHamilton Helmer (00:21:05):\nChatGPT if they get it right sometimes I find they hallucinate.\n\nLenny Rachitsky (00:21:12):\nHallucinate an eighth power. You mentioned how some companies think they have a certain power or it's common to think you have a certain power. If you look at every startup deck, there's always like, \"Here's our moat, here's the way we're going to have barriers to entry.\" I'm guessing in almost every case they're delusional about the power that they actually have and the power they think they'll have. Do you also find that to be true that often founders are wrong about how much barrier they've actually created and is there a power you often find most wrong and mistaken?\n\nHamilton Helmer (00:21:43):\nYeah, so I agree with your observation, but I don't want to be unkind. So I mean there are two things to keep in mind here in terms of making people feel better about that incorrect slide in the deck. One is that founders have to be optimistic. I think it's an important quality that they maybe understate the risk a little bit, but they're so committed to I'm going to do this thing that they go through that and that may give them an advantage over a large corporation trying to do the same thing. The other is that despite the name 7 Powers, which makes it sound like, oh, you can sort this out. Actually understanding whether or not there is a type of power in place is hard. I mean I did it with my colleagues here at Strategy Capital and we're looking at a well-known company, it might take us weeks to answer that question for a single company.\n\n(00:22:47):\nAnd it comes down to the hard part is industry economics is what really are the economic relationships and it's very hard. So with those caveats that... Give them some courtesy, I'll say some of the obvious ones are I mentioned before, people sometimes think they have branding power, but another one that I think I've heard you mention is people often think that they get scale economies through data. And I'd say that that's possible, but it's rare. And the reason it's rare is not because there aren't scale economies in data, but rather that the range of scale that the existing competitors have are often large enough to be able to put them in shouting distance of each so that the differences in their cost per unit is not that great.\n\n(00:23:50):\nThe curve flattens in other words, which is typical of any, because the most common scale economy is you've got a big fixed cost and you prorate that. And as you get more and more scale, the percentage cost advantage of a fixed cost advantage like that goes down and that often. So that's a pretty frequent one that we see. We laugh whenever we hear somebody say they have a flywheel, which gives you the idea of network economies. There are often flywheels, the ones that really are material, are rare. The key thing here is materiality, not whether the flywheel exists, but whether the effect is strong enough to really tilt returns.\n\nLenny Rachitsky (00:24:39):\nI was actually going to ask about that one because in software and social consumer products, network effects is always the pitch. Once we get big enough, we create this huge barrier. You mentioned just now you often find that's not actually true. It's rarely something that'll become a barrier. Is there anything else you find with network? And I know your power is called network economies, not network effect. I guess just to be clear, are these the same thing in your mind with different words?\n\nHamilton Helmer (00:25:02):\nYeah, kind of. I mean I have called it a type of power. So for me it's only those things which clear the significance barrier, hurdle rather that they're a large material. And so there are lots of things that I would say have network effects but not network economies.\n\nLenny Rachitsky (00:25:26):\nOh, interesting. Wait, can you speak to that? So there is a difference between these terms, network economies?\n\nHamilton Helmer (00:25:30):\nYeah. So for me the difference is materiality is that whether the value benefit is large enough to engender a price delta significant enough to give you materially different margins into the future.\n\nLenny Rachitsky (00:25:50):\nBasically, does that network effect have an actual impact on your business and your ability to price?\n\nHamilton Helmer (00:25:55):\nYeah, it's not an impact, it's a material impact. So it could have... If it's a penny to your bottom line, that's one thing if it's a billion dollars or something else.\n\nLenny Rachitsky (00:26:06):\nWow, that's actually really interesting. Is there an example of a company that comes to mind, they had network effects but not network economies as a power?\n\nHamilton Helmer (00:26:12):\nI think you could turn almost anywhere and get some modest network effects and any platform business would probably likely have some modest network effects. You asked me earlier and you sent me about Uber and Lyft, I'd say that they probably have network effects involved but not network economies.\n\nLenny Rachitsky (00:26:40):\nWow, that's interesting. And the reason you're saying they don't have network economies is because they're still so competitive they still have to spend so much money to stay ahead and so the network is not-\n\nHamilton Helmer (00:26:51):\nYeah. Right. The advantage that they get, it's not material. Right.\n\nLenny Rachitsky (00:26:59):\nWow, that's so interesting. Along those lines, it's so interesting to see Uber and Lyft these days. In theory they both had some sort of strong network effect. I was just looking, so Lyft is 5% the market cap of Uber. Is there a lesson from just what it is that allowed Uber to just win and kind of run away with the market essentially?\n\nHamilton Helmer (00:27:25):\nI'm not entirely sure, I'll take a guess, but take it as a unformed guess. I haven't really studied it carefully. I think that over time there, if I had to guess, I'd say they're probably modest scale economies in the business. And over time Uber has just very successfully played a war of attrition. And that's both been in how they run their... They made one initial misstep, which is they misdefined their business. They said it was international transportation and it's not. That business is extremely geographically specific.\n\n(00:28:14):\nIf you have a great position in the Bay Area, it doesn't help you in London. And so their forays into China and everything really didn't make, but they pulled back on that, focused down on understanding their source of power, which was a geographically specific scale economy. And then they've done interesting things like Uber Eats where they've tried to utilize the platform that they have to get other opportunities for the one side of their platform, the drivers. And so if I had to guess, I'd say it was a well-played war of attrition with modest scale economies.\n\nLenny Rachitsky (00:29:02):\nAnd is that attrition coming from a source of power or is that just like a broader, more strict?\n\nHamilton Helmer (00:29:06):\nYeah, it only works because there are modest scale economies. If there weren't any then if they did all this stuff, they'd still have a global and see.\n\nLenny Rachitsky (00:29:16):\nGot it. Let me go in a slightly different direction. We've talked about power, we've talked about strategy. There's also, there's this word moat that comes up a lot. Everyone's always trying to build a moat. In your mind, is a moat equivalent to a power? Is there a difference when people talk about these two?\n\nHamilton Helmer (00:29:30):\nPower requires a benefit and a barrier, you have to have something that you do that gives you a better outcome than your competitors, lower cost or higher price, and then something that makes it impossible for somebody else to mimic that. So moat is the second. So it's not synonymous with power because you can have a moat around a very undesirable piece of property and wouldn't get you far. But I think it is pretty synonymous with barrier. I think Warren Buffett, Charlie Munger, I admire enormously. I think I get credit for popularizing those concepts and I think the way they think about it is good. I'd say that 7 Powers is probably more systematic and comprehensive in saying that. I don't think. This is wonderful. I don't know if you've read any of the Microsoft antitrust literature that came out of their-\n\nLenny Rachitsky (00:30:42):\nNo.\n\nHamilton Helmer (00:30:42):\n...lawsuit, but there was a communication between Bill Gates and Warren Buffett where Warren Buffett was saying why he couldn't invest in Microsoft. He just didn't understand it. And so that meant the idea of network economies and what the moat was there he didn't understand. But I think the concept of a moat is a good one. The idea that you have something that gives you a refuge from competing forces.\n\nLenny Rachitsky (00:31:14):\nIn terms of Warren Buffett, I found the quote about moats. Warren Buffett famously said, \"In business, I look for economic castles protected by unreachable moats.\"\n\nHamilton Helmer (00:31:22):\nRight. And so he's taking care of the benefit part by saying a castle.\n\nLenny Rachitsky (00:31:27):\nRight. He's got it covered.\n\nHamilton Helmer (00:31:29):\nBut one of the tricks to understanding power is you have to have a pretty good understanding of why it's a castle and not a shack. So I'll give you a Netflix example. So a company I admire a lot, and I think if some of the things that they had to do to develop their business were so important for their business that don't guarantee a castle. So for example, UI development, it's been an enormous amount of resources on trying to get just the very best UI, I mean a zillion AB tests, all kinds of things. Their recommendation engine, everybody's knows the story about how that went, their interface with the content world and all this. So those things are important. They're things they have to spend a lot of time and resources on, but they can largely be mimicked.\n\n(00:32:31):\nSo when Netflix in an earlier phase was fighting Blockbuster, when Blockbuster finally threw in the towel, said, \"Well, we done. Well, better do a mail or a DVD business.\" If you look at the Blockbuster site, their UI site, you couldn't tell it different from Netflix. They just copied it. And so all that thoughtfulness about which things you put first and how you structure it and all that to make this suitable was mimicable. So that's an understanding of looking at the properties you have and trying to figure out if they're a castle or a shack.\n\nLenny Rachitsky (00:33:06):\nI love that. This episode is brought to you by Paragon, the embedded integration platform for B2B SaaS product development teams. Are your users constantly requesting new integrations with other SaaS platforms that they use? Unfortunately, native product integrations take months of engineering to build and the maintenance never ends. Paragon enables your engineering team to ship integrations seven times faster than building in-house by removing the complexities around authentication, messy third-party APIs and debugging integration errors. Engineering teams at companies like CopyAI, Cinch, TLDB and over 100 other SaaS companies are using Paragon so they can focus their efforts on core product features, not integrations.\n\n(00:33:53):\nThe results, their shipping integrations on demand, which has led to higher product usage, better retention and more customer upsells. Visit useparagon.com/Lenny to see how Paragon can help you go to market faster with integrations today. That's use paragon.com/Lenny. So let's take this concept of 7 Powers. A lot of people listening to this are just like individual contributor product managers on teams building new products or iterating on products they already have. What do you suggest they do with this knowledge of there exists these ways to build benefits and barriers? I'm working on say a new product. What do you recommend they do? What's something they could do this week, next month to infuse these lessons into the products they're building?\n\nHamilton Helmer (00:34:41):\nI'd say they're a few and it's a little bit different. I'm not a great fan of the strategically driven organization because that idea, because it fuzzes over how this knowledge is useful at different stages in the business. And so for somebody in that position, say a product manager in an existing successful business. So it is important in terms of just understanding their business to know what their source of power is because they... And that can inform them about what it is that they're working for. And also they may see things since they're down in the weeds, they may see things that are important to that that they need to bring to other people's attention. Because they're the ones that really have the knowledge of what the heck's going on. There's another aspect I mentioned before, this idea of transforming, of starting up entirely new things. And usually... I wouldn't say, usually I say it's not uncommon for ideas about that to bubble up from down below.\n\n(00:36:03):\nAnd so that's another source. So let's separate a business into three phases, origination, takeoff, and stability. So the answers I've given are more in the stability phase. In the takeoff phase, let's say you've launched a product, you've gotten customer traction, now you're in a phase where there's just very rapid growth, probably other entrants like you, what are you facing? What you're facing is remember that underneath all of this is a change in technology. That's what made the product market fit possible in the first place. But that doesn't just stop. That if you're in a technology wave, often there are all kinds of offshoots both for you and the compliments to your business and everything else going on at the same time.\n\n(00:36:59):\nAnd to win at that stability phase, which is really a market share win, you have to be aware of those and understand, okay, we have to incorporate this new feature. Or maybe now things have gotten to the point where this new market segment is, our product is attractive before it wasn't. This is meat and potato stuff for somebody at that level and it may well be the decisive element in terms of whether you win that market share battle with the other contenders. And so you have a very, very important role at that point. And so I'd say the first thing to do with your question is to make sure you think about the different phases of this and then ask what those responsibilities are.\n\nLenny Rachitsky (00:37:51):\nWhat about for people that are just trying to get better at being strategic, thinking strategically something every product leader is always encouraged to do, become a better strategic thinker, build as muscle of strategy, what do you often advise to people just get better at the stuff? Obviously read your book.\n\nHamilton Helmer (00:38:09):\nRead the book, and then have conversations with your colleagues about the topic because as you internalize what that means and how, have conversations with them about do we really have this kind of power? What's going on here? What's important? What isn't. Those conversations tend to allow you to get a better grip on things. I mean in the case of Netflix, Reed actually had me come in and train the top a hundred people in Netflix and strategy. We actually ran classes in the company, but that's unusual I'd say. And I didn't have the book yet. And so I think the book gets people pretty far down that path already.\n\nLenny Rachitsky (00:38:55):\nAnd you don't do that anymore, I imagine if someone wanted to do that today, not an option.\n\nHamilton Helmer (00:38:59):\nI sadly don't have the time. I mean I do often do fireside chats at company meetings and that kind of thing, but not a full-blown course and I'm not teaching at Stanford anymore either. And so I don't. I enjoyed that immensely wonderful people to work with. But sadly I don't have the time.\n\nLenny Rachitsky (00:39:23):\nYou're about to get a lot of requests for fireside chats. I hope you're ready. You mentioned AI at some point in our chat. I'm curious how you think AI is going to change your 7 Powers framework. Do you think defensibility goes down in general? Will certain forms of power become more important or harder to achieve? How do you think about AI?\n\nHamilton Helmer (00:39:44):\nYeah, it's a great question. I mean we're all in this phase of wondering exactly how generative AI is going to play out. My own view, currently I don't see any change in seven powers from it in terms of an eighth power or something. But the issues that it brings up, like scale economies, I mean think of scale economies. If you have a fixed cost of trying to develop a model of a billion dollars or something or network effects, will AI models develop so that they learn in a way that for one user's interaction helps another user's interaction? That would be a powerful network economy. Or if it learns, if you think of if it learns about you and becomes a better psychiatrist or something, then that's a switching cost. So all these things are relevant to which business models will work and I find that useful.\n\n(00:40:51):\nBut I think in general, the way I think about it is it's a standard form of potentially very powerful technology that is being introduced into the business world and just asking how that plays out. I currently tend to think of that there's three types of plays. There's the company that's the technology play itself. So if you think of microprocessors, it would be Intel. There's the companies that wouldn't exist without the technology. So for semiconductors it would be Microsoft. And then there are the companies that utilize the technology, but it had existed before and after. So for semiconductors it would be automobiles. They used a ton of chips, but there were still cars before, after.\n\n(00:41:48):\nAnd so I'm of the view, I'm very much uncertain at this point that generative AI, its biggest impact will be that tertiary class. It will be used in a lot of things that existed before and exist after, but are made better by it like semiconductors and automobiles. And so it reminds me, if you think of really big technology shifts like this, it reminds me of electricity. When electricity came, you could completely reconfigure a factory floor. You no longer had to have... You could have the power source essentially at the operating unit of an operator. But that took a lot of redesign, incorporation, investment, learning, complements, all kinds of stuff. I tend to think this will be more like that. There will be some pure cases. People will want to have them write their term paper with ChatGPT or something. But if you think of businesses, it's hard for me to think of a single functional area in a business that with redesign couldn't benefit. So accounting, HR, R&D all have uses of this but requires incorporation, which is always troublesome.\n\n(00:43:30):\nAnd so that's my view. But there certainly will be businesses that couldn't exist without it. And there's some that are coming up and some of those are in fact the businesses that empower the tertiary need, they're the ones that bring in. But if you go back, this will date me, but if you go back into business history, back in, I guess it was the 90s, there was this thing called business process re-engineering, the idea that you could take a computer sensibility into business processes, redesign them and get these monstrous cost savings. And it was a gigantic consulting opportunity for people. Whole companies got developed based on that and it feels more like that to me, but it's very interesting. I could be wrong, but it feels different than crypto. It feels like there's more of a real ultimate use case. I mean, if it really is true what they say that a 50% improvement in programming efficacy is not uncommon, just that proposition alone is worth an awful lot of money if you think how many programmers are in the world.\n\nLenny Rachitsky (00:44:56):\nNo question. You mentioned eighth power. I just want to check, is there an emerging eighth power you wish you maybe would've included or maybe added in the future that's like, \"Oh, maybe this is on the edge,\" or it's like, \"Nope, we got these seven?\"\n\nHamilton Helmer (00:45:15):\nIt's a great question. I'm always looking for it because if you find it it probably means it's so obscure there, it'll also be a great investment opportunity. We're looking all the time, but so far, no. So far I'm pretty satisfied that seven is an exhaustive set, but never say never. That's an empirical seven, not a theoretical seven.\n\nLenny Rachitsky (00:45:37):\nIf we start seeing you making incredible returns, you've clearly found an eighth power [inaudible 00:45:41]-\n\nHamilton Helmer (00:45:41):\nThat's right.\n\nLenny Rachitsky (00:45:43):\nI want to close one thread on a power that you mentioned that is often a pitfall, which is around process power and basically execution. A lot of people think so in a lot of decks it's like, oh, we have the most amazing team, we move the fastest, we're earliest. You mentioned how rarely is that actually a power actually being able to execute and create a process that is an actual barrier? Can you just talk a bit more about that to help people understand okay, it's probably not our power?\n\nHamilton Helmer (00:46:09):\nOne of the great thinkers in strategy was this Harvard professor Michael Porter, and he in probably 40 years ago, made the very controversial statement that operational excellence is not strategy. He got a lot of people of the Harvard Business School faculty really mad at him because that's what their careers were about and it sounded like he was dissing them. But the point he was making was when you get to this end state, if you already have power that things that drive operational excellence can be mimicked because you can hire a consulting firm who has best practices, knowledges that come in and get you up to snuff. You can hire people from your competitors who know how to do it better. And that's true, but it's also true in this takeoff phase in a business that we talked about before, when you're trying to attain competitive position, operational excellence is everything.\n\n(00:47:14):\nAnd so if you think of strategy, not statically endpoints like Professor Porter was, but if you look at dynamically how you get there, operational excellence is essential for a strategy. So think of those things I mentioned before about Netflix, about their UI and recommendation engine and so on, or international rollout, all those things. They were in a battle to get more subscribers than other people and those were critical for that. But in themselves, they're critical in attaining competitive position, but in themselves, they're not sources of power typically, unless there's some very tight considerations here or very demanding considerations for the... Unless if they have to be material, but they also have to be opaque or some way people can't easily imitate them. Either they don't understand what's going on or they might be opaque. So for example, think of TSMC. So when they put up the latest fab and get that operational, are there a lot of steps in doing that?\n\n(00:48:43):\nThey know how to do with their staff is trained to do it, but it's not documented necessarily and you can't imitate it. Then maybe they have it. I don't know if they have process power or not, but it takes that level of complexity. In my book, I use the example of Toyota and a car manufacturing is complex enough that you can have this opacity in terms of material steps, but it's not common. So if you're in a stability phase of business, you're stuck with this funny thing, which is most of your day is on those issues and it should be because if you don't do it, a competitor can and they can end up better than you. And so you're on a treadmill and that's the way business is. That's fine. And if you stop running that treadmill, you get creamed. So you got to do it, it's most of your day, but it's not power. And there are those rare cases where it's so material and so inimitable that it can be power, but they're rare.\n\nLenny Rachitsky (00:50:02):\nI like the heuristic that if you haven't written it down or you can't describe it, that might be a sign that maybe process power is a power of yours.\n\nHamilton Helmer (00:50:13):\nYeah. And there isn't a consulting firm that offers to bring you up to speed on that.\n\nLenny Rachitsky (00:50:20):\nThey'll make you more like Amazon as a service. Kind of along these same lines, you talk about how the only things that create value in a company are power, market size and operational excellence. And I think hearing that will blow a lot of people's minds because they think there's so many things that contribute to the value of a company and you whittle it down to these three things. What can you say about that insight?\n\nHamilton Helmer (00:50:47):\nSo I'd say they're right and I'm right. They're right because there are this incredible... I mean business is really hard and there are just a multitude of things you have to pay attention for. I'm right because all those things fall into those three categories. So it's an exhaustive set and it simply comes out of the math. So we're both right I'd say.\n\nLenny Rachitsky (00:51:16):\nFinal question, the intent of a lot of your work is to empower founders. I'm curious if you've noticed any broad economic trends or shifts that you think will make life easier or harder for founders in the coming years?\n\nHamilton Helmer (00:51:32):\nPersonally, I am very, very concerned about the debt trajectory of the United States and of many countries around the world, but I'll pick on the United States, but it's a trend going on everywhere. We're on a trajectory for this extremely high indebtedness. And so if you think about the last 30 years, right? There's been a crisis about once every 10 years. So there's the dot-com bust, there's the financial crisis, there was COVID. Nothing makes me think that the frequency will be a lot less. I don't know what, who knows? These are all uncertain events. But imagine if we got to one of those and we had no dry powder and dry powder for us is the ability to heavily deficit spend take on debt. And fortunately our government did that in both the financial crisis and in COVID. And because that people had jobs. My own view about the financial crisis is that if we hadn't done that, plus having Ben Bernanke as the head of the Fed, we would've gone into another great depression.\n\n(00:52:52):\nIt was that ugly. So this current debt trajectory, you don't know how long it will take exactly when, but eventually that will mean we will not have dry powder. People will not respect the credit worthiness of this country. So that worries me a lot. And it utterly will affect the idea of company founding because if you get into a crisis like that, the capital markets lock up and it gets very difficult to do anything. And to really stretch my credibility here, I'll opine on just how hard a problem this is to solve. The reason this is so difficult for this country to solve and other countries is that it is right at the crux of the delicate dance between capitalism and democracy. So if you think about the problem, of course that's driving all this is entitlements. There's discretionary spending, certain recovery programs and stuff, but those can go away. The underlying trend that people just can't get their arms around is entitlements. And anybody who looks at the numbers can see that. Every economist knows that. But that's not an easy fix.\n\n(00:54:18):\nAnd the reason it's not an easy fix is there are two opposing views of what's going on, and there's no way to resolve those two views. One is that capitalism is rapacious and results in more and more inequality and the government has to do something about it. And the other is that the government is on a path that is creeping socialism that will undermine our freedom and economic sufficiency. And the poster child for the rapacious capitalism. One is, I don't know if you've seen the recent analysis of inequality in the United States just came out, much more robust analysis. And it said that basically inequality in the last 60 years of the United States is unchanged, but that's post-transfer, post-tax inequality, which basically says the amount of taxing and transferring going on was about right. So it says that the amount we're spending is about right from that perspective, that it compensates for the other inequities and we should be spending that much so we should tax more.\n\n(00:55:30):\nAnd the poster child for the other point of view of the dangers of government is the steadily increasing without interruption percentage of the economy that is government. And to levels that 75 years ago, people would've thought absolutely impossible. So there are these two views about what's going on. One is that we're compensating for the normal inequities of capitalism, and the other is that we're headed down a path to socialism and ruin. And that leads to a deadlock, which is you don't tax anymore and you don't cut spending and that leaves deficits. So anyway, a long, long rant, sorry but that trend is extremely politically difficult to deal with and extremely threatening and that concerns me immensely.\n\nLenny Rachitsky (00:56:32):\nNot to leave listeners with a very sad state of affairs.\n\nHamilton Helmer (00:56:36):\nYeah, sorry.\n\nLenny Rachitsky (00:56:37):\nNo, no. I think this is important. I think it's important people think about this and know these things. Is there anything that gives you hope? Is there anything that gets you excited about either for founders or anyone in general, just to leave folks with maybe on a happy?\n\nHamilton Helmer (00:56:50):\nYeah, I mean, I am an optimist really in a way, and I do. There's a famous Austrian and eventually American economist named Joseph Schumpeter who wrote this wonderful book Theory of Economic Development way back when over a hundred years ago, where he took the unusual view of saying that the vitality of an economy depended on entrepreneurs. And I ascribe to that. I think that creativity and action are the ways society and people advance. And I think the US's and a free society has huge advantages in that. And I think that I feel very lucky to be in a place I'm in Silicon Valley, but to be in a place in a country where that is vital and active and I think that's ground zero for me. And so I think you see that alive and well, I think. And there are people that are very Enthusiastic about that as I am.\n\nLenny Rachitsky (00:58:12):\nBeautiful way to close out our chat. Is there anything else you want to share before we get to our very exciting lightning round? Is there anything you want to leave listeners with or any last tidbit of advice?\n\nHamilton Helmer (00:58:22):\nI alluded to it before, but just that remember, action is the first principle of business. You do stuff and my book is very oriented towards that. The idea was not to tell you what to do, but to give you guideposts while you're on that journey. People that are enthused about it, I encourage you to do stuff. That's where it all starts and I can think about it and maybe help a little bit, but it's mostly doing stuff.\n\nLenny Rachitsky (00:58:59):\nI love that point so much. It was something I was going to touch on but I didn't get to is just there's so many people that just sit around and theorize about a strategy of their business, especially in their early stage. Here's our grand master plan, here's an amazing strategy or just read about startup ideas and don't actually try it. I love this final note of just try it. Just do it. Don't just sit there and-\n\nHamilton Helmer (00:59:18):\nYeah, yeah, just do it. And life is full of surprises. You'll end up in a place you didn't expect.\n\nLenny Rachitsky (00:59:24):\nAmazing. Speaking of ending up in a place you didn't expect, it's time for our very exciting lightning round. Are you ready?\n\nHamilton Helmer (00:59:32):\nOh, sure. I'll do my best here. I'm not very good on lightning.\n\nLenny Rachitsky (00:59:37):\nFirst question, what are two or three books that you've recommended most to other people?\n\nHamilton Helmer (00:59:42):\nOne book that is extremely wonky, and I can only take it in very small doses, but is magnificent, is one called The Road to Reality by Roger Penrose, who's this brilliant mathematician. And it will be very daunting for anybody unless you're a deep math person, but his brilliance in erudition just shines through in this thing. And it's an amazing book. I would say there's another book, boy, I wish I could remember the name. Maybe you can get the name of the author called Gene by this geneticist. And I think he's a Harvard Medical School professor that's about the history of genetics and he is an absolutely luminous writer. I mean, it puts me to shame. I'm embarrassed when I read it because I think how pedestrian my writing is and incredibly knowledgeable about the history of genetics and I think that's such an important topic. So those are two that I would recommend highly, fairly wonky, but I like them both.\n\nLenny Rachitsky (01:00:56):\nI love them. The author, I just looked him up. Siddhartha Mukherjee.\n\nHamilton Helmer (01:01:01):\nYes. Just amazing. You read it and you go, \"How did he think of that phrasing?\" I mean, it's just, he's amazing.\n\nLenny Rachitsky (01:01:10):\nDo you have a favorite recent movie or TV show you really enjoyed?\n\nHamilton Helmer (01:01:14):\nI'm a huge movie fan and have been all my life, and I'm particularly keen on animated films. But the movie that I've recently seen that I liked particularly it was American Fiction. I thought that was... It didn't make any of the easy choices in a movie and as a result was just incredibly interesting and thoughtful I thought.\n\nLenny Rachitsky (01:01:39):\nDo you have a favorite product you recently discovered that you really love?\n\nHamilton Helmer (01:01:43):\nIn my office, just this last week, we actually put a Persian rug in our entry room, and this is what's called a Farahan Sarouk rug and it's 150 years old. And it had an effect in me that I didn't expect, which is it is a work of great beauty. And it was before, it was all hand-done before machines. You get all this wonderful variation of the actual icons in the rug and the different dye colors. And I find that every morning when I walk in, I go, \"That's really beautiful,\" and it's uplifting and it shows you the importance of, or the value of the quality of art. I mean, just blows my mind actually. So that's probably not the usual product discussion that you get.\n\nLenny Rachitsky (01:02:50):\nNo, I love that answer. Recently we've had some really unique choices. One is a very nice Mercedes and a Rivian and a minivan recently. We've got a lot of very nice things.\n\nHamilton Helmer (01:03:00):\nOh, that's great. I'm a car guy too, so I didn't answer on the car question.\n\nLenny Rachitsky (01:03:05):\nOh man, I love that. We have Rory Sutherland coming on the podcast soon. He is one of the leaders of Ogilvy and he has a whole thing about how buying a home is the best value of art to buy art if you live in a home that makes you feel inspired and is beautiful.\n\nHamilton Helmer (01:03:20):\nYeah, I'm a big believer in that. I mean, I think that your place and how you connect to it has an important grounding effect. And then before I talked earlier about creativity and I think surrounding yourself in an environment that stimulates that is really important. And so I couldn't agree with him more.\n\nLenny Rachitsky (01:03:51):\nTwo more questions. Do you have a favorite life motto that you often think about, come back to share with friends or family?\n\nHamilton Helmer (01:03:58):\nOne is what Clint Eastwood's advice to actors, which is don't just do something, stand there. And so there are a lot of things that are long-termish with low signal to noise. And you can often just do a lot of stuff that you think makes a difference, but it really doesn't. And so that's one. The other one which is somewhat more profound I think was one that I had a famous Sri Lankan journalist was a mentor of mine and very dear friend, and he had a favorite expression that I adhere to all the time, which is everything is always about something else.\n\nLenny Rachitsky (01:04:49):\nWow. Deep.\n\nHamilton Helmer (01:04:53):\nAnd that's so true if you're dealing with this power stuff when if you really dig down, everything is always about something else.\n\nLenny Rachitsky (01:04:58):\nSpeaking of power, final question. People that have a lot of power are leaders in the world. I'm curious, do you have a favorite historical leader?\n\nHamilton Helmer (01:05:07):\nSo yeah. I have some that I admire a great deal. I'm a tremendous fan of Winston Churchill's. Most really great people are quirky and he qualified. There are things you could say about him that you might not have liked that so much, but he was a genius and had great fortitude, human sense. He understood things long before other people. I've give him high marks. Some of the great artists I admire enormously. I'm reading a book right now on the last 20 years of Michelangelo, which wonderful book actually, and which is a very interesting period because in his first 70 years he finished up all that, he wasn't going to do sculpture anymore. He just finished The Last Judgment, which was the wall of the Sistine Chapel.\n\n(01:06:08):\nAnd I think the last major fresco he did. And a lot of his friends at that time, he was exiled from Florence and living in Rome and he had this Roman community and a lot of his close friends that had recently died or had difficulties. So he's at this inflection point in his life at 70, which in those days was very old, and yet he went on to do some of the most remarkable architecture in the history of the world. And so you've got to admire that. I mean, just doing that, which is that rare second act. But I think for world leaders, Winston Churchill is very high. I'm a fan of Teddy Roosevelt, I must say too, in this country.\n\nLenny Rachitsky (01:07:00):\nI love that. Hamilton, you are wonderful. I feel like we have helped a lot of people up-level their ability to think about strategy and moats and power. Thank you so much for being here. Two final questions. Where can folks find more online if they want to dig in further, and how can listeners be useful to you?\n\nHamilton Helmer (01:07:15):\nYeah, so as I say, I'm an idea person and I'm about trying to empower company founders, and so only thing I can say is read the book, spread the ideas, start your own company. Those are the things that would make me happy.\n\nLenny Rachitsky (01:07:37):\nAmazing. Hamilton, Thank you so much for being here.\n\nHamilton Helmer (01:07:40):\nGreat. My pleasure, Lenny.\n\nLenny Rachitsky (01:07:42):\nBye everyone. Thank you so much for listening. If you found this valuable, you can subscribe to the show on Apple Podcasts, Spotify, or your favorite podcast app. Also, please consider giving us a rating or leaving a review as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at Lenny'sPodcasts.com. See you in the next episode."
}
```

Maintain cross-references with all previously processed episodes (1-100).
Update the master index, topic organization, and framework library.
